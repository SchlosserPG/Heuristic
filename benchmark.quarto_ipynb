{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Benchmark Optimization Problems\"\n",
        "author: \"Pamela Schlosser\"\n",
        "format: html\n",
        "runtime: python\n",
        "---\n",
        "\n",
        "\n",
        "# Benchmark Problems: Overview\n",
        "\n",
        "-   OneMax Problem: The OneMax problem is widely used as a benchmark in evolutionary algorithms to test how well algorithms can evolve a binary string towards an optimal solution (a string of all 1s). It is simple and useful for evaluating basic evolutionary or heuristic search methods. Knapsack Problem: The Knapsack problem is another classic benchmark problem in optimization, particularly for combinatorial algorithms. Variants such as the 0/1 Knapsack and Fractional Knapsack are commonly used to evaluate algorithms like dynamic programming, greedy algorithms, and evolutionary methods.\n",
        "-   Ackley Function: The Ackley function is a well-known continuous optimization benchmark problem. It is often used to test optimization algorithms' ability to handle multi-modal functions with many local minima. Algorithms like simulated annealing and genetic algorithms are frequently evaluated using this function.\n",
        "-   Schaffer Min-Min Problem: he Schaffer Min-Min is a well-known benchmark in multi-objective optimization. It provides a simple yet effective test case for algorithms that need to identify Pareto-optimal solutions in multi-objective spaces.\n",
        "-   These benchmark problems are critical for testing and comparing the performance of optimization algorithms, especially in research and development of new heuristic methods.\n",
        "\n",
        "# Discrete Optimization Problems\n",
        "\n",
        "## OneMax Problem\n",
        "\n",
        "-   In evolutionary algorithms, the OneMax problem serves as a simple test problem where the goal is to evolve a population of binary strings towards the optimal solution (a string of all 1s). The fitness function is used to evaluate the quality of each candidate solution in the population.\n",
        "\n",
        "-   Fitness Function: Imagine life had a personal ‚Äòfitness function‚Äô just for you. What variables would you include in it, and how would you weigh them?\n",
        "\n",
        "### One Max Formula\n",
        "\n",
        "-   Binary String: A binary string is generated using NumPy‚Äôs randint function, which creates a list of 0s and 1s. It can be any positive integer.\n",
        "-   Fitness Function: The one_max function calculates the ‚Äúfitness‚Äù of the binary string, which is simply the sum of all 1s in the string. This is the value that needs to be maximized.\n",
        "-   Example Run: If the generated binary string is \\[1, 0, 1, 1, 0, 1, 0, 1, 1, 0\\], the fitness would be 6, since there are six 1s in the string.\n",
        "-   The objective is to maximize the number of 1s in a binary string.\n",
        "\n",
        "$$\\max_{s \\in A} f(s) = \\sum_{i=1}^{n} s_i, \\quad \\text{subject to} \\ s_i \\in \\{0, 1\\}.$$\n",
        "\n",
        "### Optimal Solution OneMax\n",
        "\n",
        "-   The optimal solution of this problem is that all the subsolutions assume the value 1; i.e., $s_i=1$ for all $i$. For instance, the optimal solution for $n=4$ is $s^*=(1111)$ and the objective value of a possible solution $s^*=(0111)$ can be easily calculated as the count of the number of ones in the solution $s$ as the objective function if $f(s) = f(0111) = 0+1+1+1 = 3$\n",
        "\n",
        "### OneMax Pseudocode\n",
        "\n",
        "\n",
        "```{asis}\n",
        "Pseudocode:\n",
        "\n",
        "\n",
        "FUNCTION one_max(binary_string):\n",
        "# Calculate the fitness as the sum of 1s in the binary string\n",
        "    RETURN sum(binary_string)\n",
        "\n",
        "# Example usage\n",
        "SET n = 10  # Length of the binary string\n",
        "\n",
        "# Generate a random binary string of length n\n",
        "SET binary_string = generate a random list of 0s and 1s of size n\n",
        "\n",
        "# Calculate the fitness\n",
        "SET fitness = one_max(binary_string)\n",
        "\n",
        "# Print the binary string and its fitness\n",
        "PRINT \"Binary string:\", binary_string\n",
        "PRINT \"Fitness (number of 1s):\", fitness\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "### OneMax Python Implementation\n"
      ],
      "id": "dd52fc80"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def one_max(binary_string):\n",
        "    return sum(binary_string)\n",
        "\n",
        "# Example usage: # Length of the binary string\n",
        "n = 10\n",
        "\n",
        "# Generate a random binary string of length n\n",
        "binary_string = np.random.randint(0, 2, size=n).tolist()\n",
        "\n",
        "# Calculate the fitness\n",
        "fitness = one_max(binary_string)\n",
        "\n",
        "print(f\"Binary string: {binary_string}\")\n",
        "print(f\"Fitness (number of 1s): {fitness}\")\n",
        "\n",
        "\n",
        "iterations = 20\n",
        "fitness_over_time = [np.random.randint(0, n + 1) for _ in range(iterations)]  # Random example of fitness change\n",
        "\n",
        "# Line plot of fitness over iterations\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(range(iterations), fitness_over_time, marker='o', color='green', linestyle='-', linewidth=2)\n",
        "plt.fill_between(range(iterations), fitness_over_time, color='lightgreen', alpha=0.4)\n",
        "plt.title(f\"Fitness Evolution Over Time\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Fitness (number of 1s)\")\n",
        "plt.xticks(np.arange(0, iterations, step=1))  # Show whole numbers on the x-axis\n",
        "\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "50a7c1c4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   The plot above tracks how fitness improves or changes across iterations in an optimization algorithm, giving insight into the convergence of the algorithm.\n",
        "\n",
        "### Comparing OneMax Problem to Greedy Algorithm\n",
        "\n",
        "-   If a greedy search algorithm is used and is allowed to randomly add one to or subtract one from the current solution $s$ to create the next possible solution $v$ for solving the one-max problem, that is, it is allowed to move one and only one step to either the left or the right of the current solution in the landscape of the solution space.\n",
        "-   Without knowledge of the landscape of the solution space, the search process will easily get stuck in the peaks of this solution space.\n",
        "-   Hence, most researchers prefer using the one-max problem as an example because it is easy to implement and also because it can be used to prove if a new concept for a search algorithm is correct.\n"
      ],
      "id": "73e0d556"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Defining binary solutions and their corresponding objective values\n",
        "solutions = [\n",
        "    \"(0000)\", \"(0001)\", \"(0010)\", \"(0011)\", \"(0100)\", \"(0101)\", \"(0110)\", \n",
        "    \"(0111)\", \"(1000)\", \"(1001)\", \"(1010)\", \"(1011)\", \"(1100)\", \"(1101)\", \n",
        "    \"(1110)\", \"(1111)\"\n",
        "]\n",
        "objective_values = [0, 1, 1, 2, 1, 2, 3, 2, 3, 3, 2, 4, 3, 3, 4, 5]\n",
        "\n",
        "# Plotting the graph\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(solutions, objective_values, marker='*', markersize=10, linestyle='-', color='black')\n",
        "plt.xlabel(\"Solution\")\n",
        "plt.ylabel(\"Objective Value\")\n",
        "plt.title(\"Objective Value vs. Solution\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "id": "8500b0ba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Knapsack Problem\n",
        "\n",
        "-   The Knapsack Problem is a classic NP-complete optimization problem, where you are given a set of items, each with a weight and a value.\n",
        "\n",
        "-   The goal is to determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible.\n",
        "\n",
        "-   Types of Knapsack Problems:\n",
        "\n",
        "    -   0/1 Knapsack Problem:\n",
        "        -   Each item can be included (1) or excluded (0) in the knapsack.\n",
        "        -   You cannot break items into smaller parts. $\\max_{s \\in A} f(s) = \\sum_{i=1}^{n} s_i v_i, \\quad \\text{subject to} \\quad w(s) = \\sum_{i=1}^{n} s_i w_i \\leq W, \\quad s_i \\in \\{0, 1\\}$ Where $v_i$ is the value associated with $s_1$ and $w_i$ is the weight associated with $s_i$\n",
        "    -   Fractional Knapsack Problem:\n",
        "        -   You can break items into smaller parts and include fractions of them in the knapsack.\n",
        "        -   Ratio = $\\frac{v_i}{w_i}$, where $v_i$ is the value of item $i$, and $w_i$ is the weight of item $i$.\n",
        "\n",
        "### NP Complete\n",
        "\n",
        "-   NP (Nondeterministic Polynomial Time): A problem is in NP if a solution can be verified in polynomial time by a deterministic algorithm. In other words, given a solution, it is possible to check if it is correct relatively quickly (in polynomial time). However, finding the solution itself might take much longer (potentially exponential time) unless the problem can also be solved in polynomial time.\n",
        "-   NP-complete refers to a class of problems in computational complexity theory that are both NP (nondeterministic polynomial time) and every problem in NP can be reduced to it in polynomial time\n",
        "-   NP-hard problems are optimization or decision problems that are at least as difficult to solve as the hardest problems in NP (nondeterministic polynomial time).\n",
        "    -   Unlike NP-complete problems, NP-hard problems do not have to be verifiable in polynomial time. This means that while it may be incredibly hard to find an optimal solution, even verifying a proposed solution might take more than polynomial time.\n",
        "    -   Essentially, NP-hard problems are hard to solve optimally, and their complexity often prevents efficient algorithms from finding or checking solutions within a reasonable time frame.\n",
        "    -   NP-hard problems are broader and potentially harder than NP-complete problems because they can include problems that aren't even in NP. They may not have a polynomial-time verification process.\n",
        "\n",
        "### Key Characteristics of NP-complete Problems\n",
        "\n",
        "-   Difficult to solve: No known algorithms can solve NP-complete problems efficiently (in polynomial time) for all instances.\n",
        "\n",
        "-   Verification in polynomial time: If someone provides a solution, it can be verified quickly. Equivalence to other NP-complete problems: If one NP-complete problem can be solved in polynomial time, all NP-complete problems can be solved in polynomial time.\n",
        "\n",
        "-   Knapsack Problem: The 0/1 knapsack problem is NP-complete. Finding the optimal solution is hard, but verifying if a solution meets the constraints and maximizes value can be done in polynomial time.\n",
        "\n",
        "-   The Fractional knapsack problem is not NP-complete and can be solved in polynomial time using a greedy algorithm.\n",
        "\n",
        "-   The Travelling Salesman is a NP-hard problem.\n",
        "\n",
        "### Example Fractional Knapsack Problem\n",
        "\n",
        "-   Knapsack Capacity: 15 kg\n",
        "\n",
        "    -   Items Available:\n",
        "        -   Item 1: Value = 10, Weight = 5 kg\n",
        "        -   Item 2: Value = 40, Weight = 10 kg\n",
        "        -   Item 3: Value = 30, Weight = 15 kg\n",
        "\n",
        "-   Objective: Maximize the total value without exceeding 15 kg.\n",
        "\n",
        "-   The greedy algorithm works well by prioritizing items with the highest value-to-weight ratio.\n",
        "\n",
        "-   0/1 Knapsack Problem requires more complex algorithms like dynamic programming to find the optimal solution over the Fractional Knapsack Problem\n",
        "\n",
        "### Greedy Algorithm for Fractional Knapsack\n",
        "\n",
        "-   Step 1: Calculate Value-to-Weight Ratio:\n",
        "    -   Item 1: 10/5=2\n",
        "    -   Item 2: 40/10=4\n",
        "    -   Item 3: 30/15=2\n",
        "-   Step 2: Sort Items by Ratio (Descending): Item 2, Item 1, Item 3\n",
        "    -   Step 3: Fill the Knapsack:\n",
        "    -   Take Item 2 (10 kg, Value = 40).\n",
        "    -   Take as much of Item 1 as possible (5 kg, Value = 10).\n",
        "-   Results: Total Weight = 15 kg, Total Value = 50.\n",
        "\n",
        "## Binary to Decimal (B2D) Problem: B2D-1\n",
        "\n",
        "-   The binary to decimal model is often used in optimization problems, particularly in the context of genetic algorithms and heuristic methods.\n",
        "-   With a minor modification, the solution space of the one-max problem can be simplified as the solution space of another optimization problem.\n",
        "-   The model uses binary strings to represent numbers. Each string represents a decimal number when interpreted in binary form.\n",
        "-   The B2D-1 problem is to maximize the value of the objective function of a binary string.\n",
        "\n",
        "### Characteristics and Visualization of B2D-1\n",
        "\n",
        "-   These two examples are possible landscapes to the B2D problem.\n",
        "-   The first chart to the left implies that there are only two possible next states (candidate solutions) that can be generated from the current solution except for solutions (0000) and (1111), which can only be moved to the right and to the left, respectively.\n",
        "-   If another search algorithm can generate a new candidate solution by randomly inverting (flipping) one of the subsolutions of the current solution, the number of possible states of the new candidate solution will be $n$, where $n$ is the number of subsolutions.\n",
        "\n",
        "![OneMax](Pictures/OneMax.png \"OneMax\")\n",
        "\n",
        "## B2D with Deception: B2D-2\n",
        "\n",
        "-   B2D deception problems mislead optimization algorithms away from finding the global optimum by presenting local optima that seem promising but are actually suboptimal.\n",
        "-   Used to test whether a search algorithm is capable of escaping local optima or not.\n",
        "-   Deception problems highlight the necessity of exploration in heuristic algorithms, such as introducing diversity through mutation or crossover in genetic algorithms. If the algorithm becomes too greedy and focuses only on local fitness improvements (exploitation), it may get stuck at deceptive local optima.\n",
        "\n",
        "# Continuous Optimization Problems\n",
        "\n",
        "-   Unlike the COP, the possible solutions for a continuous optimization problem are typically ‚Äúuncountably infinite.‚Äù This means that the number of solutions in the solution space is tantamount to the number of real values in the given space, that is, infinite.\n",
        "    -   Single-objective optimization problem (SOP)\n",
        "    -   The Ackley optimization problem\n",
        "-   Multi-objective optimization problem (MOP)\n",
        "    -   The Schaffer Min-Min Global Optimization Problem\n",
        "\n",
        "## Single-objective Optimization Problem (SOP)\n",
        "\n",
        "-   A single-objective optimization problem involves finding the best solution from a set of feasible solutions based on a single objective function. The goal is to either maximize or minimize this objective function.\n",
        "\n",
        "$$\\underset{s \\in \\mathbb{R}^n}{\\text{opt}} f(s), \\quad \\text{subject to } \\, c_i(s) \\odot b_i, \\quad i = 1, 2, \\ldots, m,$$\n",
        "\n",
        "-   where\n",
        "    -   ${R}^n$ and ${R}$ are the domain and codomain, respectively,\n",
        "    -   $f(s) {R}^n$ and ${R}$ is the objective function to be optimized,\n",
        "    -   $c_i(s): {R}^n$ and ${R}\\odot b_i, \\quad i = 1, 2, \\ldots, m,$ are the constraints,\n",
        "    -   and $opt$ and $\\odot$ are as given in Definition 1 as \\<, \\>, =, ‚©Ω, or ‚©æ.\n",
        "\n",
        "### Ackley Function: A Single Optimization Problem\n",
        "\n",
        "-   The Ackley Function is a widely used benchmark function for testing optimization algorithms. It is characterized by its multi-modal nature with a nearly flat outer region and a large hole at the center.\n",
        "-   Applications\n",
        "    -   Used as a standard test case in evaluating the performance of optimization algorithms like genetic algorithms, simulated annealing, and particle swarm optimization.\n",
        "    -   Relevant in fields such as machine learning, control systems, and operations research.\n",
        "-   Limitations\n",
        "    -   The function's large search space and numerous local minima make it difficult for algorithms to converge to the global minimum.\n",
        "    -   Large importance of balancing exploration and exploitation in optimization strategies when dealing with the Ackley Function.\n",
        "\n",
        "#### Explanation of Ackley Function(x, y)\n",
        "\n",
        "-   Computes the value of the Ackley function given a point (x, y) in the search space.\n",
        "-   The optimization algorithm optimizes the Ackley function to find the point where it reaches its minimum. It initializes a population of random solutions, evaluates their fitness (using the Ackley function), and iteratively improves them using an optimization method (like gradient descent or a genetic algorithm).\n",
        "-   The best solution and corresponding function value (score) are returned as the result.\n",
        "\n",
        "#### Ackley function and B2D: Converting to Decimal\n",
        "\n",
        "-   The Ackley function uses the binary representation, where the binary strings need to be converted to decimal values (i.e., real numbers). In this case, the converted decimal values correspond to points in the continuous search space.\n",
        "-   For example, a binary string like 1010 can be converted into a decimal value, which can then be used as input to the Ackley function. Binary String: 1010, where the binary number is 1010_2.\n",
        "-   Each position in the binary number represents a power of 2, starting from the right (least significant bit):\n",
        "-   The rightmost bit (0) is $2^0$,The next bit (1) is $2^1$ ,The next bit (0) is $2^2$,The leftmost bit (1) is $2^3$.\n",
        "\n",
        "$1010_2= 0 ‚àó 2^0+ 1 ‚àó 2^1 +0 ‚àó 2^2+1 ‚àó 2^3$ $= 0 ‚àó 1+ 1 ‚àó 2 + 0 ‚àó 4 + 1 ‚àó 8$ $=  0 + 2 + 0 + 8 = 10$ Thus, the decimal equivalent of the binary string \"1010\" is 10. Use this value as input for the Ackley function.\n",
        "\n",
        "#### Characteristics and Visualization of Ackley Function\n",
        "\n",
        "-   The Ackley function is evaluated in the hypercube.\n",
        "-   The global optimum (minimum) of the Ackley function is ùëì(ùë†\\^‚àó)=0 is located at $s^*=(0,0,‚Ä¶0)$.\n",
        "-   This function has many local optima, which makes it hard for the search algorithm to find the global optimum.\n",
        "\n",
        "![Ackley](Pictures/GenericAckley.png \"Ackley\")\n",
        "\n",
        "#### Ackley Function Formula\n",
        "\n",
        "$$\n",
        "\\begin{array}{rl}\n",
        "\\min_{s \\in \\mathbb{R}^n} f(s) &= -20 \\exp \\left(-0.2 \\sqrt{\\frac{1}{n} \\sum_{i=1}^n s_i^2} \\right) \\\\\n",
        "& \\quad - \\exp \\left( \\frac{1}{n} \\sum_{i=1}^n \\cos(2 \\pi s_i) \\right) + 20 + e, \\\\\n",
        "\\text{subject to} & \\quad -30 \\leq s_i \\leq 30, \\quad i = 1, 2, \\dots, n.\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "#### Ackley Function: Pseudocode\n",
        "\n",
        "-   The below example uses a random function to pull a point that we want to hit the local minima. You can imagine, this might not be the best way to do this.\n",
        "\n",
        "\n",
        "```{asis}\n",
        "PSEUDOCODE\n",
        "FUNCTION Ackley(s):\n",
        "    SET a = 20, b = 0.2, c = 2 * pi\n",
        "    SET n = length of s\n",
        "    COMPUTE sum_sq_term = sum of squares of all elements in s\n",
        "    COMPUTE cos_term = sum of cos(2 * pi * each element in s)\n",
        "    \n",
        "    COMPUTE term1 = -a * exp(-b * sqrt(sum_sq_term / n))\n",
        "    COMPUTE term2 = -exp(cos_term / n)\n",
        "    \n",
        "    RETURN term1 + term2 + a + e\n",
        "\n",
        "# Main Execution\n",
        "SET n = 2  # Dimension of the Ackley function\n",
        "\n",
        "# Generate random vector s with elements between -30 and 30\n",
        "SET s = random values in range [-30, 30] of length n\n",
        "\n",
        "# Compute the Ackley function result for the vector s\n",
        "SET result = Ackley(s)\n",
        "\n",
        "PRINT vector s\n",
        "PRINT Ackley function result for vector s\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "#### Ackley Function Python Implementation\n"
      ],
      "id": "5c4693f0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "np.random.seed(5042)\n",
        "\n",
        "# Ackley function implementation\n",
        "def ackley(s):\n",
        "    a, b, c = 20, 0.2, 2 * np.pi\n",
        "    n = len(s)\n",
        "    sum_sq_term = np.sum(s**2)\n",
        "    cos_term = np.sum(np.cos(c * s))\n",
        "    term1 = -a * np.exp(-b * np.sqrt(sum_sq_term / n))\n",
        "    term2 = -np.exp(cos_term / n)\n",
        "    return term1 + term2 + a + np.e\n",
        "\n",
        "# Example usage: \n",
        "n = 2  # Dimension \n",
        "s = np.random.uniform(-30, 30, n)  # Generate random s_i values in the range [-30, 30]\n",
        "\n",
        "result = ackley(s)  # Evaluate the Ackley function\n",
        "\n",
        "print(f\"Vector s: {s}\")\n",
        "print(f\"Ackley function result: {result}\")\n",
        "\n",
        "# Visualization of the Ackley function\n",
        "x = np.linspace(-30, 30, 400)\n",
        "y = np.linspace(-30, 30, 400)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "\n",
        "# Compute Z for the Ackley function\n",
        "Z = np.array([ackley(np.array([x_val, y_val])) for x_val, y_val in zip(np.ravel(X), np.ravel(Y))])\n",
        "Z = Z.reshape(X.shape)\n",
        "\n",
        "# Plotting the Ackley function surface\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none')\n",
        "\n",
        "# Customize the plot\n",
        "ax.set_title(\"Ackley Function Surface\")\n",
        "ax.set_xlabel(\"s_1\")\n",
        "ax.set_ylabel(\"s_2\")\n",
        "ax.set_zlabel(\"f(s)\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "id": "1f2cee54",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Example results: Vector s: \\[12.8 3.16\\]; Ackley function result: 17.9\n",
        "-   Distance from the Origin: The Ackley function reaches its global minimum of 0 at the origin (i.e., when both $x_1$ and $x_2$ are close to 0). Our vector values are quite far from the origin, which is why the function result is positive and relatively large at 17.9\"\n",
        "-   The Ackley landscape has an exponentially increasing structure as you move away from the global minimum. It has many local minima, which makes optimization algorithms prone to getting stuck in suboptimal solutions. A result like 17.9 is far from zero, and indicates that the vector is located in such a suboptimal region of the function space.\n",
        "-   Thus, the Ackley result of 17.9 suggests that the point \\[12.8 3.16\\]; is not close to the global minimum (which is zero at the origin) and is located in a region of higher function values.\n",
        "\n",
        "#### Differential Evolution with the Ackley Function\n",
        "\n",
        "-   To demonstrate how an algorithm does well on the Ackley function, we can use a global optimization algorithm such as Differential Evolution (DE), which is effective for non-convex functions with many local minima.\n",
        "-   Differential Evolution (DE) is a population-based optimization algorithm used for solving complex multidimensional problems. It belongs to the family of evolutionary algorithms, where a population of candidate solutions evolves over time to find the global optimum of a function.\n",
        "-   The differential_evolution function from the scipy.optimize module is a powerful optimization tool designed to solve global optimization problems. It is a type of evolutionary algorithm, which is used when the function to optimize is non-linear, has many local minima, or is not differentiable.\n"
      ],
      "id": "958af415"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import differential_evolution\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(5042)\n",
        "\n",
        "# Define the Ackley function\n",
        "# Ackley function implementation\n",
        "def ackley(s):\n",
        "    a, b, c = 20, 0.2, 2 * np.pi\n",
        "    n = len(s)\n",
        "    sum_sq_term = np.sum(s**2)\n",
        "    cos_term = np.sum(np.cos(c * s))\n",
        "    term1 = -a * np.exp(-b * np.sqrt(sum_sq_term / n))\n",
        "    term2 = -np.exp(cos_term / n)\n",
        "    return term1 + term2 + a + np.e\n",
        "\n",
        "# Set the bounds for the variables \n",
        "bounds = [(-30, 30), (-30, 30)]\n",
        "\n",
        "# Use differential evolution to minimize the Ackley function\n",
        "result = differential_evolution(ackley, bounds, seed=42)\n",
        "\n",
        "# Print the result\n",
        "print(f'Optimized parameters (x1, x2): {result.x}')\n",
        "print(f'Function value at minimum: {result.fun}')\n",
        "\n",
        "\n",
        "# Visualization of the Ackley function\n",
        "x = np.linspace(-30, 30, 400)\n",
        "y = np.linspace(-30, 30, 400)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "\n",
        "# Compute Z for the Ackley function\n",
        "Z = np.array([ackley(np.array([x_val, y_val])) for x_val, y_val in zip(np.ravel(X), np.ravel(Y))])\n",
        "Z = Z.reshape(X.shape)\n",
        "\n",
        "# Plotting the Ackley function surface\n",
        "fig = plt.figure(figsize=(10, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none')\n",
        "\n",
        "# Customize the plot\n",
        "ax.set_title(\"Ackley Function Surface (2D)\")\n",
        "ax.set_xlabel(\"s_1\")\n",
        "ax.set_ylabel(\"s_2\")\n",
        "ax.set_zlabel(\"f(s)\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "id": "cccf8364",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   Using a great function that is designed to solve problems with multiple local minimums, you can see that we got extremely close to the local minimum 0.\n",
        "\n",
        "## Multi-objective Optimization Problem (MOP)\n",
        "\n",
        "-   Given a set of functions and a set of constraints, the MOP is to find the optimal value or a set of optimal values (also called Pareto front), subject to the constraints, out of all possible solutions of these functions.\n",
        "\n",
        "$$\\text{opt}\\left( f_1(s), f_2(s), \\dots, f_k(s) \\right),\n",
        "\\quad \\mathbf{s} \\in \\mathbb{R}^n, \n",
        "\\quad \\text{subject to } c_i(s) \\odot b_i, \\quad i = 1, 2, \\dots, m,$$\n",
        "\n",
        "-   Where\n",
        "    -   ${R}^n$ and ${R}$ are the domain and codomain, respectively,\n",
        "    -   $f(s) {R}^n$ and ${R}$ is the objective function to be optimized,\n",
        "    -   $c_i(s): {R}^n$ and ${R}\\odot b_i, \\quad i = 1, 2, \\ldots, m,$ are the constraints,\n",
        "    -   and $opt$ and $\\odot$ are as given in Definition 1 as \\<, \\>, =, ‚©Ω, or ‚©æ.\n",
        "\n",
        "### The Schaffer min-min Multi-objective Optimization Problem\n",
        "\n",
        "-   The Schaffer min-min problem is a well-known test function in the field of multi-objective optimization.\n",
        "-   It is often used to evaluate optimization algorithms due to its simplicity and well-defined structure. The problem is particularly famous for having a simple Pareto-optimal front.\n",
        "-   The Schaffer function can be defined as a two-objective optimization problem, where the objectives are functions of a single variable x.\n",
        "-   The goal is to minimize both of these objective functions simultaneously.\n",
        "\n",
        "#### Characteristics and Visualization\n",
        "\n",
        "-   Convexity: The Pareto front of the Schaffer min-min problem is convex, making it relatively easy to identify the trade-off surface between the two objectives.\n",
        "-   Uniqueness: The problem has a unique global Pareto-optimal front.\n",
        "-   Complexity: Despite its simplicity, the problem is useful for benchmarking optimization algorithms, especially in multi-objective optimization.\n",
        "-   Graphically, the Pareto front of the Schaffer min-min problem can be visualized as a curve in the objective space, where $f_1(x)$ is plotted against $f_2(x)$ and the curve represents the set of optimal trade-offs between the two objectives.\n"
      ],
      "id": "39a1b283"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        " \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate points for the Pareto front curve\n",
        "f1 = np.linspace(0.5, 1.5, 100)  # Narrow range to zoom in\n",
        "f2 = 4 / f1  # Example relationship to create a Pareto front shape\n",
        "\n",
        "# Sample points for sa and sb closer together\n",
        "sa_f1, sa_f2 = 0.7, 4 / 0.7\n",
        "sb_f1, sb_f2 = 0.9, 4 / 0.9\n",
        "\n",
        "# Plot the Pareto front\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(f1, f2, 'k-', label=\"Pareto front\")  # Pareto front curve\n",
        "plt.scatter([sa_f1, sb_f1], [sa_f2, sb_f2], color=\"black\")  \n",
        "\n",
        "# Add labels for sa and sb\n",
        "plt.text(sa_f1, sa_f2, r'$s_a$', fontsize=12, verticalalignment='bottom', horizontalalignment='right')\n",
        "plt.text(sb_f1, sb_f2, r'$s_b$', fontsize=12, verticalalignment='bottom', horizontalalignment='right')\n",
        "\n",
        "# Add vertical and horizontal lines for sa and sb projections\n",
        "plt.plot([sa_f1, sa_f1], [0, sa_f2], 'k--', linewidth=0.8)\n",
        "plt.plot([sb_f1, sb_f1], [0, sb_f2], 'k--', linewidth=0.8)\n",
        "plt.plot([0, sa_f1], [sa_f2, sa_f2], 'k--', linewidth=0.8)\n",
        "plt.plot([0, sb_f1], [sb_f2, sb_f2], 'k--', linewidth=0.8)\n",
        "\n",
        "# Axis labels and limits for the zoomed-in view\n",
        "plt.xlabel(r'$f_1$')\n",
        "plt.ylabel(r'$f_2$')\n",
        "plt.xlim(0.5, 1.5)\n",
        "plt.ylim(0, 6)\n",
        "plt.grid(True)\n",
        "\n",
        "plt.title(\"Schaffer Min-Min\")\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "id": "5a901d4a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### scipy.optimize import minimize\n",
        "\n",
        "-   minimize is a general-purpose function from scipy.optimize used for finding the minimum of a scalar function.\n",
        "-   Uses the BFGS (Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno) algorithm when no specific method is provided. This method is a quasi-Newton optimization algorithm, particularly useful for smooth unconstrained problems.\n",
        "    -   It can handle different types of optimization problems, including: Unconstrained minimization\n",
        "    -   Constrained minimization (equality and inequality constraints) Bounded minimization (where variables are limited to a certain range)\n",
        "-   Basic Workflow:\n",
        "    -   Define the objective function (the function to minimize). Choose an initial guess for the variables.\n",
        "    -   Run the minimize function with the desired method.\n",
        "    -   Analyze the results: returns optimized variables, the function value, and other diagnostic information.\n",
        "\n",
        "#### Schaffer Min-Min Formula\n",
        "\n",
        "$$\n",
        "\\min_{s \\in \\mathbb{R}^n} \n",
        "\\begin{cases}\n",
        "f_1(s) = s^2, \\\\\n",
        "f_2(s) = (s - 2)^2,\n",
        "\\end{cases}\n",
        "\\quad \\text{subject to} \\quad s \\in [-10^3, 10^3].\n",
        "$$\n",
        "\n",
        "#### Schaffer Min-Min Pseudocode\n",
        "\n",
        "\n",
        "```{asis}\n",
        "Pseudocode\n",
        "\n",
        "# FUNCTION to calculate f1(s)\n",
        "FUNCTION f1(s):\n",
        "    RETURN s^2\n",
        "\n",
        "# FUNCTION to calculate f2(s)\n",
        "FUNCTION f2(s):\n",
        "    RETURN (s - 2)^2\n",
        "\n",
        "# FUNCTION for combined objective, weighted sum of f1 and f2\n",
        "FUNCTION combined_objective(s, w1=0.5, w2=0.5):\n",
        "    RETURN w1 * f1(s) + w2 * f2(s)\n",
        "\n",
        "# MAIN EXECUTION\n",
        "# Step 1: Set up the bounds for the solution (s ‚àà [-1000, 1000])\n",
        "SET bounds = [-1000, 1000]\n",
        "\n",
        "# Step 2: Initialize a starting guess for the solution\n",
        "SET initial_guess = 0\n",
        "\n",
        "# Step 3: Minimize the combined objective function using an optimization algorithm\n",
        "CALL minimize function with combined_objective, initial_guess, and bounds\n",
        "STORE the result in result\n",
        "\n",
        "# Step 4: Print the optimization result\n",
        "PRINT \"Optimal value of s:\", result.x\n",
        "PRINT \"f1(s):\", f1(result.x)\n",
        "PRINT \"f2(s):\", f2(result.x)\n",
        "PRINT \"Combined objective:\", combined_objective(result.x)\n",
        "\n",
        "# Step 5: Visualization - Create a range of values for s from -1000 to 1000\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "#### Schaffer Min-Min Python Implementation\n",
        "\n",
        "-   Population Initialization: A population of random solutions is initialized within the bounds \\[‚àí1000,1000\\]. Objective Function Evaluation: For each solution, both objective functions are evaluated.\n",
        "-   Score Combination: The results of the two functions are combined into a single score, which can be minimized. In this case, the combination is a simple sum of f1 and f2.\n",
        "-   Optimization Loop: Iteratively updates the solutions to find the minimum combined score using an optimization technique (e.g., gradient descent, genetic algorithm).\n"
      ],
      "id": "1c9a21be"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Define the two objective functions for the Schaffer problem\n",
        "def f1(s):\n",
        "    return s**2\n",
        "\n",
        "def f2(s):\n",
        "    return (s - 2)**2\n",
        "\n",
        "# Combined objective function: weighted sum of f1 and f2\n",
        "# You can adjust the weights to explore different trade-offs between the two objectives\n",
        "def combined_objective(s, w1=0.5, w2=0.5):\n",
        "    return w1 * f1(s) + w2 * f2(s)\n",
        "\n",
        "# Bounds for the solution (s ‚àà [-1000, 1000])\n",
        "bounds = [(-1000, 1000)]\n",
        "\n",
        "# Initial guess for the solution\n",
        "initial_guess = np.array([0])\n",
        "\n",
        "# Use scipy's minimize function to find the solution\n",
        "result = minimize(combined_objective, initial_guess, bounds=bounds)\n",
        "\n",
        "# Print the result\n",
        "print(\"Optimal value of s:\", result.x[0])\n",
        "print(\"f1(s):\", f1(result.x[0]))\n",
        "print(\"f2(s):\", f2(result.x[0]))\n",
        "print(\"Combined objective:\", combined_objective(result.x[0]))\n",
        "\n",
        "# Visualization of the objective functions and combined objective\n",
        "s_values = np.linspace(-1000, 1000, 400)\n",
        "f1_values = f1(s_values)\n",
        "f2_values = f2(s_values)\n",
        "combined_values = combined_objective(s_values)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot f1(s), f2(s), and combined objective\n",
        "plt.plot(s_values, f1_values, label=\"f1(s) = s^2\", color='blue')\n",
        "plt.plot(s_values, f2_values, label=\"f2(s) = (s - 2)^2\", color='green')\n",
        "plt.plot(s_values, combined_values, label=\"Combined Objective\", color='red', linestyle='--')\n",
        "\n",
        "# Mark the optimal solution found\n",
        "plt.axvline(x=result.x[0], color='black', linestyle=':', label=f\"Optimal s = {result.x[0]:.2f}\")\n",
        "\n",
        "# Customize the plot\n",
        "plt.title(\"Schaffer Min-Min Problem Visualization\")\n",
        "plt.xlabel(\"s\")\n",
        "plt.ylabel(\"Objective Function Value\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "id": "b0d4b004",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-   The results you achieved for the Schaffer Min-Min problem look excellent, as they closely approximate the optimal solution.\n",
        "-   Optimal value of $s$ we found is nearly exactly $1$, the known optimal solution for the Schaffer function.\n",
        "    -   Objective function values: $f_1(s) = s^2 = 1.000000027$\n",
        "    -   : $f_2(s) = (s-2)^2 = 0.999999973$\n",
        "    -   These values are very close to 1 for both $f_1$ and $f_2$, indicating that the function values at this $s$ are near-optimal.\n",
        "-   Combined objective: The combined objective (likely calculated as a weighted sum or some other combination of $f_1$ and $f_2$ is 1.00000, which is extremely close to the expected combined optimal value of 1. This negligible difference suggests that the optimization algorithm has performed very well.\n",
        "\n",
        "# Using AI\n",
        "* Use the following prompt on a generative AI, like chatGPT, to learn more about the topics covered.\n",
        "** evaluating optimization algorithms? Provide examples of their use in different domains.\n",
        "* Benchmark Problems Overview: What are benchmark problems, and why are they essential in evaluating optimization algorithms? Provide examples of their use in different domains.\n",
        "* Comparing Problems: Compare the OneMax problem, the Knapsack problem, and the Ackley function. Discuss the type of optimization each addresses and the challenges it presents.\n",
        "* Fitness Function: Write a Python function to evaluate the fitness of a binary string in the OneMax problem. Explain how this function helps in evolutionary algorithms.\n",
        "* Greedy vs Dynamic Programming: Solve the Fractional Knapsack problem using a greedy algorithm. Then, solve the 0/1 Knapsack problem using dynamic programming. Compare the results and explain the differences in their complexity and solutions.\n",
        "* Applications: Discuss real-world applications of the Knapsack problem. How do its constraints reflect practical optimization challenges?\n",
        "* Function Analysis: Explain why the Ackley function is challenging for optimization algorithms. What characteristics make it a good benchmark for multi-modal optimization?\n",
        "* Python Implementation: Use the provided Ackley function implementation to evaluate random points in the search space. Visualize the function in 3D and identify the global minimum.\n",
        "* Optimization Challenges: Discuss the concept of deception in B2D problems. Why is it critical for testing an algorithm's ability to escape local optima?\n",
        "* Schaffer Min-Min Problem: Solve the Schaffer Min-Min problem using a weighted sum approach. Experiment with different weight combinations and discuss how they affect the solution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Conclusions\n",
        "\n",
        "-   We introduced a set of popular optimization benchmark problems, widely used to evaluate the performance of various algorithms across different types of optimization challenges. These benchmarks include the OneMax Problem, which is commonly used in evolutionary algorithm research to assess an algorithm‚Äôs capacity to evolve a binary string towards an optimal solution, specifically a string composed entirely of 1s. This problem is straightforward and serves as a foundation for evaluating basic evolutionary or heuristic methods. The Knapsack Problem is a classic combinatorial, NP-complete problem that tests algorithms‚Äô abilities to maximize the total value of selected items without exceeding a weight limit. This problem is pivotal in assessing algorithms designed for discrete optimization tasks, where finding an optimal solution is computationally intensive.\n",
        "\n",
        "-   The Binary-to-Decimal (B2D) Problem introduces a deceptive landscape that misleads optimization algorithms toward suboptimal solutions. It is particularly useful for testing whether an algorithm can avoid getting trapped in local optima and demonstrates the importance of incorporating diversity strategies like mutation in evolutionary algorithms.\n",
        "\n",
        "-   The Ackley Function represents a single continuous optimization benchmark with a highly multi-modal landscape. It is challenging due to its numerous local minima, making it ideal for testing an algorithm‚Äôs ability to balance exploration and exploitation in continuous search spaces. In multi-objective optimization, the Schaffer Min-Min Problem serves as a benchmark, presenting a simple yet effective test for algorithms to identify Pareto-optimal solutions, as it requires the simultaneous minimization of two objectives. Collectively, these benchmark problems allow researchers and practitioners to compare the strengths and limitations of optimization algorithms across both discrete and continuous scenarios."
      ],
      "id": "2c1754d9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}