{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Genetic Algorithms\"\n",
        "author: \"Pamela Schlosser\"\n",
        "format: html\n",
        "runtime: python\n",
        "---\n",
        "\n",
        "\n",
        "# Genetic Algorithms\n",
        "\n",
        "-   If you could ‘mutate’ one skill you have to be even better, which one would it be? And if you could ‘crossover’ with someone else’s skill, what would you pick?\n",
        "\n",
        "-   Genetic Algorithms (GAs) are optimization techniques inspired by the process of natural selection and evolution.\n",
        "\n",
        "-   Population: A set of potential solutions (individuals) to the problem. Genes/Chromosomes: Each individual is represented by a chromosome (a string of genes), typically encoded as a binary, real-number, or symbolic representation.\n",
        "\n",
        "-   Objective: To evolve the population toward better solutions by mimicking evolutionary processes such as selection, crossover, and mutation.\n",
        "\n",
        "-   Diagram illustrating the flow of a genetic algorithm: initial population → selection → crossover → mutation → new population.\n",
        "\n",
        "## Key Steps\n",
        "\n",
        "-   Initialization: Randomly generate an initial population of individuals (solutions).\n",
        "-   Selection: Choose the fittest individuals based on a fitness function that measures solution quality.\n",
        "-   Crossover (Recombination): Combine parts of two parent solutions to create offspring (new solutions).\n",
        "-   Mutation: Introduce random changes to an individual to maintain diversity and explore new parts of the solution space.\n",
        "-   Replacement: Replace the old population with the new one, ensuring improvement over generations.\n",
        "-   Termination: Continue until a stopping condition (e.g., number of generations or convergence) is met.\n",
        "\n",
        "## Advantages\n",
        "\n",
        "-   Global Search: Capable of exploring large solution spaces and avoiding local optima.\n",
        "-   Flexibility: Can be applied to many types of optimization problems, including those with non-linear or non-differentiable objectives.\n",
        "-   Heuristic Nature: Useful when problem-solving methods like calculus-based optimization are not feasible.\n",
        "\n",
        "## Genetic Algorithms Defined\n",
        "\n",
        "-   Each iteration during the convergence process is called a generation. GA will search more than one candidate solution per generation.\n",
        "\n",
        "-   All the solutions of each generation are called a population indicating that there are many candidate solutions.\n",
        "\n",
        "-   Each solution is called a chromosome or an individual.\n",
        "\n",
        "-   Each subsolution of a solution is called a gene.\n",
        "\n",
        "-   Fitness is the objective function value.\n",
        "\n",
        "-   Selection is the selection based on the objective function value.\n",
        "\n",
        "-   Optimization patterned on evolution\n",
        "\n",
        "    -   Maintain a population of solutions.\n",
        "    -   “Survival of the fittest”.\n",
        "\n",
        "-   Evolution of an improving solution\n",
        "\n",
        "    -   The population evolves over many generations.\n",
        "    -   The fittest population members are more likely to reproduce and create offspring with their genetic material.\n",
        "    -   Population fitness improves through the generations.\n",
        "\n",
        "## The relationship between population $s$, chromosome $si$, and gene $si,j$\n",
        "\n",
        "-   $s$ represents the population (a set of solutions),\n",
        "-   $s_i$ represents a chromosome (a solution),\n",
        "-   $s_{ij}$ represents a gene (a subsolution),\n",
        "-   $m$ represents the number of chromosomes in the population (also called population size, and\n",
        "-   $n$ represents the number of subsolutions in a chromosome.\n",
        "-   The objective value of each chromosome will also be transformed to a value called the *fitnes value* by a so-called fitness function.\n",
        "\n",
        "![Genetic Algorithm](Pictures/ga1.png \"Genetic Algorithm\")\n",
        "\n",
        "### The Relationships between Parents and Children\n",
        "\n",
        "-   Each current solution selected by the selection operator is called a parent, while each new candidate solution is called an offspring during the convergence process.\n",
        "-   The parents s’ will be selected by the selection operator from the current population s at iteration t. The fitness values of all the chromosomes will also be calculated by the selection operator.\n",
        "-   Once the parents are selected by the selection operator, the crossover and mutation operators are used to generate a new population $v$, which is called the offspring of the parents $s’$.\n",
        "-   The offspring at iteration $t$ will become the current population s of iteration $t+1$. This is referred to as the reproduction process.\n",
        "\n",
        "![Genetic Algorithm](Pictures/ga2.png \"Genetic Algorithm\")\n",
        "\n",
        "-   The strategies of the selection, crossover, and mutation operators of GA: (a) using only the selection operator and (b) using all the three operators.\n",
        "\n",
        "-   GA will first randomly select some of the chromosomes based on the fitness value of each chromosome.\n",
        "\n",
        "-   Shows the situation in which only the selection operator is used, i.e., no other transition operators are used. In this case, the distribution of all the chromosomes will be shifted and changed from left to right on the x-axis from generation t=1 to generation t=3.\n",
        "\n",
        "-   This means that the average objective value of the population will be increased while the variance is decreased. If GA uses only the selection operator but none of the transition operators (e.g., crossover or mutation), it will not generate any new candidate solutions even though the average objective value of all the chromosomes is raised from 1.5 to 2.5.\n",
        "\n",
        "-   Shows that if GA uses not only the selection operator to select better chromosomes for the next generation but also the crossover and mutation operators as the transition operators to generate new chromosomes for the population, this makes it possible for GA to find better candidate solutions. In this example, the average objective value of all the chromosomes will be increased from 1.5 to 3, while the best objective value will exceed 3.\n",
        "\n",
        "![Genetic Algorithm](Pictures/ga3.png \"Genetic Algorithm\")\n",
        "\n",
        "-   The strategy of the transition operator of GA. (a) How the crossover operator works. (b) How the mutation operator works.\n",
        "\n",
        "-   GA will typically apply the crossover and mutation operators to the chromosomes selected by the selection operator.\n",
        "\n",
        "-   Two-dimensional landscape will be used as compared to one dimensional used in traditional methods, simulated annealing or tabu search.\n",
        "\n",
        "![Genetic Algorithm](Pictures/ga4.png \"Genetic Algorithm\")\n",
        "\n",
        "# GA Algorithm\n",
        "\n",
        "-   Initialization: Each chromosome represents a potential solution. In our case, it's a binary vector indicating whether a DC is selected or not.\n",
        "-   Fitness Function: The fitness of a solution is calculated based on how many plants can be served by the selected DCs and whether the total investment remains within the budget.\n",
        "-   Selection: Select a set of solutions based on their fitness to proceed to the next generation.\n",
        "-   Crossover: Perform crossover between pairs of selected solutions to create new offspring solutions.\n",
        "-   Mutation: Randomly mutate some solutions to introduce diversity.\n",
        "-   Termination: Stop when a satisfactory solution is found or after a fixed number of generations.\n",
        "\n",
        "![Genetic Algorithm](Pictures/ga5.png \"Genetic Algorithm\")\n",
        "\n",
        "# The Probability Calculation\n",
        "\n",
        "-   The probability of each chromosome being elected as a parent can be computed as follows: $$p_i = \\frac{f_i}{\\sum_{j=1}^{m} f_j}$$\n",
        "-   Fitness $f_i$ of member i as a proportion of sum of fitness of all population members, suggesting that $p_i = \\frac{\\text{fit}}{\\sum \\text{fit}}$\n",
        "\n",
        "## Refining Genetic Algorithms\n",
        "\n",
        "-   There’s no perfect setup for GA parameters. Experiment with combinations and monitor performance to find the best results for your specific problem.\n",
        "-   GA Parameters Are Crucial: The effectiveness of GA depends heavily on carefully tuning parameters like population size, mutation rate, and the number of generations.\n",
        "-   Population Size: For more complex problems, use larger populations (e.g., 1000–2000 individuals)\n",
        "-   Mutation Rate: A low probability, typically 0.001 to 0.002 per gene.\n",
        "-   Number of Generations: For more generations (e.g., 1000–2000 generations).\n",
        "-   Fitness Function: Carefully tailored to the specific problem being solved, as it guides the selection process.\n",
        "-   Selection Method: rank-based linear selection or proportional selection can be used.\n",
        "-   Crossover Mechanism: Single-point crossover chosen at a random location\n",
        "\n",
        "## Fitness Function Alternatives in Heuristic Models\n",
        "\n",
        "-   Fitness measures and selection mechanisms together determine the quality of offspring (solutions) in evolutionary or heuristic algorithms.\n",
        "\n",
        "    -   While fitness measures are problem-specific, several reasonable alternatives often exist.\n",
        "    -   It's essential to tailor the fitness function to the specific goals of the problem (e.g., precision, speed, or robustness).\n",
        "\n",
        "-   Selection strategies (e.g., roulette wheel, tournament selection) could also have significant impacts on solution quality, and finding the right balance often requires experimentation.\n",
        "\n",
        "-   Selecting the Right Fitness Function: Some fitness measures work significantly better than others depending on the problem.\n",
        "\n",
        "-   Leverage prior knowledge or insights from similar problems to guide fitness function selection.\n",
        "\n",
        "-   Trial and Error: Expect to experiment with different fitness measures to find the one that best suits your specific problem.\n",
        "\n",
        "-   Common Distance Measures for Comparing Solutions to a Target:\n",
        "\n",
        "    -   Sum of Squared Differences: A measure of the total squared deviation of each pixel from the target, commonly used when larger errors should be penalized more.\n",
        "    -   Euclidean Distance: The square root of the sum of squared differences, providing a more intuitive \"distance\" measure in the image space.\n",
        "    -   Sum of Absolute Pixel Differences: A simpler alternative that sums the absolute differences for each pixel, often less sensitive to outliers.\n",
        "    -   Maximum Absolute Pixel Difference: Focuses on the largest deviation, highlighting the worst pixel match.\n",
        "\n",
        "## Approaches to Optimize the Mutation Rate\n",
        "\n",
        "-   Hyperparameter Tuning: Use various fixed mutation rates and evaluate the performance of the GA over multiple runs to identify the best one. This is the simplest method but can be time-consuming as it involves manual experimentation.\n",
        "-   Dynamic Mutation Rate: Adjust the mutation rate dynamically during the evolution process. For example, you can start with a high mutation rate to encourage exploration and gradually reduce it as the algorithm converges.\n",
        "-   Self-Adaptive Mutation Rate: Introduce a mechanism in the GA where each individual in the population has its own mutation rate, which evolves over time. The mutation rate itself becomes part of the genetic material.\n",
        "-   Cross-Validation: Use techniques like k-fold cross-validation to evaluate the impact of different mutation rates and find the one that generalizes the best.\n",
        "\n",
        "## Comparing GA to SA\n",
        "\n",
        "-   Multiple search directions: Compared to the single-solution-based metaheuristic algorithms (e.g., SA) that search only one solution at a time, GA searches for more than one solution at a time during the convergence process. Since GA will search for multiple directions or regions at a time, its search diversity will normally be much higher than single-solution-based metaheuristics that search for only one direction or region at a time during the convergence process. Selection operator: Another characteristic of GA is that it uses the selection and fitness function operators to determine solutions to be searched, not just based on the objective value of each solution. This kind of mechanism keeps the search process of GA from looking for the best solution in the population all the time so that it will not always choose the solution with the best objective or fitness value to search its neighbors again and again. Consequently, GA will not easily get stuck in local optima at early iterations.\n",
        "-   Crossover operator: This operator is one of the transition operators of GA, which plays the role of exchanging information between parent chromosomes, such as moving portions of the genes of a chromosome to another. This kind of mechanism allows GA to restructure its solutions to form new solutions in such a way that the structure of the new solutions is not confined to the structure of the initial solutions and may even inherit partial structures from their parents; as a consequence, the search process of GA will quickly jump from one region to another in the solution space during the convergence process.\n",
        "-   Mutation operator: This operator is another transition operator of GA, which ensures that the search process of GA is capable of escaping from a local optimum by changing the value of some genes randomly. Of course, this kind of mechanism will also play the role of fine-tuning the chromosomes of GA because only a few genes will be changed at a time.\n",
        "\n",
        "# Genetic Algorithms\n",
        "\n",
        "-   To compare Simulated Annealing (SA) with Genetic Algorithms (GA), we can implement a Genetic Algorithm for the same optimization problem (Ackley function) and then compare the two approaches based on:\n",
        "    -   Performance: Compare the final optimized values.\n",
        "    -   Convergence Speed: How fast each algorithm converges to a solution.\n",
        "    -   Exploration vs. Exploitation: How each method balances searching new areas (exploration) versus refining the current solution (exploitation).\n",
        "\n",
        "### Genetic Algorithm for Ackley Function\n",
        "\n",
        "-   The key components of a genetic algorithm include:\n",
        "    -   Population: A set of candidate solutions.\n",
        "    -   Selection: Selecting parents based on their fitness (Ackley function value).\n",
        "    -   Crossover: Combining parents to produce offspring.\n",
        "    -   Mutation: Introducing random changes to maintain diversity.\n",
        "    -   Fitness Function: The objective function we are minimizing, which is the Ackley function in this case.\n",
        "\n",
        "### Comparison with Simulated Annealing (SA)\n",
        "\n",
        "-   Population-based vs. Single Solution:\n",
        "    -   GA maintains and evolves a population of solutions.\n",
        "    -   SA operates on a single solution and modifies it over time.\n",
        "-   Exploration Strategy:\n",
        "    -   GA uses crossover and mutation to explore the solution space, encouraging diversity.\n",
        "    -   SA explores the space by making probabilistic changes to a single solution, allowing uphill moves early on (controlled by the temperature).\n",
        "-   Deterministic vs. Probabilistic:\n",
        "    -   GA uses deterministic selection mechanisms, such as tournament selection, and a fixed mutation rate.\n",
        "    -   SA uses a temperature mechanism to probabilistically accept worse solutions early in the search.\n",
        "-   Convergence:\n",
        "    -   GA uses generations and selection pressure to converge on the best solution over time.\n",
        "    -   SA cools down gradually, converging based on the temperature schedule.\n",
        "\n",
        "# GA Example Ackley\n",
        "\n",
        "## Startup Commands\n"
      ],
      "id": "337b21f0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Genetic Algorithm Ackley Example (1D)\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the necessary global variables\n",
        "pop_size = 50\n",
        "num_generations = 100\n",
        "crossover_rate = 0.7\n",
        "mutation_rate = 0.02\n",
        "num_parents_mating = 10\n",
        "num_players = 3\n",
        "best_obj_val = float('inf')\n",
        "best_sol = None\n",
        "\n",
        "# Ackley function (1D)\n",
        "def ackley(x):\n",
        "    a = 20\n",
        "    b = 0.2\n",
        "    c = 2 * np.pi\n",
        "    term1 = -a * np.exp(-b * np.sqrt(np.mean(np.square(x))))\n",
        "    term2 = -np.exp(np.mean(np.cos(c * np.array(x))))\n",
        "    return term1 + term2 + a + np.exp(1)"
      ],
      "id": "51c5d6d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initiation Function (I)\n",
        "\n",
        "-   A population of solutions is initialized using the init_ga function, where each solution is a random number between -10 and 10, as per the problem domain.\n"
      ],
      "id": "afd48793"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initialization function (I) to set the starting point\n",
        "def init_ga(pop_size):\n",
        "    return np.random.uniform(-10, 10, (pop_size, 1))"
      ],
      "id": "b8994f12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Function (E)\n",
        "\n",
        "-   The fitness of each solution in the population is evaluated using the Ackley function.The best solution found so far is updated by comparing the current population's fitness values.\n"
      ],
      "id": "62e01e74"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Evaluation function (E)\n",
        "def evaluate(pop):\n",
        "    return np.array([ackley(ind) for ind in pop])"
      ],
      "id": "b3ef4e4c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Determination (D): Update Function\n",
        "* This function, update_best_sol, is responsible for keeping track of the best solution found so far in a genetic algorithm. It compares the fitness values of the current population (curr_obj_vals) to the global best objective value (best_obj_val).\n",
        "     + It identifies the index (best_idx) of the individual in the current population with the lowest objective value (best fitness) using np.argmin.\n",
        "     + If this individual's objective value is better (lower) than the current global best (best_obj_val), it updates best_obj_val to this new lower value and updates best_sol to the corresponding individual from the current population.\n",
        "* This ensures that best_sol and best_obj_val always store the best solution and its fitness value found across all generations during the algorithm's execution. The use of global allows the function to modify these variables outside its local scope.\n"
      ],
      "id": "82d892c2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Update best solution function\n",
        "def update_best_sol(curr_pop, curr_obj_vals):\n",
        "    global best_sol, best_obj_val\n",
        "    best_idx = np.argmin(curr_obj_vals)\n",
        "    if curr_obj_vals[best_idx] < best_obj_val:\n",
        "        best_obj_val = curr_obj_vals[best_idx]\n",
        "        best_sol = curr_pop[best_idx]"
      ],
      "id": "955c275f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Determination (D): Selection Function\n",
        "\n",
        "-   Selection: A subset of the population is selected for reproduction using a tournament selection process. This selection favors individuals with better fitness, helping to propagate good solutions.\n",
        "     + This code implements a tournament selection mechanism in a genetic algorithm to select individuals for reproduction based on their fitness values. \n",
        "     + For each individual in the current population (curr_pop), a tournament is conducted by randomly selecting a subset of individuals (of size num_players) without replacement. \n",
        "     + Within this subset, the individual with the best fitness value (lowest objective value in curr_obj_vals) is identified using np.argmin and added to the selected population (selected_pop). This process ensures that fitter individuals have a higher chance of being chosen, promoting the propagation of good solutions while maintaining diversity through randomness. \n",
        "     + The function returns the newly selected population as a NumPy array.\n"
      ],
      "id": "cb25ad6c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Determination (D)\n",
        "# Selection function using tournament selection\n",
        "def select(curr_pop, curr_obj_vals, num_players):\n",
        "    selected_pop = []\n",
        "    for _ in range(len(curr_pop)):\n",
        "        tournament = np.random.choice(len(curr_pop), num_players, replace=False)\n",
        "        best_idx = tournament[np.argmin(curr_obj_vals[tournament])]\n",
        "        selected_pop.append(curr_pop[best_idx])\n",
        "    return np.array(selected_pop)"
      ],
      "id": "eea272a3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transition (T): Crossover Function\n",
        "* In genetic algorithms, the transition function refers to the combined operations of crossover and mutation that generate a new population (offspring) from the current population. The transition function first applies crossover to exchange genetic material between selected parent solutions, creating new offspring, and then applies mutation to introduce small random changes in the offspring, maintaining diversity and enabling exploration of the solution space. This ensures a balance between exploitation of good solutions (via crossover) and exploration of new solutions (via mutation).\n",
        "\n",
        "-   Crossover: Pairs of solutions from the selected population are combined to create new offspring by mixing parts of the parent solutions.\n",
        "* A new population (new_pop) is initialized as a copy of the current population (pop).\n",
        "The loop iterates over the population in pairs. For each pair, a random number is generated using np.random.rand(). If this number is less than the crossover_rate, crossover occurs:\n",
        "     + A crossover point is chosen randomly (here, using np.random.randint(1)), which specifies where the two parent solutions will exchange segments. The segments of the two parent solutions from the crossover point onward are swapped, creating two new offspring solutions. \n",
        "     + If the random number is greater than the crossover_rate, no crossover occurs, and the parent solutions remain unchanged. The function returns the new_pop containing the modified population after applying the crossover operation."
      ],
      "id": "b8acd6b7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Transition function (T): Crossover and Mutation\n",
        "# Crossover function (T)\n",
        "def crossover(pop, crossover_rate):\n",
        "    new_pop = pop.copy()\n",
        "    for i in range(0, len(pop) - 1, 2):\n",
        "        if np.random.rand() < crossover_rate:\n",
        "            crossover_point = np.random.randint(1)\n",
        "            new_pop[i, crossover_point:], new_pop[i + 1, crossover_point:] = (\n",
        "                pop[i + 1, crossover_point:], \n",
        "                pop[i, crossover_point:]\n",
        "            )\n",
        "    return new_pop"
      ],
      "id": "958569ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transition (T): Mutation Function\n",
        "\n",
        "-   Mutation: Random mutations are introduced in the offspring solutions to maintain diversity and avoid premature convergence.\n",
        "* It introduces random changes to a population of solutions in a genetic algorithm. It iterates over each individual in the population (pop) and, with a probability specified by the mutation_rate, replaces the current individual with a new value randomly drawn from a uniform distribution between -10 and 10. This random alteration helps maintain diversity in the population, enabling the genetic algorithm to explore new areas of the solution space and avoid premature convergence. The modified population is returned at the end of the function.\n"
      ],
      "id": "d87b5f25"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Mutation function (M)\n",
        "def mutation(pop, mutation_rate):\n",
        "    for i in range(len(pop)):\n",
        "        if np.random.rand() < mutation_rate:\n",
        "            pop[i] = np.random.uniform(-10, 10)\n",
        "    return pop"
      ],
      "id": "62a6ae7e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Loop\n",
        "\n",
        "-   The algorithm iterates through a fixed number of generations (iterations). Each generation represents an iteration of evolving the population to improve the solutions.\n",
        "-   The new population (after crossover and mutation) replaces the old one for the next generation.\n"
      ],
      "id": "ee9c030a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Genetic Algorithm function (D)\n",
        "def genetic_algorithm():\n",
        "    global best_sol, best_obj_val\n",
        "    pop = init_ga(pop_size)\n",
        "    \n",
        "    for generation in range(num_generations):\n",
        "        obj_vals = evaluate(pop)\n",
        "        update_best_sol(pop, obj_vals)\n",
        "        \n",
        "        selected_pop = select(pop, obj_vals, num_players)\n",
        "        offspring_pop = crossover(selected_pop, crossover_rate)\n",
        "        mutated_pop = mutation(offspring_pop, mutation_rate)\n",
        "        \n",
        "        pop = mutated_pop  \n",
        "    return best_sol, best_obj_val"
      ],
      "id": "7d7c582c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Execution\n"
      ],
      "id": "d9351d1c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Main execution\n",
        "start_time = time.time()\n",
        "best_solution, best_value = genetic_algorithm()\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time"
      ],
      "id": "766c844e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Output (O)\n"
      ],
      "id": "b384fc76"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Output (O)\n",
        "print(f\"Optimal solution: {best_solution}\")\n",
        "print(f\"Optimal value: {best_value}\")\n",
        "print(f\"Execution time for Genetic Algorithm: {execution_time:.6f} seconds\")\n",
        "\n",
        "# Plot the Ackley function and genetic algorithm progress\n",
        "x_values = np.linspace(-10, 10, 1000)\n",
        "y_values = [ackley([x]) for x in x_values]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(x_values, y_values, label=\"Ackley Function\", color='b')\n",
        "plt.scatter(best_solution[0], best_value, color='green', label='GA Best Solution', s=100)\n",
        "plt.title(\"Ackley Function in 1D with Genetic Algorithm Best Solution\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "id": "03c57d87",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GA example One Max\n"
      ],
      "id": "da794bb0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# Define the necessary global variables\n",
        "num_bits = 10\n",
        "pop_size = 50\n",
        "num_generations = 100\n",
        "crossover_rate = 0.7\n",
        "mutation_rate = 0.01\n",
        "num_players = 3  \n",
        "\n",
        "# Evaluation Function (E)\n",
        "def one_max(sol):\n",
        "    return sum(sol)\n",
        "\n",
        "# Initialization function (I)\n",
        "def initialize_population(pop_size, num_bits):\n",
        "    return np.random.randint(0, 2, (pop_size, num_bits))\n",
        "\n",
        "# Selection function using tournament selection (D)\n",
        "def select(population, fitness_vals, num_players):\n",
        "    selected_pop = []\n",
        "    for _ in range(len(population)):\n",
        "        tournament = np.random.choice(len(population), num_players, replace=False)\n",
        "        best_idx = tournament[np.argmax(fitness_vals[tournament])]\n",
        "        selected_pop.append(population[best_idx])\n",
        "    return np.array(selected_pop)\n",
        "\n",
        "# Crossover function (T)\n",
        "def crossover(population, crossover_rate):\n",
        "    new_population = population.copy()\n",
        "    for i in range(0, len(population) - 1, 2):\n",
        "        if random.random() < crossover_rate:\n",
        "            crossover_point = np.random.randint(1, num_bits)\n",
        "            new_population[i, crossover_point:], new_population[i + 1, crossover_point:] = (\n",
        "                population[i + 1, crossover_point:], \n",
        "                population[i, crossover_point:]\n",
        "            )\n",
        "    return new_population\n",
        "\n",
        "# Mutation function (T)\n",
        "def mutate(population, mutation_rate):\n",
        "    for i in range(len(population)):\n",
        "        for j in range(num_bits):\n",
        "            if random.random() < mutation_rate:\n",
        "                population[i, j] = 1 - population[i, j]  # Flip the bit\n",
        "    return population\n",
        "\n",
        "# Genetic Algorithm function\n",
        "def genetic_algorithm(num_bits, pop_size, num_generations, crossover_rate, mutation_rate):\n",
        "    population = initialize_population(pop_size, num_bits)\n",
        "    best_sol, best_value = None, 0\n",
        "    value_history = []\n",
        "\n",
        "    for generation in range(num_generations):\n",
        "        fitness_vals = np.array([one_max(ind) for ind in population])\n",
        "        best_idx = np.argmax(fitness_vals)\n",
        "        \n",
        "        #Update solution (D)\n",
        "        if fitness_vals[best_idx] > best_value:\n",
        "            best_sol, best_value = population[best_idx], fitness_vals[best_idx]\n",
        "        \n",
        "        value_history.append(best_value)\n",
        "        \n",
        "        selected_pop = select(population, fitness_vals, num_players)\n",
        "        offspring_pop = crossover(selected_pop, crossover_rate)\n",
        "        population = mutate(offspring_pop, mutation_rate)\n",
        "\n",
        "    return best_sol, best_value, value_history\n",
        "\n",
        "# Main execution\n",
        "start_time = time.time()\n",
        "best_sol, best_value, value_history = genetic_algorithm(num_bits, pop_size, num_generations, crossover_rate, mutation_rate)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "\n",
        "# Output results (O)\n",
        "print(f\"Best solution: {best_sol}\")\n",
        "print(f\"Best fitness (one-max value): {best_value}\")\n",
        "print(f\"Execution time: {execution_time:.6f} seconds\")\n",
        "\n",
        "# Plot the progress of the genetic algorithm\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(value_history, marker='o', linestyle='-', color='b', label='Best One-Max Value')\n",
        "plt.title(\"Genetic Algorithm Progress for One-Max Problem\")\n",
        "plt.xlabel(\"Generation\")\n",
        "plt.ylabel(\"Best Fitness\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "3a3441aa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using AI\n",
        "\n",
        "-   Use the following prompt on a generative AI, like chatGPT, to learn more about the topics covered.\n",
        "-   Concept of Genetic Algorithms: Explain the biological inspiration behind Genetic Algorithms. How do the concepts of selection, crossover, and mutation in GA mimic natural evolution?\n",
        "-   Write a Python implementation of a Genetic Algorithm to minimize a simple quadratic function $f(x) = x^2$ include steps for initialization, selection, crossover, mutation, and termination. How does the choice of parameters like mutation rate and population size affect the outcome?\n",
        "-   Comparison with Simulated Annealing: Compare Genetic Algorithms to Simulated Annealing for solving the Ackley function. Discuss their strengths and weaknesses in terms of convergence speed, exploration, and exploitation.\n",
        "-   Real-World Applications: Identify three real-world problems where Genetic Algorithms are commonly used (e.g., scheduling, vehicle routing, or portfolio optimization). Why are GAs particularly suited for these problems?\n",
        "-   Parameter Tuning: Discuss how to determine the optimal mutation rate and crossover probability for a given problem. Explore techniques like self-adaptive mutation rates and their impact on performance.\n",
        "-   How does the diversity of the population change as the genetic algorithm progresses over generations?\n",
        "\n",
        "# Conclusions\n",
        "\n",
        "-   Our discussions on Genetic Algorithms (GA) provide a comprehensive exploration of one of the most versatile and biologically inspired optimization techniques. GA mimics the principles of natural selection and evolution, employing operators like selection, crossover, and mutation to iteratively refine solutions within a population. The method’s ability to balance exploration (via genetic diversity) and exploitation (via selection pressure) makes it particularly effective for solving complex, non-linear, and multimodal optimization problems. Key advantages include its flexibility to adapt to a wide range of applications, such as scheduling, portfolio optimization, and function minimization, and its capacity to avoid local optima by maintaining a diverse population. However, success depends heavily on careful parameter tuning, including population size, mutation rate, and crossover probability. By comparing GA with other methods like Simulated Annealing, the slides highlight GA's strengths in global search and parallel exploration. Ultimately, Genetic Algorithms emerge as a powerful heuristic, especially when the optimization landscape demands both creativity and robustness in the search for solutions."
      ],
      "id": "d7d457ba"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}