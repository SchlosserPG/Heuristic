[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Heuristic Modelling",
    "section": "",
    "text": "Course Overview",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Heuristic Algorithms</span>"
    ]
  },
  {
    "objectID": "index.html#course-goals",
    "href": "index.html#course-goals",
    "title": "Heuristic Modelling",
    "section": "Course Goals",
    "text": "Course Goals\n\nDevelop a solid process for algorithm development.\nEnhance Python programming skills.\nUnderstand the structure of heuristic models, focusing on:\n\nHill climbing\nSimulated annealing\nGenetic algorithms",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Heuristic Algorithms</span>"
    ]
  },
  {
    "objectID": "index.html#required-book",
    "href": "index.html#required-book",
    "title": "Heuristic Modelling",
    "section": "Required Book",
    "text": "Required Book\nHandbook of Metaheuristic Algorithms\nAuthors: Chun-Wei Tsai & Ming-Chao Chiang\nPublisher: Academic Press\n\n\nAccess the book free through O‚ÄôReilly‚Äôs website with school credentials.\nPython code is available on the author‚Äôs GitHub.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Heuristic Algorithms</span>"
    ]
  },
  {
    "objectID": "algo.html",
    "href": "algo.html",
    "title": "Algorithm Design and Pseudocode",
    "section": "",
    "text": "Automate This by Christopher Steiner",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#algorithms-key-concepts-and-examples",
    "href": "algo.html#algorithms-key-concepts-and-examples",
    "title": "Algorithm Design and Pseudocode",
    "section": "Algorithms Key Concepts and Examples",
    "text": "Algorithms Key Concepts and Examples\n\nAlgorithms in Finance: High-Frequency Trading (HFT) revolutionized Wall Street by executing trades at lightning speeds, leading to both massive profits and new risks.\nAlgorithms in Healthcare: Algorithms that diagnose diseases faster and more accurately than doctors.\nMusic: Algorithms used by platforms like Pandora to predict and recommend songs.\nImpact on Jobs: Automation‚Äôs role in replacing jobs traditionally done by humans, particularly in industries like finance, journalism, and even art.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#future-implications-and-ethical-considerations",
    "href": "algo.html#future-implications-and-ethical-considerations",
    "title": "Algorithm Design and Pseudocode",
    "section": "Future Implications and Ethical Considerations",
    "text": "Future Implications and Ethical Considerations\n\nExpansion of Algorithms: The growing reach of algorithms in decision-making processes, from hiring practices to legal judgments.\nEthical Concerns: The potential for bias in algorithms and the importance of transparency in their design. The need for regulation and oversight as algorithms increasingly influence critical aspects of life.\nLooking Ahead: Steiner‚Äôs call for society to adapt to the new algorithm-driven world, balancing innovation with ethical responsibility.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#bias-in-algorithms-explored",
    "href": "algo.html#bias-in-algorithms-explored",
    "title": "Algorithm Design and Pseudocode",
    "section": "Bias in Algorithms Explored",
    "text": "Bias in Algorithms Explored\n\nFacial Recognition Technology algorithms have shown significant biases, particularly in accurately identifying people of different races and genders.\n\nStudies have found that some facial recognition systems have higher error rates when identifying individuals with darker skin tones and women. This can lead to discriminatory outcomes, such as misidentifying people of color at higher rates than white individuals.\nImpact: This bias can result in wrongful accusations or the exclusion of certain groups from services that rely on facial recognition technology.\n\nHiring algorithms are used by companies to screen job applicants, but they can unintentionally perpetuate biases present in the training data.\n\nA famous case involved an AI hiring tool developed by Amazon, which was found to be biased against female applicants. The algorithm was trained on resumes submitted over the previous decade, which were predominantly from male applicants, leading the AI to favor male candidates.\nImpact: This bias can reinforce gender inequalities in the workplace by systematically disadvantaging qualified female applicants.\n\nPredictive Policing algorithms analyze historical crime data to predict where future crimes are likely to occur, influencing law enforcement patrols.\n\nThese algorithms often reflect existing biases in policing practices, such as disproportionately targeting minority neighborhoods. Because the training data may contain biased policing patterns, the algorithm can perpetuate over-policing in certain communities.\nImpact: This can lead to a cycle of increased surveillance and criminalization of specific racial or ethnic groups, reinforcing systemic biases in the criminal justice system.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#algorithm-development-process",
    "href": "algo.html#algorithm-development-process",
    "title": "Algorithm Design and Pseudocode",
    "section": "Algorithm Development Process:",
    "text": "Algorithm Development Process:\n\nWhat should you do first?\n0: Think of a conceptual approach\n1: Write step-by-step outline\n\nIn words (maybe pseudo code)\nFocus on sound logic\n\n2: Plan programming implementation\n\nChoose appropriate data structures + Speed, memory, convenience\nConsider functions, which kind of loops\n\n3: Write the program and debug\n\nUse the outline as comment statements\nWrite your algorithms in chunks, write a line, then test it\n\n\n\n\n\nProcess",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#understanding-the-structure-of-algorithms",
    "href": "algo.html#understanding-the-structure-of-algorithms",
    "title": "Algorithm Design and Pseudocode",
    "section": "Understanding the Structure of Algorithms",
    "text": "Understanding the Structure of Algorithms\n\nIdentify the Purpose:\n\nStart by understanding what the algorithm is supposed to achieve.\nLook for a brief description or goal at the beginning.\n\nBreak Down the Steps:\n\nAlgorithms are typically presented as a sequence of steps.\nEach step corresponds to a specific action or decision.\nin words. Focus on sound logic.\n\nRecognize Input and Output:\n\nDetermine what inputs the algorithm requires.\nIdentify the expected output(s) after the algorithm is executed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#understanding-the-structure-of-algorithms-flow-control",
    "href": "algo.html#understanding-the-structure-of-algorithms-flow-control",
    "title": "Algorithm Design and Pseudocode",
    "section": "Understanding the Structure of Algorithms: Flow Control",
    "text": "Understanding the Structure of Algorithms: Flow Control\n\nDetermine how the algorithm progresses through its steps and notice the flow control structures like loops (for, while) and conditionals (if, else).\n\nWhich data types should we use?\n\nBegin to consider speed, memory usage, convenience tradeoffs\n\nFiner details of data organization\n\nWhat fields for dictionary labels?\nHow should lists be sorted, if at all?\n\nLoop types\n\nShould we use a for or while loop?\nHow to we increase the ease of coding and development?\n\nShould we use functions?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#deciphering-algorithmic-notation-pseudocode-vs.-code",
    "href": "algo.html#deciphering-algorithmic-notation-pseudocode-vs.-code",
    "title": "Algorithm Design and Pseudocode",
    "section": "Deciphering Algorithmic Notation: Pseudocode vs.¬†Code",
    "text": "Deciphering Algorithmic Notation: Pseudocode vs.¬†Code\n\nRecognize that many algorithms are written in pseudocode, a high-level description that isn‚Äôt tied to any specific programming language.\nSome feel that with Python, the pseudo-code step would not be necessary anymore\nTranslate pseudocode into actual code if needed.\n\nProgram in chunks\nTest each chunk before moving on\nUse functions where reasonable\n\nEasier to test\n\n\nCheck code where solutions are known\n\nSmall problems\nObvious solutions\n\nDebug Often\n\nBreakpoints\nCheck variables in console\nVariable explorer\nPrint statements",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#deciphering-algorithmic-notation-mathematical-symbols",
    "href": "algo.html#deciphering-algorithmic-notation-mathematical-symbols",
    "title": "Algorithm Design and Pseudocode",
    "section": "Deciphering Algorithmic Notation: Mathematical Symbols",
    "text": "Deciphering Algorithmic Notation: Mathematical Symbols\n\nMathematical Symbols:\n\nAlgorithms often include mathematical notation, such as sums (\\(\\sum\\)) or products (\\(\\prod\\)).\nUnderstand these as they relate to the algorithm‚Äôs operations.\n\nBig-O Notation:\n\nLook for references to Big-O notation, which indicates the algorithm‚Äôs efficiency in terms of time or space.\nUnderstand the implications for performance, especially with large inputs.\n\nCommenting\n\nCommenting improves code readability, reuse, and maintainability.\nCan transfer algorithm outline to a program as comments.\nThen, you have comments that provide an outline for coding.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#design-paradigms",
    "href": "algo.html#design-paradigms",
    "title": "Algorithm Design and Pseudocode",
    "section": "Design Paradigms",
    "text": "Design Paradigms\n\nAlgorithms can be based on:\n\nIntuitive ideas\nMath\n\nOptimization\n\nPossibly with calculus analysis (e.g., gradients)\nHeuristic model analysis\nDynamic programming",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#common-symbols-in-algorithms",
    "href": "algo.html#common-symbols-in-algorithms",
    "title": "Algorithm Design and Pseudocode",
    "section": "Common Symbols in Algorithms",
    "text": "Common Symbols in Algorithms\n\nMathematical Symbols:\n\n\\(\\forall\\): ‚ÄúFor all‚Äù, used in universal quantification.\n\\(\\exists\\): ‚ÄúThere exists‚Äù, used in existential quantification.\n\\(\\sum_{i=1}^n x_i\\): Summation from \\(i = 1\\) to \\(n\\).\n\\(\\prod_{i=1}^n x_i\\): Product from \\(i = 1\\) to \\(n\\).\n\nLogical Symbols:\n\n\\(\\land\\): Logical AND.\n\\(\\lor\\): Logical OR.\n\\(\\neg\\): Logical NOT.\n\\(\\implies\\): Logical implication.\n\nAlgorithm-Specific Notation:\n\n\\(O(n)\\): Big-O notation, representing algorithm complexity.\n\\(P \\leftarrow Q\\): Assign the value of \\(Q\\) to \\(P\\).\nfor \\(i = 1\\) to \\(n\\): A loop from \\(i = 1\\) to \\(n\\).\nif \\((condition)\\): A conditional statement.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#summation-and-loop-example",
    "href": "algo.html#summation-and-loop-example",
    "title": "Algorithm Design and Pseudocode",
    "section": "Summation and Loop Example",
    "text": "Summation and Loop Example\n\nSummation Formula: \\(S = \\sum_{i=1}^{n} i = 1 + 2 + 3 + \\dots + n\\)\n\nExplanation: This formula calculates the sum of the first \\(n\\) natural numbers. The symbol \\(\\sum\\) represents the summation, and \\(i\\) is the index that runs from 1 to \\(n\\).\n\nNested Loop Calculation: \\(T = \\sum_{i=1}^{n} \\sum_{j=1}^{m} (i \\times j)\\)\n\nExplanation: This formula represents a nested summation where \\(i\\) runs from 1 to \\(n\\) and \\(j\\) runs from 1 to \\(m\\). It calculates the sum of the product of \\(i\\) and \\(j\\) over these ranges.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#conditional-statements-and-recursion",
    "href": "algo.html#conditional-statements-and-recursion",
    "title": "Algorithm Design and Pseudocode",
    "section": "Conditional Statements and Recursion",
    "text": "Conditional Statements and Recursion\n\nConditional Formula: \\[\nf(x) =\n\\begin{cases}\n0 & \\text{if } x &lt; 0 \\\\\n1 & \\text{if } x \\geq 0\n\\end{cases}\n\\]\n\nExplanation: This piecewise function returns 0 if ( x ) is less than 0, and 1 if ( x ) is greater than or equal to 0. It‚Äôs an example of a conditional statement in algorithmic form.\n\nRecursive Formula: \\[\nF(n) =\n\\begin{cases}\n1 & \\text{if } n = 1 \\\\\nn \\times F(n-1) & \\text{if } n &gt; 1\n\\end{cases}\n\\]\n\nExplanation: This is a recursive definition of the factorial function. For ( n = 1 ), ( F(n) = 1 ). For ( n &gt; 1 ), ( F(n) ) is defined as ( n ) times the factorial of ( n-1 ).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#big-o-notation-and-algorithm-complexity",
    "href": "algo.html#big-o-notation-and-algorithm-complexity",
    "title": "Algorithm Design and Pseudocode",
    "section": "Big-O Notation and Algorithm Complexity",
    "text": "Big-O Notation and Algorithm Complexity\n\nBig-O Notation Example: \\(T(n) = O(n^2)\\)\n\nExplanation: This formula describes the time complexity of an algorithm, where\\(T(n)\\) represents the runtime as a function of input size \\(n\\). The notation \\(O(n^2)\\) indicates that the algorithm‚Äôs runtime grows quadratically with the size of the input.\n\nLogarithmic Complexity: \\(T(n) = O(\\log n)\\)\n\nExplanation: This formula represents an algorithm with logarithmic time complexity. The runtime increases logarithmically as the input size \\(n\\) increases, which is common in algorithms like binary search.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#basic-tsp-formulation",
    "href": "algo.html#basic-tsp-formulation",
    "title": "Algorithm Design and Pseudocode",
    "section": "Basic TSP Formulation",
    "text": "Basic TSP Formulation\n\nObjective Function: \\[\n\\text{Minimize} \\quad Z = \\sum_{i=1}^{n} \\sum_{j=1, j \\neq i}^{n} c_{ij} x_{ij}\n\\]\n\nExplanation: This formula represents the objective function of the TSP, where \\(c_{ij}\\) is the cost (or distance) of traveling from city \\(i\\) to city \\(j\\), and \\(x_{ij}\\) is a binary variable that equals 1 if the path from \\(i\\) to \\(j\\) is included in the solution and 0 otherwise. The goal is to minimize the total travel cost.\n\nConstraints:\n\\(\\sum_{j=1, j \\neq i}^{n} x_{ij} = 1 \\quad \\forall i\\)\n\\(\\sum_{i=1, i \\neq j}^{n} x_{ij} = 1 \\quad \\forall j\\)\n\\(x_{ij} \\in \\{0, 1\\}\\)\n\nExplanation: The first constraint ensures that each city \\(i\\) is exited exactly once, and the second constraint ensures that each city \\(j\\) is entered exactly once. The binary constraint on \\(x_{ij}\\) ensures that the solution only includes valid paths.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#tsp-approximation-algorithm",
    "href": "algo.html#tsp-approximation-algorithm",
    "title": "Algorithm Design and Pseudocode",
    "section": "TSP Approximation Algorithm",
    "text": "TSP Approximation Algorithm\n\nApproximation Algorithm Cost: \\(Z \\leq 2 \\times \\text{OPT}\\)\n\nExplanation: This formula represents the performance guarantee of a 2-approximation algorithm for the TSP, where \\(Z\\) is the cost of the approximate solution and \\(\\text{OPT}\\) is the cost of the optimal solution. It guarantees that the approximate solution will be at most twice as costly as the optimal solution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#heuristic-nearest-neighbor-example",
    "href": "algo.html#heuristic-nearest-neighbor-example",
    "title": "Algorithm Design and Pseudocode",
    "section": "Heuristic Nearest Neighbor Example:",
    "text": "Heuristic Nearest Neighbor Example:\n\\(Z = \\sum_{i=1}^{n-1} c_{i, \\text{NN}(i)} + c_{n, \\text{NN}(1)}\\)\n\nExplanation: This is the cost calculation for the nearest neighbor heuristic, where \\(\\text{NN}(i)\\) denotes the nearest neighbor of city \\(i\\). The tour starts at a city, repeatedly visits the nearest unvisited city, and returns to the starting city.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#activity",
    "href": "algo.html#activity",
    "title": "Algorithm Design and Pseudocode",
    "section": "Activity:",
    "text": "Activity:\n\nState what the following algorithms do:\n\n\\[Z = \\sum_{i=1}^n \\left( x_i \\times y_i \\right)\\] \\[ \\bar{X} = \\frac{\\sum_{i=1}^{n} w_i x_i}{\\sum_{i=1}^{n} w_i}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#tips-for-writing-good-pseudocode",
    "href": "algo.html#tips-for-writing-good-pseudocode",
    "title": "Algorithm Design and Pseudocode",
    "section": "Tips for Writing Good Pseudocode",
    "text": "Tips for Writing Good Pseudocode\n\nMaintain consistent terms throughout: Use the same terminology for variables, functions, and processes. If you define a variable as total, always refer to it as total, not sum later on.\nBe clear on naming: Use descriptive names for variables, functions, and operations to make your pseudocode intuitive.\nPseudocode should be easy to understand after a single read-through. If it feels too complex, break it down further.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#examples",
    "href": "algo.html#examples",
    "title": "Algorithm Design and Pseudocode",
    "section": "Examples",
    "text": "Examples\n\nThis pseudocode finds the smallest number in a given list by iterating through all elements and updating the min_number whenever a smaller number is found.\n\n# Pseudocode for finding the minimum number in a list\nSET min_number = Infinity\nFOR each number IN list:\n    IF number &lt; min_number:\n        SET min_number = number\nRETURN min_number\n\n\nThis pseudocode below calculates the sum of all numbers in the list that are greater than a specified threshold by iterating through the list and adding qualifying numbers to the sum.\nedge cases: an empty list, all even numbers, or no numbers greater than the threshold.\n\n# Pseudocode for summing numbers greater than a given threshold\nSET sum = 0\nFOR each number IN list:\n    IF number &gt; threshold:\n        SET sum = sum + number\nRETURN sum",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#activity-read-this-pseudocode",
    "href": "algo.html#activity-read-this-pseudocode",
    "title": "Algorithm Design and Pseudocode",
    "section": "Activity: Read this Pseudocode",
    "text": "Activity: Read this Pseudocode\nSET min_even = Infinity \nFOR each number IN list: \n    IF number is even AND number &lt; min_even: \n        SET min_even = number \nRETURN min_even",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#pseudocode-for-basic-tsp-formation",
    "href": "algo.html#pseudocode-for-basic-tsp-formation",
    "title": "Algorithm Design and Pseudocode",
    "section": "Pseudocode for Basic TSP Formation",
    "text": "Pseudocode for Basic TSP Formation\n\nGeneric Inputs: The pseudocode is now generic and works for any TSP model where you have a set of cities and their corresponding distances.\nNearest Neighbor Heuristic: The algorithm works by greedily choosing the nearest unvisited city until all cities are visited, then returning to the start city.\nOutput: It outputs the route and the total distance traveled.\n\n#inputs\nSET cities = &lt;list of cities&gt;\nSET distances = &lt;distance matrix or dictionary&gt;\nSET start_city = &lt;initial city&gt;\n\n\n# FUNCTION to find the nearest unvisited city\nFUNCTION find_nearest_neighbor(current_city, unvisited, distances):\n    SET nearest_city = None\n    SET min_distance = infinity\n    \n    FOR each city IN unvisited:\n        IF distances[current_city][city] &lt; min_distance:\n            SET min_distance = distances[current_city][city]\n            SET nearest_city = city\n    \n    RETURN nearest_city, min_distance\n\n# FUNCTION to solve TSP using Nearest Neighbor algorithm\nFUNCTION nearest_neighbor_tsp(start_city, cities, distances):\n# Initialize list of unvisited cities\n    SET unvisited = list of all cities EXCEPT start_city",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "numpy.html",
    "href": "numpy.html",
    "title": "Python for Heuristics & NumPy",
    "section": "",
    "text": "Coding Know-How\nWhat problems does this code have?\nx1 = [0, 1, 2, 3, 4, 5]\nx2 = [6, 7, 8, 9, 10]\n\nprint(sum(x1))\nprint(sum(x2))\n\n15\n40",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#basic-array-creation",
    "href": "numpy.html#basic-array-creation",
    "title": "Python for Heuristics & NumPy",
    "section": "Basic Array Creation",
    "text": "Basic Array Creation\n\nThe np.array function in NumPy is used to create an array (a grid of values) from data provided as lists, tuples, or other array-like structures. The resulting NumPy array is a powerful and flexible structure for mathematical operations, as it supports multiple dimensions, broadcasting, and various data types.\n\n\n# Creating a simple numpy array from a Python list\narray = np.array([1, 2, 3, 4])\nprint(\"Array:\", array)\n\nArray: [1 2 3 4]\n\n\n\nnp.arange() is a NumPy function that generates an array with evenly spaced values within a given range.\n\n\narray = np.arange(1, 5)  # Generates [1, 2, 3, 4]\nprint(\"Array:\", array)\n\nArray: [1 2 3 4]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#element-wise-operations",
    "href": "numpy.html#element-wise-operations",
    "title": "Python for Heuristics & NumPy",
    "section": "Element-Wise Operations",
    "text": "Element-Wise Operations\n\nElement-wise operators are mathematical or logical operations applied independently to corresponding elements in arrays or matrices of the same shape.\nEach element in one array is combined with the corresponding element in the other array using the operator.\nIn the context of arrays (such as in NumPy), common element-wise operators include basic arithmetic operators:\n\nElement-wise addition +\nElement-wise subtraction -\nElement-wise multiplication *\nElement-wise division /\nElement-wise exponentiation **\n\n\n\n# Performing element-wise addition\narray = np.array([1, 2, 3, 4])\nadded_array = array + 5\nprint(\"Added Array:\", added_array)\n\nAdded Array: [6 7 8 9]\n\n\n\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\nc = a + b\nprint(c)\n\n[5 7 9]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#taking-an-exponent-np.exp",
    "href": "numpy.html#taking-an-exponent-np.exp",
    "title": "Python for Heuristics & NumPy",
    "section": "Taking an Exponent: np.exp",
    "text": "Taking an Exponent: np.exp\n\nnp.exp is a function in the NumPy library that calculates the exponential of all elements in an input array. Specifically, it computes the base-e exponential function, which is ùëí^ùë•, where ùëí is Euler‚Äôs number (approximately 2.71828), and ùë• is the input array or scalar.\n\n\n# Applying np.exp to the array\narray = np.array([1, 2, 3, 4])\nexp_array = np.exp(array)\nprint(\"Exponential Array:\", exp_array)\n\nExponential Array: [ 2.71828183  7.3890561  20.08553692 54.59815003]\n\n\n\nnp.exp from Simulated Annealing example\nThis function is part of a Simulated Annealing algorithm, specifically handling the temperature decay mechanism to decide whether to accept a new solution, even if it‚Äôs worse than the current one. Here‚Äôs a breakdown of the function based on the np.exp command and the logic:\n\ntmp_obj_val: The objective value of a new (temporary) solution.\nobj_val: The objective value of the current solution. temperature: The current temperature in the simulated annealing process, which controls how likely the algorithm is to accept worse solutions.\nA random number r between 0 and 1 is generated. This represents a threshold for whether the new solution will be accepted using random.rand()\nThe probability p of accepting the new solution is computed using the exponential function.\nIf the random value r is less than the calculated probability p, the function returns True, meaning the new solution is accepted (even if it‚Äôs worse). If r is greater than p, the new solution is rejected, and the current solution is maintained.\n\nThe function decides whether to accept a new solution in simulated annealing, balancing exploration and exploitation based on the temperature and objective values of the solutions. The np.exp() function ensures that worse solutions have a chance to be accepted, particularly early in the process, fostering a broader search space.\n\n\n# Simulated annealing temperature decay\ndef determine(self, tmp_obj_val, obj_val, temperature):\n     r = np.random.rand()\n     p = np.exp((tmp_obj_val - obj_val) / temperature)\n     return r &lt; p",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#taking-a-square-root-np.sqrt",
    "href": "numpy.html#taking-a-square-root-np.sqrt",
    "title": "Python for Heuristics & NumPy",
    "section": "Taking a square root: np.sqrt()",
    "text": "Taking a square root: np.sqrt()\n\nnp.sqrt is a function in NumPy that returns the non-negative square root of an element-wise input array. It operates on each element of the array and computes the square root.\n\n\n# Applying np.sqrt to the array\nsqrt_array = np.sqrt(array)\nprint(\"Square Root Array:\", sqrt_array)\n\nSquare Root Array: [1.         1.41421356 1.73205081 2.        ]\n\n\n\nThe Ackley function is commonly used as a benchmark problem in optimization, and is known for its many local minima. The Ackley function uses the np.sqrt within its formula.\n\n\ndef ackley(s):\n     a, b, c = 20, 0.2, 2 * np.pi\n     n = len(s)\n     sum_sq_term = np.sum(s**2)\n     cos_term = np.sum(np.cos(c * s))\n     term1 = -a * np.exp(-b * np.sqrt(sum_sq_term / n))\n     term2 = -np.exp(cos_term / n)\n     return term1 + term2 + a + np.e",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#random-number-generation",
    "href": "numpy.html#random-number-generation",
    "title": "Python for Heuristics & NumPy",
    "section": "Random Number Generation",
    "text": "Random Number Generation\n\nThe np.random.rand function in NumPy generates random floating-point numbers from a uniform distribution between 0 (inclusive) and 1 (exclusive), meaning the generated numbers will always include 0 (inclusive) but never reach 1 (exclusive).\nRandom numbers are key to both genetic algorithms (mutation, crossover) and simulated annealing (random perturbations).\nThe example below selects uses np.random.rand() to generate 5 uniform random numbers between 0 and 1 [0,1). It is saved in a variable rand_nums and the values in the variable are printed.\n\n\nrand_nums = np.random.rand(5)\nprint(rand_nums)\n\n[0.42258775 0.41449356 0.83578836 0.24556806 0.63062298]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#standard-normal-distribution",
    "href": "numpy.html#standard-normal-distribution",
    "title": "Python for Heuristics & NumPy",
    "section": "Standard Normal Distribution",
    "text": "Standard Normal Distribution\n\nThe np.random.standard_normal function in NumPy generates random floating-point numbers from a standard normal (Gaussian) distribution, with a mean of 0 and a standard deviation of 1.\nGenerating 5 random numbers from a standard normal distribution (mean=0, std=1).\n\n\n# Generating random values from the standard normal distribution\nrandom_values = np.random.standard_normal(5)\nprint(\"Random Standard Normal Values:\", random_values)\n\nRandom Standard Normal Values: [ 0.21722573  0.08161093  0.0738901   0.70165888 -0.11269229]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#np.random.uniform",
    "href": "numpy.html#np.random.uniform",
    "title": "Python for Heuristics & NumPy",
    "section": "np.random.uniform",
    "text": "np.random.uniform\nThe np.random.uniform function in NumPy is used to generate random floating-point numbers drawn from a uniform distribution over a specified range.  For example, in hill climbing, the algorithm often starts with a random solution that can be simulated with np.random.uniform.\n\n# Generate a random starting point for the hill climbing algorithm\nrandom_start = np.random.uniform(low=-10, high=10, size=5)\nprint(f\"Random start: {random_start}\")\n\nRandom start: [ 7.60625241 -4.77577367 -5.90068767 -8.78667658 -6.55427277]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#np.random.randint",
    "href": "numpy.html#np.random.randint",
    "title": "Python for Heuristics & NumPy",
    "section": "np.random.randint",
    "text": "np.random.randint\n\nThe np.random.randint function in NumPy is used to generate random integers within a specified range.\nnp.random.randint(low, high=None, size=None, dtype=int)\n\nlow: The lower boundary of the random integers (inclusive).\nhigh: The upper boundary of the random integers (exclusive). If not provided, random integers are generated between 0 and low.\nsize: The shape of the output array (optional). If not provided, a single integer is returned.\ndtype: The desired data type of the output array, by default int.\n\n\n\n# Generate 5 random integers between 10 and 20\nrandom_integers = np.random.randint(10, 20, size=5)\nprint(random_integers)\n\n[10 12 16 15 13]\n\n\n\nWe can use np.random.randint to generate a 2D array instead of a 1D array by specifying the size parameter as a tuple that indicates the shape of the array.\n\n\nrandom_2d_array = np.random.randint(10, 20, size=(3, 5)) \nprint(random_2d_array)\n\n[[19 14 19 13 19]\n [15 15 17 18 13]\n [18 15 19 17 14]]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#np.random.randint-from-simulated-annealing",
    "href": "numpy.html#np.random.randint-from-simulated-annealing",
    "title": "Python for Heuristics & NumPy",
    "section": "np.random.randint from Simulated Annealing",
    "text": "np.random.randint from Simulated Annealing\n\nThis function, transit(), is used to modify a solution sol as part of a heuristic search process, likely for algorithms like genetic algorithms, hill climbing, or simulated annealing. The goal is to explore the solution space by introducing a small, random change (or ‚Äútransition‚Äù) to the current solution.\n\nThe function takes a single argument, sol, which is likely a binary array or list (a list of 0s and 1s).\nt = sol.copy(): A copy of the solution sol is made, named t. This is important because we don‚Äôt want to modify the original solution directly; instead, we work on the copy t.\ni = np.random.randint(len(sol)): The randint function from NumPy is used to randomly select an index i between 0 and the length of sol - 1. This selects a random position in the solution array.\nt[i] ^= 1: This is a bitwise XOR operation. In the context of a binary solution (a list of 0s and 1s), it flips the value at index i:If t[i] is 0, it becomes 1.If t[i] is 1, it becomes 0. This operation introduces a small, random change to the solution by flipping one bit.\nreturn t: After flipping one bit, the modified solution t is returned.\n\n\n# Transition function (T)\ndef transit(sol):\n    new_sol = sol.copy()\n    index = np.random.randint(len(sol))\n    new_sol[index] = 1 - new_sol[index]  # Flip a random bit\n    return new_sol\n\nTo compare:\n\nnp.random.standard_normal = normal distribution with mean and sd\nnp.random.rand = uniform distribution between [0,1)\nnp.random.uniform = uniform over a specified range\nnp.random.randint = random integers within a specified range",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#sorting-the-data-np.argsort",
    "href": "numpy.html#sorting-the-data-np.argsort",
    "title": "Python for Heuristics & NumPy",
    "section": "Sorting the data: np.argsort",
    "text": "Sorting the data: np.argsort\n\nThe np.argsort function in NumPy returns the indices that would sort an array along a specified axis. This allows you to reorder elements based on their sorted order without actually changing the original array.\nIn various evolutionary algorithms (such as genetic algorithms or simulated annealing), selecting the most ‚Äúfit‚Äù or optimal solutions from a population is crucial for convergence toward the global optimum.\nBy sorting individuals based on fitness, the algorithm can efficiently identify the most promising candidates for further exploration (e.g., crossover, mutation) or intensify the search around high-quality solutions.\nThe use of np.argsort allows for a fast, reliable way to rank individuals, ensuring that the evolutionary process focuses on refining the best candidates and discarding those with lower potential.\n\n\n# Dummy population and fitness values\npopulation = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n\n# Assign dummy fitness values\nfitness = np.array([10, 30, 20, 40, 50]) \n\n# Sort population based on fitness\nindices = np.argsort(fitness)\nprint(indices) \nsorted_population = population[indices]\n\n# Select top 3 individuals\ntop_individuals = sorted_population[:3] \nprint(top_individuals)\n\n[0 2 1 3 4]\n[[1 2]\n [5 6]\n [3 4]]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#selecting-the-max-np.argmax",
    "href": "numpy.html#selecting-the-max-np.argmax",
    "title": "Python for Heuristics & NumPy",
    "section": "Selecting the Max: np.argmax",
    "text": "Selecting the Max: np.argmax\n\nThe np.argmax function in NumPy returns the index of the maximum value in an array along a specified axis.\nFinding the Index of the Maximum Element in a 1D Array: The np.argmax function returns the index of the first occurrence of the maximum value in the array. In this case, the maximum value is 7, and it occurs at index 2.\n\n\narr = np.array([1, 3, 7, 2, 5])\nindex = np.argmax(arr)\nprint(\"Array:\", arr)\nprint(\"Index of max element:\", index)\nprint(\"Max element:\", arr[index])\n\nArray: [1 3 7 2 5]\nIndex of max element: 2\nMax element: 7\n\n\nArray: [1 3 7 2 5]\n\nUsing np.argmax with a 2D Array (Row-wise & Column-wise): np.argmax can work on multi-dimensional arrays. By specifying axis=0 or axis=1, you can find the maximum values column-wise or row-wise, respectively. For axis=0, you get the indices of the maximum elements for each column, and for axis=1, you get them for each row.\n\n\narr_2d = np.array([[1, 2, 3], [4, 5, 1], [0, 6, 2]])\n\n# Find the index of the max element in the flattened array\nmax_index_flat = np.argmax(arr_2d)\nprint(\"Flattened array index:\", max_index_flat)\n\nFlattened array index: 7\n\n\n\nNumber 6 is in index 7, starting at index 0 and counting up across each row.\n[[1 2 3]\n[4 5 1]\n[0 6 2]]\n\n\n# Find the index of the max element along each column (axis=0)\nmax_index_col = np.argmax(arr_2d, axis=0)\nprint(\"Max element index for each column:\", max_index_col)\n\nMax element index for each column: [1 2 0]\n\n\n\n4 is in index 1, 6 is in index 2, and 3 is in index 0, counting across each column starting at index 0. [[1 2 3]\n[4 5 1]\n[0 6 2]]\n\n\n# Find the index of the max element along each row (axis=1)\nmax_index_row = np.argmax(arr_2d, axis=1)\nprint(\"Max element index for each row:\", max_index_row)\n\nMax element index for each row: [2 1 1]\n\n\n\n3 is in index 2 in the row, 5 is in index 1, and 6 is in index 1, counting across each row starting at index 0. [[1 2 3]\n[4 5 1]\n[0 6 2]]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#an-example-modelling-stock-prices",
    "href": "numpy.html#an-example-modelling-stock-prices",
    "title": "Python for Heuristics & NumPy",
    "section": "An Example Modelling Stock Prices",
    "text": "An Example Modelling Stock Prices\n\nThe model simulates a Geometric Brownian Motion (GBM), a widely used stochastic process in financial mathematics to model the evolution of stock prices over time. This process assumes that stock prices follow a log-normal distribution, incorporating key parameters such as the initial stock price (S0), risk-free rate (r), time horizon (T), and volatility (sigma).\nThe model calculates the potential future stock prices (ST) using a mathematical formula that combines deterministic and random components, reflecting the inherent uncertainty and growth trends in financial markets. By generating a large number of simulated outcomes, the model enables analyses such as estimating expected returns, assessing risk, and valuing options, providing valuable insights for decision-making in finance.\n\n\\(S_T = S_0 \\exp\\left( (r - 0.5 \\sigma^2) T + \\sigma Z \\sqrt{T} \\right)\\)\n\nThe terminal stock price \\(S_T\\) is modeled using the Geometric Brownian Motion (GBM), a common approach to model stock prices.\n\n\\(S_0\\): The initial stock price.\n\\(r\\): The risk-free interest rate.\n\\(T\\): Time to maturity (in years).\n\\(\\sigma\\): The volatility of the stock.\n\\(S_T\\): The terminal stock price at time \\(T\\).\n\\(Z\\): A random variable drawn from a standard normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#comparing-model-clock-time-withwithout-numpy",
    "href": "numpy.html#comparing-model-clock-time-withwithout-numpy",
    "title": "Python for Heuristics & NumPy",
    "section": "Comparing Model Clock Time With/Without NumPy",
    "text": "Comparing Model Clock Time With/Without NumPy\n\nThe primary difference in wall time between the two approaches stems from the computational efficiency of NumPy compared to Python‚Äôs built-in modules and loops.\n\n\nWithout Numpy\n\na Python loop iterates 1,000,000 times, and the math.exp, random.gauss, and math.sqrt functions are called repeatedly within the loop to calculate values. This results in higher wall time due to the overhead of Python‚Äôs interpreted loop and the sequential calls to these functions.\n\n\nimport random\nfrom math import exp, sqrt\nimport time \n\n# Initial stock price\nS0 = 100 \n\n# Risk-free rate\nr = 0.05 \n\n# Time horizon (1 year)\nT = 1.0 \n\n# Volatility\nsigma = 0.2 \n\nvalues = []  \n\n# Start tracking wall time\nstart_time = time.time()\n\nfor _ in range(1000000):  \n     ST = S0 * exp((r - 0.5 * sigma ** 2) * T +\n        sigma * random.gauss(0, 1) * sqrt(T))  \n     values.append(ST)  \n\n# End tracking wall time\nend_time = time.time()\n\n\n# Calculate time difference\nwall_time = end_time - start_time\n\n# Print timing information\nprint(f\"Wall time: {wall_time:.2f} s\")\n\nWall time: 0.67 s\n\n\n\n\nWith Numpy\n\nIn contrast, the NumPy-based implementation below leverages vectorized operations. NumPy handles the entire computation in a single step using efficient, low-level C routines optimized for performance. For example:\n\nThe entire random sample generation is done in one call (np.random.standard_normal(1000000)).\n\n\n\nimport numpy as np\nimport time \n\n# Initial stock price\nS0 = 100 \n\n# Risk-free rate\nr = 0.05 \n\n# Time horizon (1 year)\nT = 1.0 \n\n# Volatility\nsigma = 0.2 \n\n# Start tracking wall time\nstart_time = time.time()\n\nST = S0 * np.exp((r - 0.5 * sigma ** 2) * T +\n    sigma * np.random.standard_normal(1000000) * np.sqrt(T))\n\n# End tracking wall time\nend_time = time.time()\n\n# Calculate time difference\nwall_time = end_time - start_time\n\n# Print timing information\nprint(f\"Wall time: {wall_time:.2f} s\")\n\nWall time: 0.03 s\n\n\n\nMathematical operations like exp and sqrt are applied to entire arrays at once. These optimizations significantly reduce the wall time, as the process avoids Python-level overhead and directly utilizes optimized native code. As a result, the NumPy implementation is typically faster, making it better suited for tasks requiring a high volume of computations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "algo.html#pseudocode-for-tsp-nearest-neighbor-model",
    "href": "algo.html#pseudocode-for-tsp-nearest-neighbor-model",
    "title": "Algorithm Design and Pseudocode",
    "section": "Pseudocode for TSP Nearest Neighbor Model",
    "text": "Pseudocode for TSP Nearest Neighbor Model\n\nNearest Neighbor Heuristic: The algorithm works by greedily choosing the nearest unvisited city until all cities are visited, then returning to the start city.\nOutput: It outputs the route and the total distance traveled.\n\n#inputs\nSET cities = &lt;list of cities&gt;\nSET distances = &lt;distance matrix or dictionary&gt;\nSET start_city = &lt;initial city&gt;\n\n\n# FUNCTION to find the nearest unvisited city\nFUNCTION find_nearest_neighbor(current_city, unvisited, distances):\n    SET nearest_city = None\n    SET min_distance = infinity\n    \n    FOR each city IN unvisited:\n        IF distances[current_city][city] &lt; min_distance:\n            SET min_distance = distances[current_city][city]\n            SET nearest_city = city\n    \n    RETURN nearest_city, min_distance\n\n# FUNCTION to solve TSP using Nearest Neighbor algorithm\nFUNCTION nearest_neighbor_tsp(start_city, cities, distances):\n# Initialize list of unvisited cities\n    SET unvisited = list of all cities EXCEPT start_city",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "greedy.html",
    "href": "greedy.html",
    "title": "Introduction to Greedy Algorithms",
    "section": "",
    "text": "Optimization Problem",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "greedy.html#definition-1",
    "href": "greedy.html#definition-1",
    "title": "Introduction to Greedy Algorithms",
    "section": "Definition 1",
    "text": "Definition 1\n\nAn optimization problem \\(P\\) is to find the optimal value, possibly subject to some constraints, out of all possible solutions.\nContains the objective function, constraint(s), and solution.\n\\(opt_{s \\in A} f(s)\\) subject to \\(\\forall c_i(s) \\odot b_i, i=i, 2, ...,m\\) where\nopt is either min (for minimization) or max (for maximization),\n\ns is a candidate solution\nA and B are the domain and codomain of the problem Image, namely, A is the set of all possible solutions and B is the set of all possible outcomes of the objective function,\n\\(c_i(s) \\odot b_i\\) is the constraint, and\n\\(f(s): A-&gt;B\\) is the objective function\n\\(\\odot\\) is \\(&gt;, &lt;, =, &lt;=, &gt;=\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "greedy.html#definition-2",
    "href": "greedy.html#definition-2",
    "title": "Introduction to Greedy Algorithms",
    "section": "Definition 2",
    "text": "Definition 2\n\nThe optimal solution is a solution, out of all feasible candidate solutions of the optimization problem \\(P\\), that gives the optimal value. \\(f(s^*) = \\operatorname{opt} \\{f(s)\\}, \\, \\forall \\, c_i(s) \\, \\odot \\, b_i, \\, i = 1, 2, \\dots, m.\\)\n\nTo compare \\(f(s)\\) with \\(f(s^*)\\), \\(f(s)\\) represents the objective function evaluated at a solution \\(s\\). \\(f(s^*)\\) refers to the optimal solution, where \\(s^*\\) is the best candidate that optimizes \\(f(s)\\) (either maximizing or minimizing it).",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "greedy.html#definition-3",
    "href": "greedy.html#definition-3",
    "title": "Introduction to Greedy Algorithms",
    "section": "Definition 3",
    "text": "Definition 3\n\nIf the optimal solution \\(s*\\) for the problem ùëÉ exists, then the optimal value \\(f^*\\) is defined as \\(min_{s \\in A} f(s)\\), subject to \\(\\forall c_i(s) \\odot b_i\\),\nWhile the maximization problem of maximizing \\(f(s)\\) subject to some constraints can be defined as \\(max_{s \\in A} f(s)\\), subject to \\(\\forall c_i(s) \\odot b_i\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "greedy.html#simple-example-of-greedy-algorithm-in-action",
    "href": "greedy.html#simple-example-of-greedy-algorithm-in-action",
    "title": "Introduction to Greedy Algorithms",
    "section": "Simple Example of Greedy Algorithm in Action",
    "text": "Simple Example of Greedy Algorithm in Action\n\nThe Coin Change Problem\nGiven a set of coin denominations and a target amount, find the minimum number of coins that add up to the target amount.\nGreedy Strategy: At each step, pick the largest denomination that doesn‚Äôt exceed the remaining amount.\n\n\\[\\operatorname{coins\\_used}(A) = \\sum_{i=1}^{n} \\left\\lfloor \\frac{A}{c_i} \\right\\rfloor \\times c_i \\quad \\operatorname{where} \\quad A = A - \\left\\lfloor \\frac{A}{c_i} \\right\\rfloor \\times c_i\\]\nWhere \\(\\left\\lfloor \\frac{A}{c_i} \\right\\rfloor\\) is the number of coins of denomination \\(c_i\\) is used.\n\\(A\\) is reduced by the value \\(\\left\\lfloor \\frac{A}{c_i} \\right\\rfloor \\times c_i\\)\nAfter using as many \\(c_i\\) denomination coins as possible. The process continues until \\(A=0\\), at which point the minimum number of coins required to make the total amount is found.\n\ndef greedy_coin_change(coins, amount):\n    coins.sort(reverse=True)\n    result = []\n    for coin in coins:\n        while amount &gt;= coin:\n            amount -= coin\n            result.append(coin)\n    \n    # Print the coins used\n    print(f\"Coins used: {result}\")\n    \n    # Return the number of coins used\n    return len(result)\n\n# Get user input (Put in 70 to show answer, but can request information from user)\namount = 70\n# amount = int(input(\"Enter the amount: \"))\n\n# Coin denominations\ncoins = [1, 5, 10, 25]\n\n# Calculate the solution\nnum_coins = greedy_coin_change(coins, amount)\n\nprint(f\"Minimum number of coins needed: {num_coins}\")\n\nCoins used: [25, 25, 10, 10]\nMinimum number of coins needed: 4",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "greedy.html#discrete-vs-continuous",
    "href": "greedy.html#discrete-vs-continuous",
    "title": "Introduction to Greedy Algorithms",
    "section": "Discrete vs Continuous",
    "text": "Discrete vs Continuous\n\n\n\nDiscrete vs Continuous\n\n\n\nDiscrete\n\nDomain: The set of all possible input values for a function.\nCodomain: The set of all potential output values that the function can map to.\nAn objective function is a function that is being optimized (maximized or minimized) in a given problem. It takes an input from the domain and produces an output in the codomain.\nGiven two sets \\(A\\) and \\(B\\), and an objective function \\(f\\), we can understand how the function maps elements from the domain \\(A\\) to the codomain \\(B\\).\n\n\n\nContinuous\n\nShows the relationship between the angle Œ∏ and the value of sin‚Å°(Œ∏) at specific points. This relationship arises from the trigonometric sine function, which describes a wave-like pattern that oscillates between -1 and 1.\nŒ∏ represents the angle, typically in radians, and the values given (0, 0.25\\(\\pi\\), 0.50\\(\\pi\\), etc.) are specific points along the unit circle.\nsin(Œ∏) represents the sine of the angle Œ∏, which is the y-coordinate of the corresponding point on the unit circle.\nThe values provided in the table correspond to these properties of the sine function. The function gradually increases from 0 to 1, then decreases back to 0, then continues to -1, and finally returns to 0, completing one full cycle.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "greedy.html#traveling-salesman-algorithm",
    "href": "greedy.html#traveling-salesman-algorithm",
    "title": "Introduction to Greedy Algorithms",
    "section": "Traveling Salesman Algorithm",
    "text": "Traveling Salesman Algorithm\nOptimal Path: \\[\\min_{s \\in \\S_{\\pi}} f(s) = \\left[ \\sum_{i=1}^{n-1} d\\left(c_{\\pi(i)}, c_{\\pi(i+1)}\\right) \\right] + d\\left(c_{\\pi(n)}, c_{\\pi(1)}\\right)\\]\nWhere min_(ùë†‚ààùíÆ_ùúã )\\(c_{\\pi} = \\{ c_{\\pi(1)}, c_{\\pi(2)}, \\dots, c_{\\pi(n)} \\}\\), that is, all permutations of the \\(n\\) cities.\nvs.¬†\nNearest Neighbor (Greedy): \\(Z = \\sum_{i=1}^{n-1} c_{i, \\text{NN}(i)} + c_{n, \\text{NN}(1)}\\)\n\n\n\nGreedy Map\n\n\n\nGreedy TSP Solution using Nearest Neighbor\n\nStart at Richmond. Find the nearest city. From Richmond, the nearest city is Petersburg (25 miles). Move to Petersburg.\nFrom Petersburg, find the nearest unvisited city, which is Newport News (65 miles). Move to Newport News .\nFrom Newport News, the nearest unvisited city is Norfolk (30 miles). Move to Norfolk.\nFrom Norfolk, the nearest unvisited city is Chesapeake (10 miles).Move to Chesapeake.\nFrom Chesapeake, the only unvisited city left is Virginia Beach (15 miles). Move to Virginia Beach.\nFinally, return to Richmond from Virginia Beach (100 miles).\nRichmond -&gt; Petersburg -&gt; Newport News -&gt; Norfolk -&gt; Chesapeake -&gt; Virginia Beach -&gt; Richmond\nTotal distance traveled: = 25+65+30+10+15+100 = 245 miles\n\n\n\n\nTSP Feasibility Map",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "benchmark.html",
    "href": "benchmark.html",
    "title": "Benchmark Optimization Problems",
    "section": "",
    "text": "Benchmark Problems: Overview",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "benchmark.html#onemax-problem",
    "href": "benchmark.html#onemax-problem",
    "title": "Benchmark Optimization Problems",
    "section": "OneMax Problem",
    "text": "OneMax Problem\n\nIn evolutionary algorithms, the OneMax problem serves as a simple test problem where the goal is to evolve a population of binary strings towards the optimal solution (a string of all 1s). The fitness function is used to evaluate the quality of each candidate solution in the population.\nFitness Function: Imagine life had a personal ‚Äòfitness function‚Äô just for you. What variables would you include in it, and how would you weigh them? If 0 meant that you were unable to satisfy that goal, and 1 meant that you were able to satisfy that goal, wouldn‚Äôt you want all 1s.\n\n\nOne Max Formula\n\nBinary String: A binary string is generated using NumPy‚Äôs randint function, which creates a list of 0s and 1s.\nFitness Function: The one_max function calculates the ‚Äúfitness‚Äù of the binary string, which is simply the sum of all 1s in the string. This is the value that needs to be maximized.\nExample Run: If the generated binary string is [1, 0, 1, 1, 0, 1, 0, 1, 1, 0], the fitness would be 6, since there are six 1s in the string.\nThe objective is to maximize the number of 1s in a binary string.\n\n\\[\\max_{s \\in A} f(s) = \\sum_{i=1}^{n} s_i, \\quad \\text{subject to} \\ s_i \\in \\{0, 1\\}.\\]\n\n\nOptimal Solution OneMax\n\nThe optimal solution of this problem is that all the subsolutions assume the value 1; i.e., \\(s_i=1\\) for all \\(i\\). For instance, the optimal solution for \\(n=4\\) is \\(s^*=(1111)\\) and the objective value of a possible solution \\(s^*=(0111)\\) can be easily calculated as the count of the number of ones in the solution \\(s\\) as the objective function if \\(f(s) = f(0111) = 0+1+1+1 = 3\\)\n\n\n\nOneMax Pseudocode\nPseudocode:\n\n\nFUNCTION one_max(binary_string):\n# Calculate the fitness as the sum of 1s in the binary string\n    RETURN sum(binary_string)\n\n# Example usage\nSET n = 10  # Length of the binary string\n\n# Generate a random binary string of length n\nSET binary_string = generate a random list of 0s and 1s of size n\n\n# Calculate the fitness\nSET fitness = one_max(binary_string)\n\n# Print the binary string and its fitness\nPRINT \"Binary string:\", binary_string\nPRINT \"Fitness (number of 1s):\", fitness\n\n\n\nOneMax Python Implementation\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef one_max(binary_string):\n    return np.sum(binary_string)  \n\n# Example usage: # Length of the binary string\nn = 10\n\n# Generate a random binary string of length n (keeps it as a NumPy array)\nbinary_string = np.random.randint(0, 2, size=n)\n\n# Calculate the fitness using np.sum\nfitness = one_max(binary_string)\n\nprint(f\"Binary string: {binary_string}\")\nprint(f\"Fitness (number of 1s): {fitness}\")\n\n#### Example of a fitness plot with new summed fitness score\niterations = 20\nfitness_over_time = np.random.randint(0, n + 1, size=iterations)\n\n# Line plot of fitness over iterations\nplt.figure(figsize=(8, 4))\nplt.plot(range(iterations), fitness_over_time, marker='o', color='green', linestyle='-', linewidth=2)\nplt.fill_between(range(iterations), fitness_over_time, color='lightgreen', alpha=0.4)\nplt.title(f\"Fitness Evolution Over Time\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Fitness (number of 1s)\")\nplt.xticks(np.arange(0, iterations, step=1))  # The step shows whole numbers on the x-axis\n\nplt.grid(True)\nplt.show()\n\nBinary string: [1 1 0 1 0 1 0 1 0 0]\nFitness (number of 1s): 5\n\n\n\n\n\n\n\n\n\n\nThe plot above tracks how fitness improves or changes across iterations in an optimization algorithm, giving insight into the convergence of the algorithm.\n\n\n\nComparing OneMax Problem to Greedy Algorithm\n\nIf a greedy search algorithm is used and is allowed to randomly add one to or subtract one from the current solution \\(s\\) to create the next possible solution \\(v\\) for solving the one-max problem, that is, it is allowed to move one and only one step to either the left or the right of the current solution in the landscape of the solution space.\nWithout knowledge of the landscape of the solution space, the search process will easily get stuck in the peaks of this solution space.\nHence, most researchers prefer using the one-max problem as an example because it is easy to implement and also because it can be used to prove if a new concept for a search algorithm is correct.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "benchmark.html#the-knapsack-problem",
    "href": "benchmark.html#the-knapsack-problem",
    "title": "Benchmark Optimization Problems",
    "section": "The Knapsack Problem",
    "text": "The Knapsack Problem\n\nThe Knapsack Problem is a classic NP-complete optimization problem, where you are given a set of items, each with a weight and a value.\nThe goal is to determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible.\nTypes of Knapsack Problems:\n\n0/1 Knapsack Problem:\n\nEach item can be included (1) or excluded (0) in the knapsack.\nYou cannot break items into smaller parts. \\(\\max_{s \\in A} f(s) = \\sum_{i=1}^{n} s_i v_i, \\quad \\text{subject to} \\quad w(s) = \\sum_{i=1}^{n} s_i w_i \\leq W, \\quad s_i \\in \\{0, 1\\}\\) Where \\(v_i\\) is the value associated with \\(s_1\\) and \\(w_i\\) is the weight associated with \\(s_i\\)\n\nFractional Knapsack Problem:\n\nYou can break items into smaller parts and include fractions of them in the knapsack.\nRatio = \\(\\frac{v_i}{w_i}\\), where \\(v_i\\) is the value of item \\(i\\), and \\(w_i\\) is the weight of item \\(i\\).\n\n\n\n\nNP Complete\n\nNP (Nondeterministic Polynomial Time): A problem is in NP if a solution can be verified in polynomial time by a deterministic algorithm. In other words, given a solution, it is possible to check if it is correct relatively quickly (in polynomial time). However, finding the solution itself might take much longer (potentially exponential time) unless the problem can also be solved in polynomial time.\nNP-complete refers to a class of problems in computational complexity theory that are both NP (nondeterministic polynomial time) and every problem in NP can be reduced to it in polynomial time\nNP-hard problems are optimization or decision problems that are at least as difficult to solve as the hardest problems in NP (nondeterministic polynomial time).\n\nUnlike NP-complete problems, NP-hard problems do not have to be verifiable in polynomial time. This means that while it may be incredibly hard to find an optimal solution, even verifying a proposed solution might take more than polynomial time.\nEssentially, NP-hard problems are hard to solve optimally, and their complexity often prevents efficient algorithms from finding or checking solutions within a reasonable time frame.\nNP-hard problems are broader and potentially harder than NP-complete problems because they can include problems that aren‚Äôt even in NP. They may not have a polynomial-time verification process.\n\n\n\n\nKey Characteristics of NP-complete Problems\n\nDifficult to solve: No known algorithms can solve NP-complete problems efficiently (in polynomial time) for all instances.\nVerification in polynomial time: If someone provides a solution, it can be verified quickly. Equivalence to other NP-complete problems: If one NP-complete problem can be solved in polynomial time, all NP-complete problems can be solved in polynomial time.\nKnapsack Problem: The 0/1 knapsack problem is NP-complete. Finding the optimal solution is hard, but verifying if a solution meets the constraints and maximizes value can be done in polynomial time.\nThe Fractional knapsack problem is not NP-complete and can be solved in polynomial time using a greedy algorithm.\nThe Travelling Salesman is a NP-hard problem.\n\n\n\nExample Fractional Knapsack Problem\n\nItems Available: - Item 1: Value = 10, Weight = 5 kg - Item 2: Value = 40, Weight = 10 kg - Item 3: Value = 30, Weight = 15 kg\nObjective: Maximize the total value without exceeding Knapsack Capacity of 15 kg.\nThe greedy algorithm works well by prioritizing items with the highest value-to-weight ratio.\n0/1 Knapsack Problem requires more complex algorithms like dynamic programming to find the optimal solution over the Fractional Knapsack Problem\n\n\n\nGreedy Algorithm for Fractional Knapsack\n\nStep 1: Calculate Value-to-Weight Ratio:\n\nItem 1: 10/5=2\nItem 2: 40/10=4\nItem 3: 30/15=2\n\nStep 2: Sort Items by Ratio (Descending): Item 2, Item 1, Item 3\n\nStep 3: Fill the Knapsack:\nTake Item 2 (10 kg, Value = 40).\nTake as much of Item 1 as possible (5 kg, Value = 10).\n\nResults: Total Weight = 15 kg, Total Value = 50.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "benchmark.html#binary-to-decimal-b2d-problem-b2d-1",
    "href": "benchmark.html#binary-to-decimal-b2d-problem-b2d-1",
    "title": "Benchmark Optimization Problems",
    "section": "Binary to Decimal (B2D) Problem: B2D-1",
    "text": "Binary to Decimal (B2D) Problem: B2D-1\n\nThe binary to decimal model is often used in optimization problems, particularly in the context of genetic algorithms and heuristic methods.\nWith a minor modification, the solution space of the one-max problem can be simplified as the solution space of another optimization problem.\nThe model uses binary strings to represent numbers. Each string represents a decimal number when interpreted in binary form.\nThe B2D-1 problem is to maximize the value of the objective function of a binary string.\n\n\nCharacteristics and Visualization of B2D-1\n\nThese two examples are possible landscapes to the B2D problem.\nThe first chart to the left implies that there are only two possible next states (candidate solutions) that can be generated from the current solution except for solutions (0000) and (1111), which can only be moved to the right and to the left, respectively.\nIf another search algorithm can generate a new candidate solution by randomly inverting (flipping) one of the subsolutions of the current solution, the number of possible states of the new candidate solution will be \\(n\\), where \\(n\\) is the number of subsolutions.\n\n\n\n\nOneMax",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "benchmark.html#b2d-with-deception-b2d-2",
    "href": "benchmark.html#b2d-with-deception-b2d-2",
    "title": "Benchmark Optimization Problems",
    "section": "B2D with Deception: B2D-2",
    "text": "B2D with Deception: B2D-2\n\nB2D deception problems mislead optimization algorithms away from finding the global optimum by presenting local optima that seem promising but are actually suboptimal.\nUsed to test whether a search algorithm is capable of escaping local optima or not.\nDeception problems highlight the necessity of exploration in heuristic algorithms, such as introducing diversity through mutation or crossover in genetic algorithms. If the algorithm becomes too greedy and focuses only on local fitness improvements (exploitation), it may get stuck at deceptive local optima.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "benchmark.html#single-objective-optimization-problem-sop",
    "href": "benchmark.html#single-objective-optimization-problem-sop",
    "title": "Benchmark Optimization Problems",
    "section": "Single-objective Optimization Problem (SOP)",
    "text": "Single-objective Optimization Problem (SOP)\n\nA single-objective optimization problem involves finding the best solution from a set of feasible solutions based on a single objective function. The goal is to either maximize or minimize this objective function.\n\n\\[\\underset{s \\in \\mathbb{R}^n}{\\text{opt}} f(s), \\quad \\text{subject to } \\, c_i(s) \\odot b_i, \\quad i = 1, 2, \\ldots, m,\\]\n\nwhere\n\n\\({R}^n\\) and \\({R}\\) are the domain and codomain, respectively,\n\\(f(s) {R}^n\\) and \\({R}\\) is the objective function to be optimized,\n\\(c_i(s): {R}^n\\) and \\({R}\\odot b_i, \\quad i = 1, 2, \\ldots, m,\\) are the constraints,\nand \\(opt\\) and \\(\\odot\\) are as given in Definition 1 as &lt;, &gt;, =, ‚©Ω, or ‚©æ.\n\n\n\nAckley Function: A Single Optimization Problem\n\nThe Ackley Function is a widely used benchmark function for testing optimization algorithms. It is characterized by its multi-modal nature with a nearly flat outer region and a large hole at the center.\nApplications\n\nUsed as a standard test case in evaluating the performance of optimization algorithms like genetic algorithms, simulated annealing, and particle swarm optimization.\nRelevant in fields such as machine learning, control systems, and operations research.\n\nLimitations\n\nThe function‚Äôs large search space and numerous local minima make it difficult for algorithms to converge to the global minimum.\nLarge importance of balancing exploration and exploitation in optimization strategies when dealing with the Ackley Function.\n\n\n\nExplanation of Ackley Function(x, y)\n\nComputes the value of the Ackley function given a point (x, y) in the search space.\nThe optimization algorithm optimizes the Ackley function to find the point where it reaches its minimum. It initializes a population of random solutions, evaluates their fitness (using the Ackley function), and iteratively improves them using an optimization method (like gradient descent or a genetic algorithm).\nThe best solution and corresponding function value (score) are returned as the result.\n\n\n\nAckley function and B2D: Converting to Decimal\n\nThe Ackley function uses the binary representation, where the binary strings need to be converted to decimal values (i.e., real numbers). In this case, the converted decimal values correspond to points in the continuous search space.\nFor example, a binary string like 1010 can be converted into a decimal value, which can then be used as input to the Ackley function. Binary String: 1010, where the binary number is 1010_2.\nEach position in the binary number represents a power of 2, starting from the right (least significant bit):\nThe rightmost bit (0) is \\(2^0\\),The next bit (1) is \\(2^1\\) ,The next bit (0) is \\(2^2\\),The leftmost bit (1) is \\(2^3\\).\n\n\\(1010_2= 0 ‚àó 2^0+ 1 ‚àó 2^1 +0 ‚àó 2^2+1 ‚àó 2^3\\) \\(= 0 ‚àó 1+ 1 ‚àó 2 + 0 ‚àó 4 + 1 ‚àó 8\\) \\(=  0 + 2 + 0 + 8 = 10\\) Thus, the decimal equivalent of the binary string ‚Äú1010‚Äù is 10. Use this value as input for the Ackley function.\n\n\nCharacteristics and Visualization of Ackley Function\n\nThe Ackley function is evaluated in the hypercube.\nThe global optimum (minimum) of the Ackley function is ùëì(ùë†^‚àó)=0 is located at \\(s^*=(0,0,‚Ä¶0)\\).\nThis function has many local optima, which makes it hard for the search algorithm to find the global optimum.\n\n\n\n\nAckley\n\n\n\n\nAckley Function Formula\n\\[\n\\begin{array}{rl}\n\\min_{s \\in \\mathbb{R}^n} f(s) &= -20 \\exp \\left(-0.2 \\sqrt{\\frac{1}{n} \\sum_{i=1}^n s_i^2} \\right) \\\\\n& \\quad - \\exp \\left( \\frac{1}{n} \\sum_{i=1}^n \\cos(2 \\pi s_i) \\right) + 20 + e, \\\\\n\\text{subject to} & \\quad -30 \\leq s_i \\leq 30, \\quad i = 1, 2, \\dots, n.\n\\end{array}\n\\]\n\n\nAckley Function: Pseudocode\n\nThe below example uses a random function to pull a point that we want to hit the local minima. You can imagine, this might not be the best way to do this.\n\nPSEUDOCODE\nFUNCTION Ackley(s):\n    SET a = 20, b = 0.2, c = 2 * pi\n    SET n = length of s\n    COMPUTE sum_sq_term = sum of squares of all elements in s\n    COMPUTE cos_term = sum of cos(2 * pi * each element in s)\n    \n    COMPUTE term1 = -a * exp(-b * sqrt(sum_sq_term / n))\n    COMPUTE term2 = -exp(cos_term / n)\n    \n    RETURN term1 + term2 + a + e\n\n# Main Execution\nSET n = 2  # Dimension of the Ackley function\n\n# Generate random vector s with elements between -30 and 30\nSET s = random values in range [-30, 30] of length n\n\n# Compute the Ackley function result for the vector s\nSET result = Ackley(s)\n\nPRINT vector s\nPRINT Ackley function result for vector s\n\n\n\nAckley Function Python Implementation\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nnp.random.seed(5042)\n\n# Ackley function implementation\ndef ackley(s):\n    a, b, c = 20, 0.2, 2 * np.pi\n    n = len(s)\n    sum_sq_term = np.sum(s**2)\n    cos_term = np.sum(np.cos(c * s))\n    term1 = -a * np.exp(-b * np.sqrt(sum_sq_term / n))\n    term2 = -np.exp(cos_term / n)\n    return term1 + term2 + a + np.e\n\n# Example usage: \nn = 2  # Dimension \ns = np.random.uniform(-30, 30, n)  # Generate random s_i values in the range [-30, 30]\n\nresult = ackley(s)  # Evaluate the Ackley function\n\nprint(f\"Vector s: {s}\")\nprint(f\"Ackley function result: {result}\")\n\n# Visualization of the Ackley function\nx = np.linspace(-30, 30, 400)\ny = np.linspace(-30, 30, 400)\nX, Y = np.meshgrid(x, y)\n\n# Compute Z for the Ackley function\nZ = np.array([ackley(np.array([x_val, y_val])) for x_val, y_val in zip(np.ravel(X), np.ravel(Y))])\nZ = Z.reshape(X.shape)\n\n# Plotting the Ackley function surface\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none')\n\n# Customize the plot\nax.set_title(\"Ackley Function Surface\")\nax.set_xlabel(\"s_1\")\nax.set_ylabel(\"s_2\")\nax.set_zlabel(\"f(s)\")\n\n# Show the plot\nplt.show()\n\nVector s: [12.84779359  3.16468632]\nAckley function result: 17.91745666838746\n\n\n\n\n\n\n\n\n\n\nExample results: Vector s: [12.8 3.16]; Ackley function result: 17.9. These results are random, so it may vary from what you see in the plot.\nDistance from the Origin: The Ackley function reaches its global minimum of 0 at the origin (i.e., when both \\(x_1\\) and \\(x_2\\) are close to 0). Our vector values are quite far from the origin, which is why the function result is positive and relatively large at 17.9‚Äù\nThe Ackley landscape has an exponentially increasing structure as you move away from the global minimum. It has many local minima, which makes optimization algorithms prone to getting stuck in suboptimal solutions. A result like 17.9 is far from zero, and indicates that the vector is located in such a suboptimal region of the function space.\nThus, the Ackley result of 17.9 suggests that the point [12.8 3.16]; is not close to the global minimum (which is zero at the origin) and is located in a region of higher function values.\n\n\n\nDifferential Evolution with the Ackley Function\n\nTo demonstrate how an algorithm does well on the Ackley function, we can use a global optimization algorithm such as Differential Evolution, which is effective for non-convex functions with many local minima.\nDifferential Evolution is a population-based optimization algorithm used for solving complex multidimensional problems. It belongs to the family of evolutionary algorithms, where a population of candidate solutions evolves over time to find the global optimum of a function.\nThe differential_evolution function from the scipy.optimize module is a powerful optimization tool designed to solve global optimization problems. It is a type of evolutionary algorithm, which is used when the function to optimize is non-linear, has many local minima, or is not differentiable.\n\n\nimport numpy as np\nfrom scipy.optimize import differential_evolution\nimport matplotlib.pyplot as plt\n\nnp.random.seed(5042)\n\n# Define the Ackley function\n# Ackley function implementation\ndef ackley(s):\n    a, b, c = 20, 0.2, 2 * np.pi\n    n = len(s)\n    sum_sq_term = np.sum(s**2)\n    cos_term = np.sum(np.cos(c * s))\n    term1 = -a * np.exp(-b * np.sqrt(sum_sq_term / n))\n    term2 = -np.exp(cos_term / n)\n    return term1 + term2 + a + np.e\n\n# Set the bounds for the variables \nbounds = [(-30, 30), (-30, 30)]\n\n# Use differential evolution to minimize the Ackley function\nresult = differential_evolution(ackley, bounds, seed=42)\n\n# Print the result\nprint(f'Optimized parameters (x1, x2): {result.x}')\nprint(f'Function value at minimum: {result.fun}')\n\n\n# Visualization of the Ackley function\nx = np.linspace(-30, 30, 400)\ny = np.linspace(-30, 30, 400)\nX, Y = np.meshgrid(x, y)\n\n# Compute Z for the Ackley function\nZ = np.array([ackley(np.array([x_val, y_val])) for x_val, y_val in zip(np.ravel(X), np.ravel(Y))])\nZ = Z.reshape(X.shape)\n\n# Plotting the Ackley function surface\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none')\n\n# Customize the plot\nax.set_title(\"Ackley Function Surface (2D)\")\nax.set_xlabel(\"s_1\")\nax.set_ylabel(\"s_2\")\nax.set_zlabel(\"f(s)\")\n\n# Show the plot\nplt.show()\n\nOptimized parameters (x1, x2): [0. 0.]\nFunction value at minimum: 4.440892098500626e-16\n\n\n\n\n\n\n\n\n\n\nUsing a great function that is designed to solve problems with multiple local minimums, you can see that we got extremely close to the local minimum 0.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "benchmark.html#multi-objective-optimization-problem-mop",
    "href": "benchmark.html#multi-objective-optimization-problem-mop",
    "title": "Benchmark Optimization Problems",
    "section": "Multi-objective Optimization Problem (MOP)",
    "text": "Multi-objective Optimization Problem (MOP)\n\nGiven a set of functions and a set of constraints, the MOP is to find the optimal value or a set of optimal values (also called Pareto front), subject to the constraints, out of all possible solutions of these functions.\nThe Pareto front consists of solutions where no objective can be improved without worsening at least one other objective. For example, In product design, you might want to minimize cost while maximizing performance. These two objectives are often in conflict, meaning improving one leads to trade-offs in the other.\n\n\\[\\text{opt}\\left( f_1(s), f_2(s), \\dots, f_k(s) \\right),\n\\quad \\mathbf{s} \\in \\mathbb{R}^n,\n\\quad \\text{subject to } c_i(s) \\odot b_i, \\quad i = 1, 2, \\dots, m,\\]\n\nWhere\n\n\\({R}^n\\) and \\({R}\\) are the domain and codomain, respectively,\n\\(f(s) {R}^n\\) and \\({R}\\) is the objective function to be optimized,\n\\(c_i(s): {R}^n\\) and \\({R}\\odot b_i, \\quad i = 1, 2, \\ldots, m,\\) are the constraints,\nand \\(opt\\) and \\(\\odot\\) are as given in Definition 1 as &lt;, &gt;, =, ‚©Ω, or ‚©æ.\n\n\n\nThe Schaffer min-min Multi-objective Optimization Problem\n\nThe Schaffer min-min problem is a well-known test function in the field of multi-objective optimization.\nIt is often used to evaluate optimization algorithms due to its simplicity and well-defined structure. The problem is particularly famous for having a simple Pareto-optimal front.\nThe Schaffer function can be defined as a two-objective optimization problem, where the objectives are functions of a single variable x.\nThe goal is to minimize both of these objective functions simultaneously.\n\n\nCharacteristics and Visualization\n\nConvexity: The Pareto front of the Schaffer min-min problem is convex, making it relatively easy to identify the trade-off surface between the two objectives.\nGraphically, the Pareto front of the Schaffer min-min problem can be visualized as a curve in the objective space, where \\(f_1(x)\\) is plotted against \\(f_2(x)\\) and the curve represents the set of optimal trade-offs between the two objectives.\n\n\n\n\n\n\n\n\n\n\n\n\nscipy.optimize import minimize\n\nminimize is a general-purpose function from scipy.optimize used for finding the minimum of a scalar function.\nUses the BFGS (Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno) algorithm when no specific method is provided. This method is a quasi-Newton optimization algorithm, particularly useful for smooth unconstrained problems.\n\nIt can handle different types of optimization problems, including: Unconstrained minimization\nConstrained minimization (equality and inequality constraints) Bounded minimization (where variables are limited to a certain range)\n\nBasic Workflow:\n\nDefine the objective function (the function to minimize). Choose an initial guess for the variables.\nRun the minimize function with the desired method.\nAnalyze the results: returns optimized variables, the function value, and other diagnostic information.\n\n\n\n\nSchaffer Min-Min Formula\n\\[\n\\min_{s \\in \\mathbb{R}^n}\n\\begin{cases}\nf_1(s) = s^2, \\\\\nf_2(s) = (s - 2)^2,\n\\end{cases}\n\\quad \\text{subject to} \\quad s \\in [-10^3, 10^3].\n\\]\n\n\nSchaffer Min-Min Pseudocode\nPseudocode\n\n# FUNCTION to calculate f1(s)\nFUNCTION f1(s):\n    RETURN s^2\n\n# FUNCTION to calculate f2(s)\nFUNCTION f2(s):\n    RETURN (s - 2)^2\n\n# FUNCTION for combined objective, weighted sum of f1 and f2\nFUNCTION combined_objective(s, w1=0.5, w2=0.5):\n    RETURN w1 * f1(s) + w2 * f2(s)\n\n# MAIN EXECUTION\n# Step 1: Set up the bounds for the solution (s ‚àà [-1000, 1000])\nSET bounds = [-1000, 1000]\n\n# Step 2: Initialize a starting guess for the solution\nSET initial_guess = 0\n\n# Step 3: Minimize the combined objective function using an optimization algorithm\nCALL minimize function with combined_objective, initial_guess, and bounds\nSTORE the result in result\n\n# Step 4: Print the optimization result\nPRINT \"Optimal value of s:\", result.x\nPRINT \"f1(s):\", f1(result.x)\nPRINT \"f2(s):\", f2(result.x)\nPRINT \"Combined objective:\", combined_objective(result.x)\n\n# Step 5: Visualization - Create a range of values for s from -1000 to 1000\n\n\n\nSchaffer Min-Min Python Implementation\n\nPopulation Initialization: A population of random solutions is initialized within the bounds [‚àí1000,1000]. Objective Function Evaluation: For each solution, both objective functions are evaluated.\nScore Combination: The results of the two functions are combined into a single score, which can be minimized. In this case, the combination is a simple sum of f1 and f2.\nOptimization Loop: Iteratively updates the solutions to find the minimum combined score using an optimization technique (e.g., gradient descent, genetic algorithm).\nThe plot uses the weighted sum minimization, w1 * f1(s) + w2 * f2(s) instead of plotting the Pareto front. The plot prints The optimal \\(s\\) value and the corresponding function values \\(f1(s)\\) and \\(f2(s)\\) at that point. The minimum combined objective function value.\n\n\nimport numpy as np\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\n\n\n# Define the two objective functions for the Schaffer problem\ndef f1(s):\n    return s**2\n\ndef f2(s):\n    return (s - 2)**2\n\n# Combined objective function: weighted sum of f1 and f2\n# You can adjust the weights to explore different trade-offs between the two objectives\ndef combined_objective(s, w1=0.5, w2=0.5):\n    return w1 * f1(s) + w2 * f2(s)\n\n# Bounds for the solution (s ‚àà [-1000, 1000])\nbounds = [(-1000, 1000)]\n\n# Initial guess for the solution\ninitial_guess = np.array([0])\n\n# Use scipy's minimize function to find the solution\nresult = minimize(combined_objective, initial_guess, bounds=bounds)\n\n# Print the result\nprint(\"Optimal value of s:\", result.x[0])\nprint(\"f1(s):\", f1(result.x[0]))\nprint(\"f2(s):\", f2(result.x[0]))\nprint(\"Combined objective:\", combined_objective(result.x[0]))\n\n# Visualization of the objective functions and combined objective\ns_values = np.linspace(-1000, 1000, 400)\nf1_values = f1(s_values)\nf2_values = f2(s_values)\ncombined_values = combined_objective(s_values)\n\n# Plotting\nplt.figure(figsize=(10, 6))\n\n# Plot f1(s), f2(s), and combined objective\nplt.plot(s_values, f1_values, label=\"f1(s) = s^2\", color='blue')\nplt.plot(s_values, f2_values, label=\"f2(s) = (s - 2)^2\", color='green')\nplt.plot(s_values, combined_values, label=\"Combined Objective\", color='red', linestyle='--')\n\n# Mark the optimal solution found\nplt.axvline(x=result.x[0], color='black', linestyle=':', label=f\"Optimal s = {result.x[0]:.2f}\")\n\n# Customize the plot\nplt.title(\"Schaffer Min-Min Problem Visualization\")\nplt.xlabel(\"s\")\nplt.ylabel(\"Objective Function Value\")\nplt.legend()\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\nOptimal value of s: 1.0000000134831095\nf1(s): 1.0000000269662193\nf2(s): 0.9999999730337812\nCombined objective: 1.0000000000000002\n\n\n\n\n\n\n\n\n\n\n\nLooking at the results\n\nThe results you achieved for the Schaffer Min-Min problem look excellent, as they closely approximate the optimal solution.\nOptimal value of \\(s\\) we found is nearly exactly \\(1\\), the known optimal solution for the Schaffer function.\n\nObjective function values: \\(f_1(s) = s^2 = 1.000000027\\)\n: \\(f_2(s) = (s-2)^2 = 0.999999973\\)\nThese values are very close to 1 for both \\(f_1\\) and \\(f_2\\), indicating that the function values at this \\(s\\) are near-optimal.\n\nCombined objective: The combined objective (likely calculated as a weighted sum or some other combination of \\(f_1\\) and \\(f_2\\) is 1.00000, which is extremely close to the expected combined optimal value of 1. This negligible difference suggests that the optimization algorithm has performed very well.\nHere is the code used to generate the Pareto Front for the Schaffer Min-Min problem, filtering out only the non-dominated solutions and plotting the trade-off curve. This allows us to see \\(f_1\\) and \\(f_2\\) on the x and y axis.\nThere are so many ways to graph this, but the multiple dimensions makes it more difficult to see. Explore some options using generative AI.\n\n\n# Generate s values\ns_values = np.linspace(-10, 10, 400)\n\n# Identifying the non-dominated solutions (Pareto front)\npareto_f1 = []\npareto_f2 = []\n\nfor s in s_values:\n    f1_val = f1(s)\n    f2_val = f2(s)\n\n    # A point (f1, f2) is Pareto-optimal if no other point dominates it\n    if not any(other_f1 &lt;= f1_val and other_f2 &lt;= f2_val for other_f1, other_f2 in zip(pareto_f1, pareto_f2)):\n        pareto_f1.append(f1_val)\n        pareto_f2.append(f2_val)\n\n# Sorting to ensure a smooth Pareto front curve\npareto_f1, pareto_f2 = zip(*sorted(zip(pareto_f1, pareto_f2)))\n\n# Plotting the Pareto front with a more visible optimal point\nplt.figure(figsize=(8, 6))\nplt.plot(pareto_f1, pareto_f2, marker='o', linestyle='-', color='red', label=\"Pareto Front\")\n\n# Mark the optimal solution (1,1) with a larger marker and annotation\nplt.scatter(1, 1, color='black', marker='o', s=100, label=\"Optimal (1,1)\")\nplt.annotate(\"Optimal (1,1)\", xy=(1, 1), xytext=(10, 20),\n             textcoords=\"offset points\", fontsize=12, color='black',\n             arrowprops=dict(facecolor='black', shrink=0.05))\n\n# Customize the plot\nplt.title(\"Pareto Front of the Schaffer Min-Min Problem\", fontsize=14)\nplt.xlabel(\"f1(s) = s^2\", fontsize=12)\nplt.ylabel(\"f2(s) = (s - 2)^2\", fontsize=12)\nplt.legend(fontsize=12)\nplt.grid(True)\n\n# Show the plot\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "es.html",
    "href": "es.html",
    "title": "Exhaustive Search",
    "section": "",
    "text": "Advantages and Disadvantages of ES",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "es.html#advantages-and-disadvantages-of-es",
    "href": "es.html#advantages-and-disadvantages-of-es",
    "title": "Exhaustive Search",
    "section": "",
    "text": "Advantages:\n\nGuaranteed Optimal Solution: Finds the best possible solution, ensuring optimality.\nSimplicity: Easy to understand and implement for small-scale problems.\n\nDisadvantages:\n\nScalability Issues: Infeasible for large datasets due to exponential time complexity.\nResource Intensive: High computational cost in terms of time and memory.\n\n\n\nExhaustive Search Algorithm\n\nCheck all the candidate solutions in the solution space of the problem in question; therefore, it is always capable of finding the best solution for the problem in question.\n\n\n\n\nes algorithm",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "es.html#exhaustive-search-in-continuous-vs.-discrete-domains",
    "href": "es.html#exhaustive-search-in-continuous-vs.-discrete-domains",
    "title": "Exhaustive Search",
    "section": "Exhaustive Search in Continuous vs.¬†Discrete Domains",
    "text": "Exhaustive Search in Continuous vs.¬†Discrete Domains\n\nDiscrete Domains: Solutions can be explicitly enumerated without discretization.\n\nExample: Knapsack problem.\n\nContinuous Domains: Require discretization (using a step size) to make exhaustive search feasible.\n\nExample: Ackley function.\n\nChallenges in Continuous Domains:\n\nInfinite solution space makes direct enumeration impossible.\nStep size is critical for balancing resolution and computational cost.\n\nNote that Discretization is the process of converting continuous data or variables into discrete categories or bins.\n\n\nRole of Step Size in Continuous Optimization\n\nStep size in exhaustive search is the distance between consecutive sample points in the search space. A small step size results in a finer grid with higher precision but increases computation time, while a larger step size reduces computation time but may miss optimal solutions.\nWhy Step Size Matters: Controls the granularity of the search.\n\nSmaller steps increase resolution but significantly raise computation time.\nLarger steps reduce computational cost but risk missing the optimal solution.\n\nPractical Considerations:\n\nOptimal step size depends on the problem‚Äôs scale and the required precision.\nTrade-off between efficiency and accuracy.\n\nWhen is Exhaustive Search Practical?\n\nDiscrete Domains:\n\nFeasible if the solution space is small (manageable number of combinations).\nGuarantees finding the exact optimal solution.\n\nContinuous Domains:\n\nOnly practical when the domain is small and can be discretized effectively.\n\nHigher dimensions increase complexity exponentially (curse of dimensionality).",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "es.html#the-search-strategy-of-es",
    "href": "es.html#the-search-strategy-of-es",
    "title": "Exhaustive Search",
    "section": "The Search Strategy of ES",
    "text": "The Search Strategy of ES",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "es.html#algorithmic-steps-of-es",
    "href": "es.html#algorithmic-steps-of-es",
    "title": "Exhaustive Search",
    "section": "Algorithmic Steps of ES",
    "text": "Algorithmic Steps of ES\n\nFor each binary solution (up to num_bits length), the code evaluates the number of 1‚Äôs in its binary representation (which is used as the ‚Äúfitness‚Äù value).\nIt then finds the solution with the highest number of 1‚Äôs (the one with the most bits set to 1).\nImport necessary tools: time for timing and matplotlib for plotting results.\nSet up run function\n\nLoop through all possible solutions.\nEvaluate each one and track its fitness.\nUpdate the best solution as needed.\nReturn the best solution and fitness progress.\n\nDefine helper functions:\n\nTransit: Move to the next solution.\nEvaluate: Count the number of 1s in the binary version of the solution (higher is better).\nDetermine: Compare fitness in order to track the best solution found so far.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "es.html#imports",
    "href": "es.html#imports",
    "title": "Exhaustive Search",
    "section": "Imports",
    "text": "Imports\n\nimport time\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "es.html#run_exhaustive_search-function",
    "href": "es.html#run_exhaustive_search-function",
    "title": "Exhaustive Search",
    "section": "run_exhaustive_search function",
    "text": "run_exhaustive_search function\n\nrun_exhaustive_search(num_bits):\nCalculates the total number of possible solutions max_sol is the maximum possible solution, which is \\(2^{numbits}\\). \\(2 ** numbits\\): This correctly represents the total number of possible combinations when you have numbits binary bits. For example, if numbits = 3, the possible solutions range from \\(000 (0)\\) to \\(111 (7)\\), resulting in \\(2^3=8\\) solutions.\nInitializes \\(s\\) is the current solution, initially set to 0.\nInitializes \\(best_fitness\\) and \\(best_solution\\) to track the best results.\nInitializes an empty list \\(fitness_over_time\\) to track the fitness at each step.\n\n\n# Function for initialization (I)\ndef init_es(num_bits=10):\n    max_sol = 2 ** num_bits  # Total number of solutions\n    s = 0  # Step 1: Set the initial solution s = (0)\n    f_s = evaluate(s)  # Step 2: Evaluate initial solution\n    return s, f_s, max_sol",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "es.html#helper-functions",
    "href": "es.html#helper-functions",
    "title": "Exhaustive Search",
    "section": "Helper Functions",
    "text": "Helper Functions\n\ntransit(s): Simply increments the current solution by 1.\n\n\n# Function for transit (T)\ndef transit(s):\n    s += 1\n    return s\n\n\nevaluate(s): Evaluates the ‚Äúfitness‚Äù of the solution by counting how many 1‚Äôs are present in its binary representation. The more 1‚Äôs, the better the solution.\n\n\n# Function for evaluation (E)\ndef evaluate(s):\n    return bin(s).count(\"1\")  # Counts the number of 1s in the binary representation of s\n\n\ndetermine(fv, v, fs, s): This method checks whether the fitness of the new solution (fv) is greater than the current best fitness (fs). If so, it updates the current best solution and fitness.\n\n\n# Function for determine (D)\ndef determine(fv, v, fs, s):\n    if fv &gt; fs:\n        fs, s = fv, v\n    return fs, s",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "es.html#main-loop",
    "href": "es.html#main-loop",
    "title": "Exhaustive Search",
    "section": "Main Loop",
    "text": "Main Loop\n\nA while loop iterates through all possible solutions (represented by s). For each solution s, it calculates the fitness using the evaluate() function.\n\nFor each iteration, the solution is updated (transit method), evaluated (evaluate method), and the best solution is determined (determine method).\nThe loop continues until all possible solutions (v from 0 to max_sol) have been evaluated.\n\nfitness is the ‚Äúfitness‚Äù score of the current solution, calculated using evaluate(s) (which counts the number of 1‚Äôs in the binary representation of s).\nAppends the fitness value to fitness_over_time.\ndetermine() updates fs (fitness) and s (solution) if the new solution (v) has a higher fitness.\nIf the current solution has a better fitness than the best found so far, it updates the best fitness and solution.\nReturn Values: Returns fitness_over_time and best_solution after completing the exhaustive search.\n\n\n# Run function for exhaustive search\ndef run_exhaustive_search(num_bits=10):\n    # Initialize using init_es\n    s, f_s, max_sol = init_es(num_bits)\n    \n    #Set v = s\n    best_solution = s\n    best_fitness = f_s\n    \n    fitness_over_time = []\n    \n    # While the termination criterion is not met\n    while s &lt; max_sol:\n        # Generate the next solution v = GenNext(v)\n        v = s\n        \n        # Evaluate the new solution\n        f_v = evaluate(v)\n        \n        # Track fitness over time for plotting\n        fitness_over_time.append(f_v)\n        \n        # Print current solution and its fitness: Print command commented out for simplicity because it prints all the comparisons. \n        #print(f\"{f_v} # {v:0{num_bits}b}\")\n        \n        #If f_v is better than f_s, update s = v, f_s = f_v\n        best_fitness, best_solution = determine(f_v, v, best_fitness, best_solution)\n\n        #continued: Move to the next solution\n        s += 1  # Increment solution\n    \n    #Returned for Output the best solution\n    return fitness_over_time, best_solution",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "es.html#main-execution",
    "href": "es.html#main-execution",
    "title": "Exhaustive Search",
    "section": "Main Execution",
    "text": "Main Execution\n\nIn exhaustive search, the ‚Äúmain execution‚Äù often focuses on setting the number of bits because exhaustive search involves systematically evaluating all possible solutions in the search space. When the solutions are represented as binary strings, the number of bits directly determines the size of the search space.\nWhy it matters:\n\nDefining the Search Space: The number of bits defines how many unique binary strings (or configurations) are possible. For \\(2^n\\) possible combinations. So, by setting the number of bits, you are effectively defining the boundaries of the search space.\nEnumerating All Possibilities: In exhaustive search, the algorithm needs to evaluate every possible configuration to ensure the optimal solution is found. Each unique binary string corresponds to a specific candidate solution, so having the number of bits set enables the algorithm to enumerate all possible configurations.\nComputational Complexity: The number of bits directly affects the computational complexity of exhaustive search. With \\(n\\) bits, the search space grows exponentially, which is manageable for small \\(n\\) but quickly becomes infeasible for larger \\(n\\). By controlling the number of bits, you also control the practical feasibility of the exhaustive search.\n\n\nSimplicity: Since exhaustive search doesn‚Äôt involve complex heuristics or probabilistic methods, setting the number of bits becomes the main task. The rest of the algorithm is straightforward: generate each binary string, evaluate its objective value, and keep track of the best solution.\n\n# Main execution\nnum_bits = 10  # You can adjust the number of bits",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "es.html#output",
    "href": "es.html#output",
    "title": "Exhaustive Search",
    "section": "Output",
    "text": "Output\n\n# Run the exhaustive search and get fitness values\nstart_time = time.time()\nfitness_over_time, best_solution = run_exhaustive_search(num_bits)\nend_time = time.time()\nexecution_time = end_time - start_time  # Calculate elapsed time\n\n# Output (O)\nprint(f\"Exhaustive Search One-Max Time elapsed: {execution_time:.6f} seconds\")  # Print the elapsed time\nprint(f\"# name of the search algorithm: Exhaustive Search\")\nprint(f\"# number of bits: {num_bits}\")\n\n# Plot the evolution of fitness over time\nplt.figure(figsize=(10, 6))\nplt.plot(fitness_over_time, label='Fitness over time', color='blue', linewidth=2)\nplt.title(f'Exhaustive Search: Fitness Evolution ({num_bits} bits)')\nplt.xlabel('Evaluations')\nplt.ylabel('Fitness (Number of 1s)')\nplt.grid(True)\nplt.legend()\nplt.show()\n\nExhaustive Search One-Max Time elapsed: 0.000000 seconds\n# name of the search algorithm: Exhaustive Search\n# number of bits: 10",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "hc.html",
    "href": "hc.html",
    "title": "Hill Climbing",
    "section": "",
    "text": "Hill Climbing (HC)",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#search-strategy-of-hc",
    "href": "hc.html#search-strategy-of-hc",
    "title": "Hill Climbing",
    "section": "Search Strategy of HC >",
    "text": "Search Strategy of HC &gt;\n\nThe search strategy of HC that accepts only a better solution as the next solution.\nSuppose HC has a 50/50 chance to move either left or right in solving the one-max problem; then, in addition to the global optimum (1111), HC may end up in one of the seven or three local optima. \\(f(v)&gt;f(s)\\).\n\n\n\n\nhc search",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#search-strategy-of-hc-1",
    "href": "hc.html#search-strategy-of-hc-1",
    "title": "Hill Climbing",
    "section": "Search Strategy of HC >=",
    "text": "Search Strategy of HC &gt;=\n\nThe search strategy of HC accepts both better solutions and solutions that are equally good as the next solution.\nThe possible result if HC accepts not only a better solution but also a solution that is equally good (i.e., \\(f(v)&gt;=f(s)\\) as the next solution; then only the solutions \\(x_4\\) \\((0011)\\), \\(x_8\\) \\((0111)\\), and \\(x_12\\) \\((1011)\\) will remain as the local optima, while \\(x_16\\) \\((1111)\\) is the global optimum.\n\n\n\n\nhc search",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#hill-climbing-algorithm",
    "href": "hc.html#hill-climbing-algorithm",
    "title": "Hill Climbing",
    "section": "Hill Climbing Algorithm",
    "text": "Hill Climbing Algorithm\n\nStart: Initialize with a random solution or a predefined starting point.\nEvaluate: Assess the quality of the current solution using a heuristic function.\nGenerate Neighbors: Produce a set of neighboring solutions by making small changes.\nSelect Best Neighbor: Choose the neighbor that has the highest heuristic value.\nMove: Replace the current solution with the selected neighbor.\nRepeat: Continue the process until no better neighbors are found or a stopping criterion is met.\n\n\n\n\nhc algo",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#challenges-of-hc",
    "href": "hc.html#challenges-of-hc",
    "title": "Hill Climbing",
    "section": "Challenges of HC",
    "text": "Challenges of HC\n\nLocal Maximum: Hill climbing may get stuck at a local maximum, where no neighboring solution improves, but better solutions exist further away.\nPlateaus: It can struggle on flat regions where no clear direction of improvement is evident.\nRidges: Difficulties in navigating narrow ridges that require moving sideways to find a better peak.\nVariations to Overcome Limitations\n\nStochastic Hill Climbing: Introduces randomness to avoid local maxima.\nSimulated Annealing: Uses probabilistic decisions to escape local maxima and explore a larger solution space.\nSteepest-Ascent Hill Climbing: Considers all neighbors and selects the one with the steepest ascent.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#set-up-run-function",
    "href": "hc.html#set-up-run-function",
    "title": "Hill Climbing",
    "section": "Set up Run Function",
    "text": "Set up Run Function\n\nEvaluate the initial solution by counting the number of 1s, which serves as the ‚Äúfitness‚Äù value.\nSearch for Neighbors: Generate neighboring solutions by flipping one bit at a time in the current solution. For each neighbor, calculate its fitness (number of 1s in its binary representation).\nSelect Best Neighbor: Identify the neighboring solution with the highest fitness.If this neighbor‚Äôs fitness is better than the current solution‚Äôs fitness, update the current solution to this neighbor.\nRepeat Until Convergence: Continue generating and evaluating neighbors until no neighboring solution improves the current fitness (local maximum reached).Track the fitness of the best solution at each iteration for plotting or analysis.\nReturn Best Solution and Fitness Progress: Output the best solution found along with a list of fitness values over iterations.\n\n\n# Run function for hill climbing\ndef run_hc(num_bits=10, max_evals=2 ** num_bits):\n    sol = init_hc(num_bits)  # Initialize random solution\n    fitness = evaluate(sol)  # Evaluate the initial solution\n    fitness_over_time = []\n    \n    eval_count = 0\n\n    # Main loop of evaluations\n    while eval_count &lt; max_evals:\n        tmp_sol = transit(sol)  # Make a random change (flip a bit)\n        tmp_fitness = evaluate(tmp_sol)  # Evaluate the new solution\n        sol, fitness = determine(tmp_sol, tmp_fitness, sol, fitness)  # Determine if we accept the new solution\n        \n        fitness_over_time.append(fitness)  # Track the fitness over time\n        eval_count += 1\n        \n    return fitness_over_time, sol",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#helper-functions",
    "href": "hc.html#helper-functions",
    "title": "Hill Climbing",
    "section": "Helper Functions:",
    "text": "Helper Functions:\n\nInitiate: Set Initial Solution: Randomly choose an initial binary solution of the specified bit length (num_bits).\n\n\n# Function for initialization (I)\ndef init_hc(num_bits):\n    return np.random.randint(2, size=num_bits, dtype=int)\n\n\nTransit (Generate Neighbors): Create a function that generates all neighbors of a binary solution by flipping each bit one by one.\n\n\n# Function for transit: (T) \n# Flipping a random bit in the solution\ndef transit(sol):\n    new_sol = sol.copy()\n    flip_index = np.random.randint(len(sol))\n    new_sol[flip_index] = 1 - new_sol[flip_index]  # Flip the bit (0 to 1, or 1 to 0)\n    return new_sol\n\n\nEvaluate: Count the number of 1s in the binary representation of a solution (fitness value).\n\n\n# Function for evaluation: (E) \n# Sum the number of 1s (OneMax problem)\ndef evaluate(sol):\n    return np.sum(sol)\n\n\nDetermine: Track and update the current best solution and its fitness as the search progresses.\n\n\n# Function for determine: (D) \n# Decide whether to accept the new solution\ndef determine(tmp_sol, tmp_fitness, sol, fitness):\n    if tmp_fitness &gt; fitness:\n        return tmp_sol, tmp_fitness  # Accept the new solution\n    return sol, fitness  # Keep the current solution\n\n\nThe main execution section initiates the hill climbing algorithm, recording the time taken to execute it. It captures the progression of fitness values over each iteration and identifies the best solution found, while calculating the total execution time.\n\n\n# Main Execution\n# Run hill climbing and get fitness values\nstart_time = time.time()\nfitness_over_time, best_solution = run_hc(num_bits)\nend_time = time.time()\nexecution_time = end_time - start_time  # Calculate elapsed time\n\n\nThe output section displays key details about the hill climbing algorithm, including its name, the number of bits used, and the time taken for execution. It then visualizes the fitness evolution across evaluations, providing a clear plot that shows how fitness values change over time as the algorithm progresses toward the best solution.\n\n\n# Output and visualize\nprint(f\"# Name of the search algorithm: Hill Climbing\")\nprint(f\"# number of bits: {num_bits}\")\nprint(f\"Time elapsed: {execution_time:.6f} seconds\")  # Print the elapsed time\n\n# Plot the evolution of fitness over time\nplt.figure(figsize=(10, 6))\nplt.plot(fitness_over_time, label='Fitness over time', color='blue', linewidth=2)\nplt.title(f'Hill Climbing: Fitness Evolution ({num_bits} bits)')\nplt.xlabel('Evaluations')\nplt.ylabel('Fitness (Number of 1s)')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n# Name of the search algorithm: Hill Climbing\n# number of bits: 10\nTime elapsed: 0.003999 seconds\n\n\n\n\n\n\n\n\n\n\nIn the chart above, the fitness score, which measures the number of 1s in a 10-bit solution, is shown over a series of evaluations. Initially, the fitness rapidly increases, suggesting that the hill climbing algorithm quickly finds improvements in the solution. Once it reaches the maximum fitness of 10, it stabilizes and remains flat, indicating that the algorithm has found an optimal solution and no further improvements are being made.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#comparison-of-es-and-hc-for-the-one-max-problem-of-size-n10",
    "href": "hc.html#comparison-of-es-and-hc-for-the-one-max-problem-of-size-n10",
    "title": "Hill Climbing",
    "section": "Comparison of ES and HC for the one-max Problem of size \\(n=10\\)",
    "text": "Comparison of ES and HC for the one-max Problem of size \\(n=10\\)\n\nHC-LR and HC-Rand to denote a similar thing. HC-Rand and HC-LR differ primarily in how they choose the next solution and handle exploration, impacting their likelihood of getting stuck in local optima.\nHC-LR (Hill Climbing with Limited Range): HC-LR‚Äôs transition operator restricts it to move only to adjacent solutions, which makes it less flexible. The solution v of HC-LR will be the one that is one smaller or one larger than the current solution s (i.e., \\(v=s‚àí1\\) or \\(v=s+1\\)). As described in the example, if HC-LR starts with the solution ‚Äú1000,‚Äù it can only move to ‚Äú0111‚Äù or ‚Äú1001.‚Äù If neither of these options provides an improvement, HC-LR is likely to get stuck there, leading to a local optimum. Because HC-LR only accepts moves that improve the solution, it has a high risk of getting trapped in suboptimal points without exploring further.\nHC-Rand (Hill Climbing with Randomization): HC-Rand randomly selects a part of the current solution to invert, allowing it to explore a broader range of possible next solutions. This random approach reduces the chance of getting stuck in a local optimum since the algorithm has the flexibility to try different options, even if they are not immediately better. HC-Rand‚Äôs randomness increases the probability of escaping local optima, thus improving the chances of finding a global optimum.\n\nThe solution v of HC-Rand will be created by inverting a randomly chosen subsolution of s (i.e., ‚Äú1‚Äù becomes ‚Äú0‚Äù and ‚Äú0‚Äù becomes ‚Äú1‚Äù). The transition operator in the HC-Rand (Hill Climbing with Randomization) algorithm randomly selects a part (subsolution) of the current solution and inverts the chosen bit(s). For a ‚Äúone-max problem‚Äù of a specific size (where the goal is to maximize the number of 1s in the solution), inverting different bits can produce several possible next solutions.\nIn the example given, there are four possible new solutions because there are four different bits that could be inverted from the current solution. If inverting one bit doesn‚Äôt improve the objective value (e.g., the number of 1s), HC-Rand will consider other options, allowing it to avoid getting stuck in local optima. This random inversion helps HC-Rand explore other solutions, increasing the chances of finding the global maximum (an optimal solution with the highest number of 1s).\n\nIn both cases, only a better solution will be accepted as the next solution \\(v\\).\n\n\n\n\nhc search",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#deciphering-probability-of-switching",
    "href": "hc.html#deciphering-probability-of-switching",
    "title": "Hill Climbing",
    "section": "Deciphering Probability of Switching",
    "text": "Deciphering Probability of Switching\n\nThe landscape of the ‚Äúsolution space‚Äù or ‚Äúsearch space‚Äù of an optimization problem seen by a search can be different when:\n\ndifferent ways are used to represent the solutions or\ndifferent ways are used to generate new candidate solutions from current solutions.\n\nThe solution space that a search algorithm sees can also be called the ‚Äúsearch space.‚Äù\nHC-Rand-M: If we use ‚Äú&gt;=‚Äù instead of ‚Äú&gt;‚Äù in the comparison of the current solution and the possible next solution for solving the BSD-2 problem of sizes n=4 and n=10.\nThe results of HC-Rand-M show that with this modification, HC-Rand will be able to find the optimal solution in most cases.\n\n\n\n\nhc search",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#load-imports",
    "href": "hc.html#load-imports",
    "title": "Hill Climbing",
    "section": "Load Imports",
    "text": "Load Imports\n\nimport numpy as np\nimport time\nimport random\nimport matplotlib.pyplot as plt\n\nnp.random.seed(5042) ##for consistency",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#define-the-ackley-function-1d",
    "href": "hc.html#define-the-ackley-function-1d",
    "title": "Hill Climbing",
    "section": "Define the Ackley Function (1D)",
    "text": "Define the Ackley Function (1D)\n\nFunction f(x): This is the function we want to optimize.\nIn this case, Ackley function with provided formula.\n\n\n# Ackley function in 1D\ndef ackley(x):\n    a = 20\n    b = 0.2\n    c = 2 * np.pi\n    term1 = -a * np.exp(-b * np.sqrt(np.mean(np.square(x))))\n    term2 = -np.exp(np.mean(np.cos(c * np.array(x))))\n    return term1 + term2 + a + np.exp(1)",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#main-loop",
    "href": "hc.html#main-loop",
    "title": "Hill Climbing",
    "section": "Main Loop",
    "text": "Main Loop\n\nThis is where the hill climbing algorithm repeatedly evaluates candidate solutions, compares them with the current solution, and decides whether to update the current solution. This loop is crucial because it drives the optimization process by iterating over a fixed number of steps or until a stopping criterion is met.\nThe current solution is set to start_x, which was initialized earlier. The Ackley value for this starting solution is calculated and stored in current_value.x_history and value_history keep track of all the solutions and function values (fitness) encountered during the optimization process.\nThe hill climbing loop runs for a fixed number of iterations (max_iters), which controls how many optimization steps are performed. Each iteration represents one optimization step, where two new candidate solutions are generated and evaluated.\nThe transit function generates two neighboring solutions, left_x and right_x, by subtracting and adding the step_size (0.1 by default) to the current solution (current_x). These represent potential moves to the left and right in the search space.\nBoth neighboring solutions are evaluated using the Ackley function, resulting in their corresponding values (left_value and right_value). These values represent how ‚Äúgood‚Äù the new solutions are (lower is better in this minimization problem). Then we compare all three solutions (current, left, and right).\n\nThe np.argmin(values) function finds the index of the minimum value (best solution) from the three candidates.\nThe determine function then decides whether to update the current solution (current_x) to the new best solution.\n\nThe current solution and its corresponding function value are appended to the history lists (x_history and value_history). These lists will be used later to plot the progress of the algorithm.\nFinally, we return the results.\n\n\n# Main hill climbing loop for Ackley function in 1D\ndef hill_climbing_ackley(start_x, max_iters=100, step_size=0.1):\n    current_x = start_x\n    current_value = evaluate(current_x)\n    x_history = [current_x]\n    value_history = [current_value]\n\n    for _ in range(max_iters):\n        left_x, right_x = transit(current_x, step_size)\n        left_value = evaluate(left_x)\n        right_value = evaluate(right_x)\n\n        values = [current_value, left_value, right_value]\n        x_candidates = [current_x, left_x, right_x]\n\n        best_index = np.argmin(values)\n        current_x, current_value = determine(values[best_index], current_value, x_candidates[best_index], current_x)\n\n        x_history.append(current_x)\n        value_history.append(current_value)\n\n    return current_x, current_value, x_history, value_history",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#helper-functions-1",
    "href": "hc.html#helper-functions-1",
    "title": "Hill Climbing",
    "section": "Helper Functions",
    "text": "Helper Functions\n\nInitialize: This function initializes the hill climbing process by selecting a random starting point (start_x) within the given range [-10, 10]. This represents the initial solution that hill climbing will start with.\n\n\n# Initialization function (I) to set the starting point\ndef init_hc(range_min=-10, range_max=10):\n    return random.uniform(range_min, range_max)\n\n\nTransition: This function generates two new candidate solutions by taking small steps (of size 0.1) to the left and right of the current solution (current_x). These represent possible transitions to neighboring points in the search space.\n\n\n# Transition function (T)\ndef transit(current_x, step_size=0.1):\n    return current_x - step_size, current_x + step_size\n\n\nEvaluation: This function evaluates the fitness of a solution (sol) using the Ackley function. The goal of the hill climbing algorithm is to minimize this value. The Ackley function is typically used for testing optimization algorithms due to its complex landscape with many local minima.\n\n\n# Evaluation function (E) to evaluate fitness (Ackley value in this case)\ndef evaluate(sol):\n    return ackley([sol])\n\n\nDetermination: This function compares the current solution‚Äôs value (current_value) with the value of a new candidate solution (new_value). If the new solution is better (i.e., has a lower Ackley value), it becomes the new current solution. Otherwise, the current solution is retained.\n\n\n# Determination (D)\n# Function to decide whether to accept the new solution\ndef determine(new_value, current_value, new_x, current_x):\n    return (new_x, new_value) if new_value &lt; current_value else (current_x, current_value)",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#main-execution",
    "href": "hc.html#main-execution",
    "title": "Hill Climbing",
    "section": "Main Execution",
    "text": "Main Execution\n\nThe process starts by initializing start_x using the Initialization function (init_hc). The main function (hill_climbing_ackley) then runs for a fixed number of iterations (100 by default) using Transition to generate candidate solutions, Evaluation to evaluate them, and Determination to decide whetherr to accept a new solution.\n\n\n# Main execution\nstart_x = init_hc()\nstart_time = time.time()\noptimal_x, optimal_value, x_history, value_history = hill_climbing_ackley(start_x)\nend_time = time.time()\nexecution_time = end_time - start_time",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#output",
    "href": "hc.html#output",
    "title": "Hill Climbing",
    "section": "Output",
    "text": "Output\n\nThe first part prints the optimal solution found (optimal_x), the corresponding function value (optimal_value), and the total time taken for the execution.\nThe second part plots the Ackley function over the range [-10, 10] and overlays the hill climbing progress. The points in red represent the steps taken by the hill climbing algorithm as it navigates through the search space.\n\n\n# Output (O)\nprint(f\"Optimal x: {optimal_x}\")\nprint(f\"Optimal value: {optimal_value}\")\nprint(f\"Execution time: {execution_time:.6f} seconds\")\n\n\n# Plot the Ackley function and hill climbing progress\nx_range = np.linspace(-10, 10, 1000)\ny_values = [ackley([x]) for x in x_range]\n\nplt.plot(x_range, y_values, label=\"Ackley Function\")\nplt.plot(x_history, value_history, 'ro--', label=\"Hill Climbing Progress\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.title(\"Hill Climbing on Ackley Function in 1D\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nOptimal x: 2.967380778692627\nOptimal value: 9.008226661042103\nExecution time: 0.003000 seconds\n\n\n\n\n\n\n\n\n\n\nAn example run of this put the points Optimal x: 5.03 and Optimal Value: 12.75, are suboptimal for the Ackley function.\nFor the Ackley function in 1D, the global minimum occurs at \\(x=0\\), and the function value at this point is exactly \\(f(0)=0\\).\nOur results are not close to the global minimum of \\(x=0\\). Any value greater than zero indicates that the algorithm has not found the true global minimum.\nThe blue line represents the Ackley function over a range of \\(x\\) values from -10 to 10.The red dots connected by dashed lines represent the progress of the hill climbing algorithm as it iterates through neighboring solutions.\nThe Ackley function is known for its many local minima, and hill climbing is a greedy optimization method that tends to get stuck in local minima because it only moves to the immediate best neighboring solution. Once it finds a local minimum, it stops, even if it hasn‚Äôt found the global minimum.\nIn this case, the result you obtained is a local minimum but not the global minimum. Since \\(f(0)=0\\) is the global minimum, and your result is \\(f(5.03)= 12.75\\), the algorithm likely got stuck in a local minimum during its search.\nSimulated Annealing is a more advanced version of hill climbing that sometimes accepts worse solutions to escape local optima. Will discuss later.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#imports",
    "href": "hc.html#imports",
    "title": "Hill Climbing",
    "section": "Imports",
    "text": "Imports\n\n## Hill Climbing Finance example\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time\n\nnp.random.seed(5042)",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#fetching-data",
    "href": "hc.html#fetching-data",
    "title": "Hill Climbing",
    "section": "Fetching Data",
    "text": "Fetching Data\n\nstocks = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA', 'NFLX', 'NVDA', 'META', 'DIS', 'BA'] \nstart_date = '2023-10-01'\nend_date = '2024-10-1'\n\n# Fetch historical stock data\ndef fetch_data(stocks, start_date, end_date):\n    data = yf.download(stocks, start=start_date, end=end_date)['Adj Close']\n    return data",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#calculating-performance-via-sharpe-ratio",
    "href": "hc.html#calculating-performance-via-sharpe-ratio",
    "title": "Hill Climbing",
    "section": "Calculating performance via Sharpe Ratio",
    "text": "Calculating performance via Sharpe Ratio\n\n# Calculate portfolio performance (returns, risk, and Sharpe ratio)\ndef portfolio_performance(weights, mean_returns, cov_matrix):\n    returns = np.sum(mean_returns * weights) * 252  # Annualized returns\n    risk = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) * np.sqrt(252)  # Annualized risk\n    sharpe_ratio = returns / risk  # Sharpe ratio\n    return returns, risk, sharpe_ratio",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#main-loop-1",
    "href": "hc.html#main-loop-1",
    "title": "Hill Climbing",
    "section": "Main Loop",
    "text": "Main Loop\n\n# Hill Climbing algorithm for portfolio optimization\ndef hill_climbing(mean_returns, cov_matrix, max_iterations=1000):\n    num_assets = len(mean_returns)\n    # Initial random portfolio\n    current_weights = np.random.random(num_assets)\n    current_weights /= np.sum(current_weights)  # Normalize to sum to 1\n    current_returns, current_risk, current_sharpe = portfolio_performance(current_weights, mean_returns, cov_matrix)\n    \n    # Track progress for visualization\n    score_history = [current_sharpe]\n    iteration = 0\n\n    # While loop for hill climbing\n    while iteration &lt; max_iterations:\n        # Generate neighbor: Slight random change in weights\n        neighbor_weights = current_weights + np.random.normal(0, 0.01, num_assets)\n        neighbor_weights = np.clip(neighbor_weights, 0, 1)  # Ensure weights are between 0 and 1\n        neighbor_weights /= np.sum(neighbor_weights)  # Normalize weights to sum to 1\n        \n        neighbor_returns, neighbor_risk, neighbor_sharpe = portfolio_performance(neighbor_weights, mean_returns, cov_matrix)\n        \n        # If the neighbor is better, move to the neighbor solution\n        if neighbor_sharpe &gt; current_sharpe:\n            current_weights, current_returns, current_risk, current_sharpe = neighbor_weights, neighbor_returns, neighbor_risk, neighbor_sharpe\n            score_history.append(current_sharpe)\n        \n        iteration += 1  # Increment the iteration counter\n    \n    return current_weights, current_sharpe, score_history\nstock_data = fetch_data(stocks, start_date, end_date)\nreturns = stock_data.pct_change().dropna()\nmean_returns = returns.mean()\ncov_matrix = returns.cov()\n\n[                       0%                       ][                       0%                       ][**************        30%                       ]  3 of 10 completed[**************        30%                       ]  3 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************60%****                   ]  6 of 10 completed[**********************70%*********              ]  7 of 10 completed[**********************80%*************          ]  8 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#main-execution-1",
    "href": "hc.html#main-execution-1",
    "title": "Hill Climbing",
    "section": "Main Execution",
    "text": "Main Execution\n\nstart_time = time.time()\n\n# Run hill climbing\nbest_weights, best_sharpe, score_history = hill_climbing(mean_returns, cov_matrix)\n\nend_time = time.time()\nexecution_time = end_time - start_time  # Calculate elapsed time",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#output-1",
    "href": "hc.html#output-1",
    "title": "Hill Climbing",
    "section": "Output",
    "text": "Output\n\n# Plot the progress of the hill climbing algorithm (Sharpe ratio improvement)\nplt.figure(figsize=(10, 6))\nplt.plot(score_history, marker='o', linestyle='-', color='b', label='Sharpe Ratio Progress')\nplt.title(\"Hill Climbing Progress for Portfolio Optimization (Sharpe Ratio)\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Sharpe Ratio\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\n# Output the best results\nreturns, risk, sharpe_ratio = portfolio_performance(best_weights, mean_returns, cov_matrix)\nprint(f\"Best portfolio weights: {best_weights}\")\nprint(f\"Best Sharpe ratio: {sharpe_ratio}\")\nprint(f\"Best portfolio returns: {returns}, Best risk: {risk}\")\nprint(f\"Hill Climbing Finance Example Execution time: {execution_time:.6f} seconds\")\n\n\n\n\n\n\n\n\nBest portfolio weights: [0.20944997 0.         0.         0.11527445 0.         0.21091741\n 0.         0.30274302 0.16161515 0.        ]\nBest Sharpe ratio: 2.799430383445825\nBest portfolio returns: 0.6292559494805312, Best risk: 0.22477999567396947\nHill Climbing Finance Example Execution time: 0.077021 seconds\n\n\n\nThe plot visualizes how the Sharpe ratio evolves over the iterations. This helps understand the performance of the hill climbing algorithm.\nThe weights are heavily concentrated in a few stocks, particularly:\n\nstocks = [‚ÄòAAPL‚Äô, ‚ÄòGOOGL‚Äô, ‚ÄòMSFT‚Äô, ‚ÄòAMZN‚Äô, ‚ÄòTSLA‚Äô, ‚ÄòNFLX‚Äô, ‚ÄòNVDA‚Äô, ‚ÄòMETA‚Äô, ‚ÄòDIS‚Äô, ‚ÄòBA‚Äô]\n19.560444 % in stock 1: AAPL: Apple\n13.030724 % in stock 4: AMZN: Amazon\n19.665558 % in stock 6: NFLX: Netflix\n30.926036 % in stock 8: META: Facebook\n16.817237 % in stock 9: DIS: Walt Disney Co\n\nA Sharpe ratio of 2.800 is generally considered very good. A Sharpe ratio above 1 is usually considered acceptable, above 2 is very good, and above 3 is excellent, indicating that your portfolio offers a high return per unit of risk.\nOur portfolio is highly concentrated in just 5 out of the 10 assets. The algorithm has effectively zeroed out the remaining assets, which can either be a good or bad thing depending on the actual data.\n\nPros: Concentration might mean that the algorithm has found a combination of assets that offer the best Sharpe ratio, meaning it is maximizing returns while minimizing risk.\nCons: Concentrating in a few assets can increase the overall risk due to lack of diversification.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#items-to-consider",
    "href": "hc.html#items-to-consider",
    "title": "Hill Climbing",
    "section": "Items To Consider",
    "text": "Items To Consider\n\nA Sharpe ratio of 2.800 is very good, suggesting that the portfolio has a favorable balance of return vs.¬†risk. This ratio indicates that for each unit of risk, your portfolio earns more than two times the return.\nWhether this Sharpe ratio is truly impressive depends on the quality and volatility of the returns in the dataset. Financial markets rarely produce consistently high Sharpe ratios, so the high value might indicate the presence of some anomaly, or it could be due to a limited timeframe or low volatility in the assets during this period.\nGiven the high concentration in a few stocks, there‚Äôs a risk that the algorithm might be overfitting to the specific data in the training period. This means the portfolio might perform very well during the period used for the analysis but might not generalize well to future periods.\nThings to Try Next:\n\nBacktesting: Test this portfolio over a different time period or a longer time horizon to see if it maintains its high Sharpe ratio.\nRebalancing: Consider rebalancing the portfolio periodically (monthly or quarterly) to see if the high Sharpe ratio persists.\nDiversification: If you are concerned about high concentration risk, you might want to introduce constraints to the hill climbing algorithm to enforce a minimum weight for diversification or limit the maximum weight of any asset.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  }
]