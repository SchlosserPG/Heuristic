[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Heuristic Modelling",
    "section": "",
    "text": "Course Overview",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Heuristic Algorithms</span>"
    ]
  },
  {
    "objectID": "index.html#course-goals",
    "href": "index.html#course-goals",
    "title": "Heuristic Modelling",
    "section": "Course Goals",
    "text": "Course Goals\n\nDevelop a solid process for algorithm development.\nEnhance Python programming skills.\nUnderstand the structure of heuristic models, focusing on:\n\nHill climbing\nSimulated annealing\nGenetic algorithms",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Heuristic Algorithms</span>"
    ]
  },
  {
    "objectID": "index.html#required-book",
    "href": "index.html#required-book",
    "title": "Heuristic Modelling",
    "section": "Required Book",
    "text": "Required Book\nHandbook of Metaheuristic Algorithms\nAuthors: Chun-Wei Tsai & Ming-Chao Chiang\nPublisher: Academic Press\n\n\nAccess the book free through O‚ÄôReilly‚Äôs website with school credentials.\nPython code is available on the author‚Äôs GitHub.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Heuristic Algorithms</span>"
    ]
  },
  {
    "objectID": "algo.html",
    "href": "algo.html",
    "title": "Algorithm Design and Pseudocode",
    "section": "",
    "text": "Automate This by Christopher Steiner",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#algorithms-key-concepts-and-examples",
    "href": "algo.html#algorithms-key-concepts-and-examples",
    "title": "Algorithm Design and Pseudocode",
    "section": "Algorithms Key Concepts and Examples",
    "text": "Algorithms Key Concepts and Examples\n\nAlgorithms in Finance: High-Frequency Trading (HFT) revolutionized Wall Street by executing trades at lightning speeds, leading to both massive profits and new risks.\nAlgorithms in Healthcare: Algorithms that diagnose diseases faster and more accurately than doctors.\nMusic: Algorithms used by platforms like Pandora to predict and recommend songs.\nImpact on Jobs: Automation‚Äôs role in replacing jobs traditionally done by humans, particularly in industries like finance, journalism, and even art.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#future-implications-and-ethical-considerations",
    "href": "algo.html#future-implications-and-ethical-considerations",
    "title": "Algorithm Design and Pseudocode",
    "section": "Future Implications and Ethical Considerations",
    "text": "Future Implications and Ethical Considerations\n\nExpansion of Algorithms: The growing reach of algorithms in decision-making processes, from hiring practices to legal judgments.\nEthical Concerns: The potential for bias in algorithms and the importance of transparency in their design. The need for regulation and oversight as algorithms increasingly influence critical aspects of life.\nLooking Ahead: Steiner‚Äôs call for society to adapt to the new algorithm-driven world, balancing innovation with ethical responsibility.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#bias-in-algorithms-explored",
    "href": "algo.html#bias-in-algorithms-explored",
    "title": "Algorithm Design and Pseudocode",
    "section": "Bias in Algorithms Explored",
    "text": "Bias in Algorithms Explored\n\nFacial Recognition Technology algorithms have shown significant biases, particularly in accurately identifying people of different races and genders.\n\nStudies have found that some facial recognition systems have higher error rates when identifying individuals with darker skin tones and women. This can lead to discriminatory outcomes, such as misidentifying people of color at higher rates than white individuals.\nImpact: This bias can result in wrongful accusations or the exclusion of certain groups from services that rely on facial recognition technology.\n\nHiring algorithms are used by companies to screen job applicants, but they can unintentionally perpetuate biases present in the training data.\n\nA famous case involved an AI hiring tool developed by Amazon, which was found to be biased against female applicants. The algorithm was trained on resumes submitted over the previous decade, which were predominantly from male applicants, leading the AI to favor male candidates.\nImpact: This bias can reinforce gender inequalities in the workplace by systematically disadvantaging qualified female applicants.\n\nPredictive Policing algorithms analyze historical crime data to predict where future crimes are likely to occur, influencing law enforcement patrols.\n\nThese algorithms often reflect existing biases in policing practices, such as disproportionately targeting minority neighborhoods. Because the training data may contain biased policing patterns, the algorithm can perpetuate over-policing in certain communities.\nImpact: This can lead to a cycle of increased surveillance and criminalization of specific racial or ethnic groups, reinforcing systemic biases in the criminal justice system.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#algorithm-development-process",
    "href": "algo.html#algorithm-development-process",
    "title": "Algorithm Design and Pseudocode",
    "section": "Algorithm Development Process:",
    "text": "Algorithm Development Process:\n\nWhat should you do first?\n0: Think of a conceptual approach\n1: Write step-by-step outline\n\nIn words (maybe pseudo code)\nFocus on sound logic\n\n2: Plan programming implementation\n\nChoose appropriate data structures + Speed, memory, convenience\nConsider functions, which kind of loops\n\n3: Write the program and debug\n\nUse the outline as comment statements\nWrite your algorithms in chunks, write a line, then test it\n\n\n\n\n\nProcess",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#understanding-the-structure-of-algorithms",
    "href": "algo.html#understanding-the-structure-of-algorithms",
    "title": "Algorithm Design and Pseudocode",
    "section": "Understanding the Structure of Algorithms",
    "text": "Understanding the Structure of Algorithms\n\nIdentify the Purpose:\n\nStart by understanding what the algorithm is supposed to achieve.\nLook for a brief description or goal at the beginning.\n\nBreak Down the Steps:\n\nAlgorithms are typically presented as a sequence of steps.\nEach step corresponds to a specific action or decision.\nin words. Focus on sound logic.\n\nRecognize Input and Output:\n\nDetermine what inputs the algorithm requires.\nIdentify the expected output(s) after the algorithm is executed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#understanding-the-structure-of-algorithms-flow-control",
    "href": "algo.html#understanding-the-structure-of-algorithms-flow-control",
    "title": "Algorithm Design and Pseudocode",
    "section": "Understanding the Structure of Algorithms: Flow Control",
    "text": "Understanding the Structure of Algorithms: Flow Control\n\nDetermine how the algorithm progresses through its steps and notice the flow control structures like loops (for, while) and conditionals (if, else).\n\nWhich data types should we use?\n\nBegin to consider speed, memory usage, convenience tradeoffs\n\nFiner details of data organization\n\nWhat fields for dictionary labels?\nHow should lists be sorted, if at all?\n\nLoop types\n\nShould we use a for or while loop?\nHow to we increase the ease of coding and development?\n\nShould we use functions?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#deciphering-algorithmic-notation-pseudocode-vs.-code",
    "href": "algo.html#deciphering-algorithmic-notation-pseudocode-vs.-code",
    "title": "Algorithm Design and Pseudocode",
    "section": "Deciphering Algorithmic Notation: Pseudocode vs.¬†Code",
    "text": "Deciphering Algorithmic Notation: Pseudocode vs.¬†Code\n\nRecognize that many algorithms are written in pseudocode, a high-level description that isn‚Äôt tied to any specific programming language.\nSome feel that with Python, the pseudo-code step would not be necessary anymore\nTranslate pseudocode into actual code if needed.\n\nProgram in chunks\nTest each chunk before moving on\nUse functions where reasonable\n\nEasier to test\n\n\nCheck code where solutions are known\n\nSmall problems\nObvious solutions\n\nDebug Often\n\nBreakpoints\nCheck variables in console\nVariable explorer\nPrint statements",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#deciphering-algorithmic-notation-mathematical-symbols",
    "href": "algo.html#deciphering-algorithmic-notation-mathematical-symbols",
    "title": "Algorithm Design and Pseudocode",
    "section": "Deciphering Algorithmic Notation: Mathematical Symbols",
    "text": "Deciphering Algorithmic Notation: Mathematical Symbols\n\nMathematical Symbols:\n\nAlgorithms often include mathematical notation, such as sums (\\(\\sum\\)) or products (\\(\\prod\\)).\nUnderstand these as they relate to the algorithm‚Äôs operations.\n\nBig-O Notation:\n\nLook for references to Big-O notation, which indicates the algorithm‚Äôs efficiency in terms of time or space.\nUnderstand the implications for performance, especially with large inputs.\n\nCommenting\n\nCommenting improves code readability, reuse, and maintainability.\nCan transfer algorithm outline to a program as comments.\nThen, you have comments that provide an outline for coding.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#design-paradigms",
    "href": "algo.html#design-paradigms",
    "title": "Algorithm Design and Pseudocode",
    "section": "Design Paradigms",
    "text": "Design Paradigms\n\nAlgorithms can be based on:\n\nIntuitive ideas\nMath\n\nOptimization\n\nPossibly with calculus analysis (e.g., gradients)\nHeuristic model analysis\nDynamic programming",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#common-symbols-in-algorithms",
    "href": "algo.html#common-symbols-in-algorithms",
    "title": "Algorithm Design and Pseudocode",
    "section": "Common Symbols in Algorithms",
    "text": "Common Symbols in Algorithms\n\nMathematical Symbols:\n\n\\(\\forall\\): ‚ÄúFor all‚Äù, used in universal quantification.\n\\(\\exists\\): ‚ÄúThere exists‚Äù, used in existential quantification.\n\\(\\sum_{i=1}^n x_i\\): Summation from \\(i = 1\\) to \\(n\\).\n\\(\\prod_{i=1}^n x_i\\): Product from \\(i = 1\\) to \\(n\\).\n\nLogical Symbols:\n\n\\(\\land\\): Logical AND.\n\\(\\lor\\): Logical OR.\n\\(\\neg\\): Logical NOT.\n\\(\\implies\\): Logical implication.\n\nAlgorithm-Specific Notation:\n\n\\(O(n)\\): Big-O notation, representing algorithm complexity.\n\\(P \\leftarrow Q\\): Assign the value of \\(Q\\) to \\(P\\).\nfor \\(i = 1\\) to \\(n\\): A loop from \\(i = 1\\) to \\(n\\).\nif \\((condition)\\): A conditional statement.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#summation-and-loop-example",
    "href": "algo.html#summation-and-loop-example",
    "title": "Algorithm Design and Pseudocode",
    "section": "Summation and Loop Example",
    "text": "Summation and Loop Example\n\nSummation Formula: \\(S = \\sum_{i=1}^{n} i = 1 + 2 + 3 + \\dots + n\\)\n\nExplanation: This formula calculates the sum of the first \\(n\\) natural numbers. The symbol \\(\\sum\\) represents the summation, and \\(i\\) is the index that runs from 1 to \\(n\\).\n\nNested Loop Calculation: \\(T = \\sum_{i=1}^{n} \\sum_{j=1}^{m} (i \\times j)\\)\n\nExplanation: This formula represents a nested summation where \\(i\\) runs from 1 to \\(n\\) and \\(j\\) runs from 1 to \\(m\\). It calculates the sum of the product of \\(i\\) and \\(j\\) over these ranges.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#conditional-statements-and-recursion",
    "href": "algo.html#conditional-statements-and-recursion",
    "title": "Algorithm Design and Pseudocode",
    "section": "Conditional Statements and Recursion",
    "text": "Conditional Statements and Recursion\n\nConditional Formula: \\[\nf(x) =\n\\begin{cases}\n0 & \\text{if } x &lt; 0 \\\\\n1 & \\text{if } x \\geq 0\n\\end{cases}\n\\]\n\nExplanation: This piecewise function returns 0 if ( x ) is less than 0, and 1 if ( x ) is greater than or equal to 0. It‚Äôs an example of a conditional statement in algorithmic form.\n\nRecursive Formula: \\[\nF(n) =\n\\begin{cases}\n1 & \\text{if } n = 1 \\\\\nn \\times F(n-1) & \\text{if } n &gt; 1\n\\end{cases}\n\\]\n\nExplanation: This is a recursive definition of the factorial function. For ( n = 1 ), ( F(n) = 1 ). For ( n &gt; 1 ), ( F(n) ) is defined as ( n ) times the factorial of ( n-1 ).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#big-o-notation-and-algorithm-complexity",
    "href": "algo.html#big-o-notation-and-algorithm-complexity",
    "title": "Algorithm Design and Pseudocode",
    "section": "Big-O Notation and Algorithm Complexity",
    "text": "Big-O Notation and Algorithm Complexity\n\nBig-O Notation Example: \\(T(n) = O(n^2)\\)\n\nExplanation: This formula describes the time complexity of an algorithm, where\\(T(n)\\) represents the runtime as a function of input size \\(n\\). The notation \\(O(n^2)\\) indicates that the algorithm‚Äôs runtime grows quadratically with the size of the input.\n\nLogarithmic Complexity: \\(T(n) = O(\\log n)\\)\n\nExplanation: This formula represents an algorithm with logarithmic time complexity. The runtime increases logarithmically as the input size \\(n\\) increases, which is common in algorithms like binary search.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#basic-tsp-formulation",
    "href": "algo.html#basic-tsp-formulation",
    "title": "Algorithm Design and Pseudocode",
    "section": "Basic TSP Formulation",
    "text": "Basic TSP Formulation\n\nObjective Function: \\[\n\\text{Minimize} \\quad Z = \\sum_{i=1}^{n} \\sum_{j=1, j \\neq i}^{n} c_{ij} x_{ij}\n\\]\n\nExplanation: This formula represents the objective function of the TSP, where \\(c_{ij}\\) is the cost (or distance) of traveling from city \\(i\\) to city \\(j\\), and \\(x_{ij}\\) is a binary variable that equals 1 if the path from \\(i\\) to \\(j\\) is included in the solution and 0 otherwise. The goal is to minimize the total travel cost.\n\nConstraints:\n\\(\\sum_{j=1, j \\neq i}^{n} x_{ij} = 1 \\quad \\forall i\\)\n\\(\\sum_{i=1, i \\neq j}^{n} x_{ij} = 1 \\quad \\forall j\\)\n\\(x_{ij} \\in \\{0, 1\\}\\)\n\nExplanation: The first constraint ensures that each city \\(i\\) is exited exactly once, and the second constraint ensures that each city \\(j\\) is entered exactly once. The binary constraint on \\(x_{ij}\\) ensures that the solution only includes valid paths.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#tsp-approximation-algorithm",
    "href": "algo.html#tsp-approximation-algorithm",
    "title": "Algorithm Design and Pseudocode",
    "section": "TSP Approximation Algorithm",
    "text": "TSP Approximation Algorithm\n\nApproximation Algorithm Cost: \\(Z \\leq 2 \\times \\text{OPT}\\)\n\nExplanation: This formula represents the performance guarantee of a 2-approximation algorithm for the TSP, where \\(Z\\) is the cost of the approximate solution and \\(\\text{OPT}\\) is the cost of the optimal solution. It guarantees that the approximate solution will be at most twice as costly as the optimal solution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#heuristic-nearest-neighbor-example",
    "href": "algo.html#heuristic-nearest-neighbor-example",
    "title": "Algorithm Design and Pseudocode",
    "section": "Heuristic Nearest Neighbor Example:",
    "text": "Heuristic Nearest Neighbor Example:\n\\(Z = \\sum_{i=1}^{n-1} c_{i, \\text{NN}(i)} + c_{n, \\text{NN}(1)}\\)\n\nExplanation: This is the cost calculation for the nearest neighbor heuristic, where \\(\\text{NN}(i)\\) denotes the nearest neighbor of city \\(i\\). The tour starts at a city, repeatedly visits the nearest unvisited city, and returns to the starting city.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#activity",
    "href": "algo.html#activity",
    "title": "Algorithm Design and Pseudocode",
    "section": "Activity:",
    "text": "Activity:\n\nState what the following algorithms do:\n\n\\[Z = \\sum_{i=1}^n \\left( x_i \\times y_i \\right)\\] \\[ \\bar{X} = \\frac{\\sum_{i=1}^{n} w_i x_i}{\\sum_{i=1}^{n} w_i}\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#tips-for-writing-good-pseudocode",
    "href": "algo.html#tips-for-writing-good-pseudocode",
    "title": "Algorithm Design and Pseudocode",
    "section": "Tips for Writing Good Pseudocode",
    "text": "Tips for Writing Good Pseudocode\n\nMaintain consistent terms throughout: Use the same terminology for variables, functions, and processes. If you define a variable as total, always refer to it as total, not sum later on.\nBe clear on naming: Use descriptive names for variables, functions, and operations to make your pseudocode intuitive.\nPseudocode should be easy to understand after a single read-through. If it feels too complex, break it down further.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#examples",
    "href": "algo.html#examples",
    "title": "Algorithm Design and Pseudocode",
    "section": "Examples",
    "text": "Examples\n\nThis pseudocode finds the smallest number in a given list by iterating through all elements and updating the min_number whenever a smaller number is found.\n\n# Pseudocode for finding the minimum number in a list\nSET min_number = Infinity\nFOR each number IN list:\n    IF number &lt; min_number:\n        SET min_number = number\nRETURN min_number\n\n\nThis pseudocode below calculates the sum of all numbers in the list that are greater than a specified threshold by iterating through the list and adding qualifying numbers to the sum.\nedge cases: an empty list, all even numbers, or no numbers greater than the threshold.\n\n# Pseudocode for summing numbers greater than a given threshold\nSET sum = 0\nFOR each number IN list:\n    IF number &gt; threshold:\n        SET sum = sum + number\nRETURN sum",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#activity-read-this-pseudocode",
    "href": "algo.html#activity-read-this-pseudocode",
    "title": "Algorithm Design and Pseudocode",
    "section": "Activity: Read this Pseudocode",
    "text": "Activity: Read this Pseudocode\nSET min_even = Infinity \nFOR each number IN list: \n    IF number is even AND number &lt; min_even: \n        SET min_even = number \nRETURN min_even",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#pseudocode-for-basic-tsp-formation",
    "href": "algo.html#pseudocode-for-basic-tsp-formation",
    "title": "Algorithm Design and Pseudocode",
    "section": "Pseudocode for Basic TSP Formation",
    "text": "Pseudocode for Basic TSP Formation\n\nGeneric Inputs: The pseudocode is now generic and works for any TSP model where you have a set of cities and their corresponding distances.\nNearest Neighbor Heuristic: The algorithm works by greedily choosing the nearest unvisited city until all cities are visited, then returning to the start city.\nOutput: It outputs the route and the total distance traveled.\n\n#inputs\nSET cities = &lt;list of cities&gt;\nSET distances = &lt;distance matrix or dictionary&gt;\nSET start_city = &lt;initial city&gt;\n\n\n# FUNCTION to find the nearest unvisited city\nFUNCTION find_nearest_neighbor(current_city, unvisited, distances):\n    SET nearest_city = None\n    SET min_distance = infinity\n    \n    FOR each city IN unvisited:\n        IF distances[current_city][city] &lt; min_distance:\n            SET min_distance = distances[current_city][city]\n            SET nearest_city = city\n    \n    RETURN nearest_city, min_distance\n\n# FUNCTION to solve TSP using Nearest Neighbor algorithm\nFUNCTION nearest_neighbor_tsp(start_city, cities, distances):\n# Initialize list of unvisited cities\n    SET unvisited = list of all cities EXCEPT start_city",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "numpy.html",
    "href": "numpy.html",
    "title": "Python for Heuristics & NumPy",
    "section": "",
    "text": "Coding Know-How\nWhat problems does this code have?\nx1 = [0, 1, 2, 3, 4, 5]\nx2 = [6, 7, 8, 9, 10]\n\nprint(sum(x1))\n\n15\n\nprint(sum(x2))\n\n40",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#basic-array-creation",
    "href": "numpy.html#basic-array-creation",
    "title": "Python for Heuristics & NumPy",
    "section": "Basic Array Creation",
    "text": "Basic Array Creation\n\nThe np.array function in NumPy is used to create an array (a grid of values) from data provided as lists, tuples, or other array-like structures. The resulting NumPy array is a powerful and flexible structure for mathematical operations, as it supports multiple dimensions, broadcasting, and various data types.\n\n\n# Creating a simple numpy array from a Python list\narray = np.array([1, 2, 3, 4])\nprint(\"Array:\", array)\n\nArray: [1 2 3 4]\n\n\n\nnp.arange() is a NumPy function that generates an array with evenly spaced values within a given range.\n\n\narray = np.arange(1, 5)  # Generates [1, 2, 3, 4]\nprint(\"Array:\", array)\n\nArray: [1 2 3 4]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#element-wise-operations",
    "href": "numpy.html#element-wise-operations",
    "title": "Python for Heuristics & NumPy",
    "section": "Element-Wise Operations",
    "text": "Element-Wise Operations\n\nElement-wise operators are mathematical or logical operations applied independently to corresponding elements in arrays or matrices of the same shape.\nEach element in one array is combined with the corresponding element in the other array using the operator.\nIn the context of arrays (such as in NumPy), common element-wise operators include basic arithmetic operators:\n\nElement-wise addition +\nElement-wise subtraction -\nElement-wise multiplication *\nElement-wise division /\nElement-wise exponentiation **\n\n\n\n# Performing element-wise addition\narray = np.array([1, 2, 3, 4])\nadded_array = array + 5\nprint(\"Added Array:\", added_array)\n\nAdded Array: [6 7 8 9]\n\n\n\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\nc = a + b\nprint(c)\n\n[5 7 9]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#taking-an-exponent-np.exp",
    "href": "numpy.html#taking-an-exponent-np.exp",
    "title": "Python for Heuristics & NumPy",
    "section": "Taking an Exponent: np.exp",
    "text": "Taking an Exponent: np.exp\n\nnp.exp is a function in the NumPy library that calculates the exponential of all elements in an input array. Specifically, it computes the base-e exponential function, which is ùëí^ùë•, where ùëí is Euler‚Äôs number (approximately 2.71828), and ùë• is the input array or scalar.\n\n\n# Applying np.exp to the array\narray = np.array([1, 2, 3, 4])\nexp_array = np.exp(array)\nprint(\"Exponential Array:\", exp_array)\n\nExponential Array: [ 2.71828183  7.3890561  20.08553692 54.59815003]\n\n\n\nnp.exp from Simulated Annealing example\nThis function is part of a Simulated Annealing algorithm, specifically handling the temperature decay mechanism to decide whether to accept a new solution, even if it‚Äôs worse than the current one. Here‚Äôs a breakdown of the function based on the np.exp command and the logic:\n\ntmp_obj_val: The objective value of a new (temporary) solution.\nobj_val: The objective value of the current solution. temperature: The current temperature in the simulated annealing process, which controls how likely the algorithm is to accept worse solutions.\nA random number r between 0 and 1 is generated. This represents a threshold for whether the new solution will be accepted using random.rand()\nThe probability p of accepting the new solution is computed using the exponential function.\nIf the random value r is less than the calculated probability p, the function returns True, meaning the new solution is accepted (even if it‚Äôs worse). If r is greater than p, the new solution is rejected, and the current solution is maintained.\n\nThe function decides whether to accept a new solution in simulated annealing, balancing exploration and exploitation based on the temperature and objective values of the solutions. The np.exp() function ensures that worse solutions have a chance to be accepted, particularly early in the process, fostering a broader search space.\n\n\n# Simulated annealing temperature decay\ndef determine(self, tmp_obj_val, obj_val, temperature):\n     r = np.random.rand()\n     p = np.exp((tmp_obj_val - obj_val) / temperature)\n     return r &lt; p",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#taking-a-square-root-np.sqrt",
    "href": "numpy.html#taking-a-square-root-np.sqrt",
    "title": "Python for Heuristics & NumPy",
    "section": "Taking a square root: np.sqrt()",
    "text": "Taking a square root: np.sqrt()\n\nnp.sqrt is a function in NumPy that returns the non-negative square root of an element-wise input array. It operates on each element of the array and computes the square root.\n\n\n# Applying np.sqrt to the array\nsqrt_array = np.sqrt(array)\nprint(\"Square Root Array:\", sqrt_array)\n\nSquare Root Array: [1.         1.41421356 1.73205081 2.        ]\n\n\n\nThe Ackley function is commonly used as a benchmark problem in optimization, and is known for its many local minima. The Ackley function uses the np.sqrt within its formula.\n\n\ndef ackley(s):\n     a, b, c = 20, 0.2, 2 * np.pi\n     n = len(s)\n     sum_sq_term = np.sum(s**2)\n     cos_term = np.sum(np.cos(c * s))\n     term1 = -a * np.exp(-b * np.sqrt(sum_sq_term / n))\n     term2 = -np.exp(cos_term / n)\n     return term1 + term2 + a + np.e",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#random-number-generation",
    "href": "numpy.html#random-number-generation",
    "title": "Python for Heuristics & NumPy",
    "section": "Random Number Generation",
    "text": "Random Number Generation\n\nThe np.random.rand function in NumPy generates random floating-point numbers from a uniform distribution between 0 (inclusive) and 1 (exclusive), meaning the generated numbers will always include 0 (inclusive) but never reach 1 (exclusive).\nRandom numbers are key to both genetic algorithms (mutation, crossover) and simulated annealing (random perturbations).\nThe example below selects uses np.random.rand() to generate 5 uniform random numbers between 0 and 1 [0,1). It is saved in a variable rand_nums and the values in the variable are printed.\n\n\nrand_nums = np.random.rand(5)\nprint(rand_nums)\n\n[0.66404819 0.85245772 0.65991604 0.92063824 0.28530187]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#standard-normal-distribution",
    "href": "numpy.html#standard-normal-distribution",
    "title": "Python for Heuristics & NumPy",
    "section": "Standard Normal Distribution",
    "text": "Standard Normal Distribution\n\nThe np.random.standard_normal function in NumPy generates random floating-point numbers from a standard normal (Gaussian) distribution, with a mean of 0 and a standard deviation of 1.\nGenerating 5 random numbers from a standard normal distribution (mean=0, std=1).\n\n\n# Generating random values from the standard normal distribution\nrandom_values = np.random.standard_normal(5)\nprint(\"Random Standard Normal Values:\", random_values)\n\nRandom Standard Normal Values: [-1.40249368 -1.95529569  0.72202892  0.28189136 -0.90923237]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#np.random.uniform",
    "href": "numpy.html#np.random.uniform",
    "title": "Python for Heuristics & NumPy",
    "section": "np.random.uniform",
    "text": "np.random.uniform\nThe np.random.uniform function in NumPy is used to generate random floating-point numbers drawn from a uniform distribution over a specified range. For example, in hill climbing, the algorithm often starts with a random solution that can be simulated with np.random.uniform.\n\n# Generate a random starting point for the hill climbing algorithm\nrandom_start = np.random.uniform(low=-10, high=10, size=5)\nprint(f\"Random start: {random_start}\")\n\nRandom start: [-0.74427727  8.76875529  0.84791715 -4.94620585  1.98499898]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#np.random.randint",
    "href": "numpy.html#np.random.randint",
    "title": "Python for Heuristics & NumPy",
    "section": "np.random.randint",
    "text": "np.random.randint\n\nThe np.random.randint function in NumPy is used to generate random integers within a specified range.\nnp.random.randint(low, high=None, size=None, dtype=int)\n\nlow: The lower boundary of the random integers (inclusive).\nhigh: The upper boundary of the random integers (exclusive). If not provided, random integers are generated between 0 and low.\nsize: The shape of the output array (optional). If not provided, a single integer is returned.\ndtype: The desired data type of the output array, by default int.\n\n\n\n# Generate 5 random integers between 10 and 20\nrandom_integers = np.random.randint(10, 20, size=5)\nprint(random_integers)\n\n[14 13 11 17 17]\n\n\n\nWe can use np.random.randint to generate a 2D array instead of a 1D array by specifying the size parameter as a tuple that indicates the shape of the array.\n\n\nrandom_2d_array = np.random.randint(10, 20, size=(3, 5)) \nprint(random_2d_array)\n\n[[17 15 12 19 10]\n [19 19 18 14 13]\n [13 15 13 16 17]]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#np.random.randint-from-simulated-annealing",
    "href": "numpy.html#np.random.randint-from-simulated-annealing",
    "title": "Python for Heuristics & NumPy",
    "section": "np.random.randint from Simulated Annealing",
    "text": "np.random.randint from Simulated Annealing\n\nThis function, transit(), is used to modify a solution sol as part of a heuristic search process, likely for algorithms like genetic algorithms, hill climbing, or simulated annealing. The goal is to explore the solution space by introducing a small, random change (or ‚Äútransition‚Äù) to the current solution.\n\nThe function takes a single argument, sol, which is likely a binary array or list (a list of 0s and 1s).\nt = sol.copy(): A copy of the solution sol is made, named t. This is important because we don‚Äôt want to modify the original solution directly; instead, we work on the copy t.\ni = np.random.randint(len(sol)): The randint function from NumPy is used to randomly select an index i between 0 and the length of sol - 1. This selects a random position in the solution array.\nt[i] ^= 1: This is a bitwise XOR operation. In the context of a binary solution (a list of 0s and 1s), it flips the value at index i:If t[i] is 0, it becomes 1.If t[i] is 1, it becomes 0. This operation introduces a small, random change to the solution by flipping one bit.\nreturn t: After flipping one bit, the modified solution t is returned.\n\n\n# Transition function (T)\ndef transit(sol):\n    new_sol = sol.copy()\n    index = np.random.randint(len(sol))\n    new_sol[index] = 1 - new_sol[index]  # Flip a random bit\n    return new_sol\n\nTo compare:\n\nnp.random.standard_normal = normal distribution with mean and sd\nnp.random.rand = uniform distribution between [0,1)\nnp.random.uniform = uniform over a specified range\nnp.random.randint = random integers within a specified range",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#sorting-the-data-np.argsort",
    "href": "numpy.html#sorting-the-data-np.argsort",
    "title": "Python for Heuristics & NumPy",
    "section": "Sorting the data: np.argsort",
    "text": "Sorting the data: np.argsort\n\nThe np.argsort function in NumPy returns the indices that would sort an array along a specified axis. This allows you to reorder elements based on their sorted order without actually changing the original array.\nIn various evolutionary algorithms (such as genetic algorithms or simulated annealing), selecting the most ‚Äúfit‚Äù or optimal solutions from a population is crucial for convergence toward the global optimum.\nBy sorting individuals based on fitness, the algorithm can efficiently identify the most promising candidates for further exploration (e.g., crossover, mutation) or intensify the search around high-quality solutions.\nThe use of np.argsort allows for a fast, reliable way to rank individuals, ensuring that the evolutionary process focuses on refining the best candidates and discarding those with lower potential.\n\n\n# Dummy population and fitness values\npopulation = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n\n# Assign dummy fitness values\nfitness = np.array([10, 30, 20, 40, 50]) \n\n# Sort population based on fitness\nindices = np.argsort(fitness)\nprint(indices) \n\n[0 2 1 3 4]\n\nsorted_population = population[indices]\n\n# Select top 3 individuals\ntop_individuals = sorted_population[:3] \nprint(top_individuals)\n\n[[1 2]\n [5 6]\n [3 4]]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#selecting-the-max-np.argmax",
    "href": "numpy.html#selecting-the-max-np.argmax",
    "title": "Python for Heuristics & NumPy",
    "section": "Selecting the Max: np.argmax",
    "text": "Selecting the Max: np.argmax\n\nThe np.argmax function in NumPy returns the index of the maximum value in an array along a specified axis.\nFinding the Index of the Maximum Element in a 1D Array: The np.argmax function returns the index of the first occurrence of the maximum value in the array. In this case, the maximum value is 7, and it occurs at index 2.\n\n\narr = np.array([1, 3, 7, 2, 5])\nindex = np.argmax(arr)\nprint(\"Array:\", arr)\n\nArray: [1 3 7 2 5]\n\nprint(\"Index of max element:\", index)\n\nIndex of max element: 2\n\nprint(\"Max element:\", arr[index])\n\nMax element: 7\n\n\nArray: [1 3 7 2 5]\n\nUsing np.argmax with a 2D Array (Row-wise & Column-wise): np.argmax can work on multi-dimensional arrays. By specifying axis=0 or axis=1, you can find the maximum values column-wise or row-wise, respectively. For axis=0, you get the indices of the maximum elements for each column, and for axis=1, you get them for each row.\n\n\narr_2d = np.array([[1, 2, 3], [4, 5, 1], [0, 6, 2]])\n\n# Find the index of the max element in the flattened array\nmax_index_flat = np.argmax(arr_2d)\nprint(\"Flattened array index:\", max_index_flat)\n\nFlattened array index: 7\n\n\n\nNumber 6 is in index 7, starting at index 0 and counting up across each row.\n[[1 2 3]\n[4 5 1]\n[0 6 2]]\n\n\n# Find the index of the max element along each column (axis=0)\nmax_index_col = np.argmax(arr_2d, axis=0)\nprint(\"Max element index for each column:\", max_index_col)\n\nMax element index for each column: [1 2 0]\n\n\n\n4 is in index 1, 6 is in index 2, and 3 is in index 0, counting across each column starting at index 0. [[1 2 3]\n[4 5 1]\n[0 6 2]]\n\n\n# Find the index of the max element along each row (axis=1)\nmax_index_row = np.argmax(arr_2d, axis=1)\nprint(\"Max element index for each row:\", max_index_row)\n\nMax element index for each row: [2 1 1]\n\n\n\n3 is in index 2 in the row, 5 is in index 1, and 6 is in index 1, counting across each row starting at index 0. [[1 2 3]\n[4 5 1]\n[0 6 2]]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#an-example-modelling-stock-prices",
    "href": "numpy.html#an-example-modelling-stock-prices",
    "title": "Python for Heuristics & NumPy",
    "section": "An Example Modelling Stock Prices",
    "text": "An Example Modelling Stock Prices\n\nThe model simulates a Geometric Brownian Motion (GBM), a widely used stochastic process in financial mathematics to model the evolution of stock prices over time. This process assumes that stock prices follow a log-normal distribution, incorporating key parameters such as the initial stock price (S0), risk-free rate (r), time horizon (T), and volatility (sigma).\nThe model calculates the potential future stock prices (ST) using a mathematical formula that combines deterministic and random components, reflecting the inherent uncertainty and growth trends in financial markets. By generating a large number of simulated outcomes, the model enables analyses such as estimating expected returns, assessing risk, and valuing options, providing valuable insights for decision-making in finance.\n\n\\(S_T = S_0 \\exp\\left( (r - 0.5 \\sigma^2) T + \\sigma Z \\sqrt{T} \\right)\\)\n\nThe terminal stock price \\(S_T\\) is modeled using the Geometric Brownian Motion (GBM), a common approach to model stock prices.\n\n\\(S_0\\): The initial stock price.\n\\(r\\): The risk-free interest rate.\n\\(T\\): Time to maturity (in years).\n\\(\\sigma\\): The volatility of the stock.\n\\(S_T\\): The terminal stock price at time \\(T\\).\n\\(Z\\): A random variable drawn from a standard normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#comparing-model-clock-time-withwithout-numpy",
    "href": "numpy.html#comparing-model-clock-time-withwithout-numpy",
    "title": "Python for Heuristics & NumPy",
    "section": "Comparing Model Clock Time With/Without NumPy",
    "text": "Comparing Model Clock Time With/Without NumPy\n\nThe primary difference in wall time between the two approaches stems from the computational efficiency of NumPy compared to Python‚Äôs built-in modules and loops.\n\n\nWithout Numpy\n\na Python loop iterates 1,000,000 times, and the math.exp, random.gauss, and math.sqrt functions are called repeatedly within the loop to calculate values. This results in higher wall time due to the overhead of Python‚Äôs interpreted loop and the sequential calls to these functions.\n\n\nimport random\nfrom math import exp, sqrt\nimport time \n\n# Initial stock price\nS0 = 100 \n\n# Risk-free rate\nr = 0.05 \n\n# Time horizon (1 year)\nT = 1.0 \n\n# Volatility\nsigma = 0.2 \n\nvalues = []  \n\n# Start tracking wall time\nstart_time = time.time()\n\nfor _ in range(1000000):  \n     ST = S0 * exp((r - 0.5 * sigma ** 2) * T +\n        sigma * random.gauss(0, 1) * sqrt(T))  \n     values.append(ST)  \n\n# End tracking wall time\nend_time = time.time()\n\n\n# Calculate time difference\nwall_time = end_time - start_time\n\n# Print timing information\nprint(f\"Wall time: {wall_time:.2f} s\")\n\nWall time: 0.69 s\n\n\n\n\nWhy Underscore Conventions Matter\n\nIn the example above, the loop for _ in range(): uses the underscore as a throwaway variable, signaling that the loop variable itself is not needed‚Äîonly the repetition of the loop matters. This is a common Python convention that improves readability by making intent explicit. Similarly, you will see many programs and function definitions where unused parameters are replaced with an underscore (for example, def handler(_, value):), indicating that a particular argument is intentionally ignored. These small stylistic choices communicate purpose and clarity to anyone reading the code.See the table below for some direct uses of the underscore.\n\n\n\n\nCommon Uses of the Underscore in Python\n\n\n\n\n\n\n\nPattern\nExample\nCommon.Use.in.Python\n\n\n\n\nSingle underscore: ‚Äúignore.‚Äù\nfor _ in range(10):\nUsed as a throwaway variable when the value isn‚Äôt needed. In the REPL, _ also stores the last result.\n\n\nSingle underscore: ‚Äúinternal.‚Äù\n_helper_function()\nMarks a variable or function as internal use only within a module or class (not imported with *).\n\n\nDouble underscore: ‚Äúprivate inside a class.‚Äù\nself.__secret\nUsed for name mangling inside classes to make attributes harder to access from outside.\n\n\nDouble before & after: ‚Äúspecial built-in behavior.‚Äù\nif name == ‚Äòmain‚Äô:\nCommon structure to run code only when the file is executed directly, not when it‚Äôs imported as a module.\n\n\n\n\n\n\n\nWith Numpy\n\nIn contrast, the NumPy-based implementation below leverages vectorized operations. NumPy handles the entire computation in a single step using efficient, low-level C routines optimized for performance. For example:\n\nThe entire random sample generation is done in one call (np.random.standard_normal(1000000)).\n\n\n\nimport numpy as np\nimport time \n\n# Initial stock price\nS0 = 100 \n\n# Risk-free rate\nr = 0.05 \n\n# Time horizon (1 year)\nT = 1.0 \n\n# Volatility\nsigma = 0.2 \n\n# Start tracking wall time\nstart_time = time.time()\n\nST = S0 * np.exp((r - 0.5 * sigma ** 2) * T +\n    sigma * np.random.standard_normal(1000000) * np.sqrt(T))\n\n# End tracking wall time\nend_time = time.time()\n\n# Calculate time difference\nwall_time = end_time - start_time\n\n# Print timing information\nprint(f\"Wall time: {wall_time:.2f} s\")\n\nWall time: 0.02 s\n\n\n\nMathematical operations like exp and sqrt are applied to entire arrays at once. These optimizations significantly reduce the wall time, as the process avoids Python-level overhead and directly utilizes optimized native code. As a result, the NumPy implementation is typically faster, making it better suited for tasks requiring a high volume of computations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "algo.html#pseudocode-for-tsp-nearest-neighbor-model",
    "href": "algo.html#pseudocode-for-tsp-nearest-neighbor-model",
    "title": "Algorithm Design and Pseudocode",
    "section": "Pseudocode for TSP Nearest Neighbor Model",
    "text": "Pseudocode for TSP Nearest Neighbor Model\n\nNearest Neighbor Heuristic: The algorithm works by greedily choosing the nearest unvisited city until all cities are visited, then returning to the start city.\nOutput: It outputs the route and the total distance traveled.\n\n#inputs\nSET cities = &lt;list of cities&gt;\nSET distances = &lt;distance matrix or dictionary&gt;\nSET start_city = &lt;initial city&gt;\n\n\n# FUNCTION to find the nearest unvisited city\nFUNCTION find_nearest_neighbor(current_city, unvisited, distances):\n    SET nearest_city = None\n    SET min_distance = infinity\n    \n    FOR each city IN unvisited:\n        IF distances[current_city][city] &lt; min_distance:\n            SET min_distance = distances[current_city][city]\n            SET nearest_city = city\n    \n    RETURN nearest_city, min_distance\n\n# FUNCTION to solve TSP using Nearest Neighbor algorithm\nFUNCTION nearest_neighbor_tsp(start_city, cities, distances):\n# Initialize list of unvisited cities\n    SET unvisited = list of all cities EXCEPT start_city",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "greedy.html",
    "href": "greedy.html",
    "title": "Introduction to Greedy Algorithms",
    "section": "",
    "text": "Optimization Problem",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "greedy.html#definition-1",
    "href": "greedy.html#definition-1",
    "title": "Introduction to Greedy Algorithms",
    "section": "Definition 1",
    "text": "Definition 1\n\nAn optimization problem \\(P\\) is to find the optimal value, possibly subject to some constraints, out of all possible solutions.\nContains the objective function, constraint(s), and solution.\n\\(opt_{s \\in A} f(s)\\) subject to \\(\\forall c_i(s) \\odot b_i, i=i, 2, ...,m\\) where\nopt is either min (for minimization) or max (for maximization),\n\ns is a candidate solution\nA and B are the domain and codomain of the problem Image, namely, A is the set of all possible solutions and B is the set of all possible outcomes of the objective function,\n\\(c_i(s) \\odot b_i\\) is the constraint, and\n\\(f(s): A-&gt;B\\) is the objective function\n\\(\\odot\\) is \\(&gt;, &lt;, =, &lt;=, &gt;=\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "greedy.html#definition-2",
    "href": "greedy.html#definition-2",
    "title": "Introduction to Greedy Algorithms",
    "section": "Definition 2",
    "text": "Definition 2\n\nThe optimal solution is a solution, out of all feasible candidate solutions of the optimization problem \\(P\\), that gives the optimal value. \\(f(s^*) = \\operatorname{opt} \\{f(s)\\}, \\, \\forall \\, c_i(s) \\, \\odot \\, b_i, \\, i = 1, 2, \\dots, m.\\)\n\nTo compare \\(f(s)\\) with \\(f(s^*)\\), \\(f(s)\\) represents the objective function evaluated at a solution \\(s\\). \\(f(s^*)\\) refers to the optimal solution, where \\(s^*\\) is the best candidate that optimizes \\(f(s)\\) (either maximizing or minimizing it).",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "greedy.html#definition-3",
    "href": "greedy.html#definition-3",
    "title": "Introduction to Greedy Algorithms",
    "section": "Definition 3",
    "text": "Definition 3\n\nIf the optimal solution \\(s*\\) for the problem ùëÉ exists, then the optimal value \\(f^*\\) is defined as \\(min_{s \\in A} f(s)\\), subject to \\(\\forall c_i(s) \\odot b_i\\),\nWhile the maximization problem of maximizing \\(f(s)\\) subject to some constraints can be defined as \\(max_{s \\in A} f(s)\\), subject to \\(\\forall c_i(s) \\odot b_i\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "greedy.html#simple-example-of-greedy-algorithm-in-action",
    "href": "greedy.html#simple-example-of-greedy-algorithm-in-action",
    "title": "Introduction to Greedy Algorithms",
    "section": "Simple Example of Greedy Algorithm in Action",
    "text": "Simple Example of Greedy Algorithm in Action\n\nThe Coin Change Problem\nGiven a set of coin denominations and a target amount, find the minimum number of coins that add up to the target amount.\nGreedy Strategy: At each step, pick the largest denomination that doesn‚Äôt exceed the remaining amount.\n\n\\[\\operatorname{coins\\_used}(A) = \\sum_{i=1}^{n} \\left\\lfloor \\frac{A}{c_i} \\right\\rfloor \\times c_i \\quad \\operatorname{where} \\quad A = A - \\left\\lfloor \\frac{A}{c_i} \\right\\rfloor \\times c_i\\]\nWhere \\(\\left\\lfloor \\frac{A}{c_i} \\right\\rfloor\\) is the number of coins of denomination \\(c_i\\) is used.\n\\(A\\) is reduced by the value \\(\\left\\lfloor \\frac{A}{c_i} \\right\\rfloor \\times c_i\\)\nAfter using as many \\(c_i\\) denomination coins as possible. The process continues until \\(A=0\\), at which point the minimum number of coins required to make the total amount is found.\n\ndef greedy_coin_change(coins, amount):\n    coins.sort(reverse=True)\n    result = []\n    for coin in coins:\n        while amount &gt;= coin:\n            amount -= coin\n            result.append(coin)\n    \n    # Print the coins used\n    print(f\"Coins used: {result}\")\n    \n    # Return the number of coins used\n    return len(result)\n\n# Get user input (Put in 70 to show answer, but can request information from user)\namount = 70\n# amount = int(input(\"Enter the amount: \"))\n\n# Coin denominations\ncoins = [1, 5, 10, 25]\n\n# Calculate the solution\nnum_coins = greedy_coin_change(coins, amount)\n\nprint(f\"Minimum number of coins needed: {num_coins}\")\n\nCoins used: [25, 25, 10, 10]\nMinimum number of coins needed: 4",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "greedy.html#discrete-vs-continuous",
    "href": "greedy.html#discrete-vs-continuous",
    "title": "Introduction to Greedy Algorithms",
    "section": "Discrete vs Continuous",
    "text": "Discrete vs Continuous\n\n\n\nDiscrete vs Continuous\n\n\n\nDiscrete\n\nDomain: The set of all possible input values for a function.\nCodomain: The set of all potential output values that the function can map to.\nAn objective function is a function that is being optimized (maximized or minimized) in a given problem. It takes an input from the domain and produces an output in the codomain.\nGiven two sets \\(A\\) and \\(B\\), and an objective function \\(f\\), we can understand how the function maps elements from the domain \\(A\\) to the codomain \\(B\\).\n\n\n\nContinuous\n\nShows the relationship between the angle Œ∏ and the value of sin‚Å°(Œ∏) at specific points. This relationship arises from the trigonometric sine function, which describes a wave-like pattern that oscillates between -1 and 1.\nŒ∏ represents the angle, typically in radians, and the values given (0, 0.25\\(\\pi\\), 0.50\\(\\pi\\), etc.) are specific points along the unit circle.\nsin(Œ∏) represents the sine of the angle Œ∏, which is the y-coordinate of the corresponding point on the unit circle.\nThe values provided in the table correspond to these properties of the sine function. The function gradually increases from 0 to 1, then decreases back to 0, then continues to -1, and finally returns to 0, completing one full cycle.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "greedy.html#traveling-salesman-algorithm",
    "href": "greedy.html#traveling-salesman-algorithm",
    "title": "Introduction to Greedy Algorithms",
    "section": "Traveling Salesman Algorithm",
    "text": "Traveling Salesman Algorithm\nOptimal Path: \\[\\min_{s \\in \\S_{\\pi}} f(s) = \\left[ \\sum_{i=1}^{n-1} d\\left(c_{\\pi(i)}, c_{\\pi(i+1)}\\right) \\right] + d\\left(c_{\\pi(n)}, c_{\\pi(1)}\\right)\\]\nWhere min_(ùë†‚ààùíÆ_ùúã )\\(c_{\\pi} = \\{ c_{\\pi(1)}, c_{\\pi(2)}, \\dots, c_{\\pi(n)} \\}\\), that is, all permutations of the \\(n\\) cities.\nvs.¬†\nNearest Neighbor (Greedy): \\(Z = \\sum_{i=1}^{n-1} c_{i, \\text{NN}(i)} + c_{n, \\text{NN}(1)}\\)\n\n\n\nGreedy Map\n\n\n\nGreedy TSP Solution using Nearest Neighbor\n\nStart at Richmond. Find the nearest city. From Richmond, the nearest city is Petersburg (25 miles). Move to Petersburg.\nFrom Petersburg, find the nearest unvisited city, which is Newport News (65 miles). Move to Newport News .\nFrom Newport News, the nearest unvisited city is Norfolk (30 miles). Move to Norfolk.\nFrom Norfolk, the nearest unvisited city is Chesapeake (10 miles).Move to Chesapeake.\nFrom Chesapeake, the only unvisited city left is Virginia Beach (15 miles). Move to Virginia Beach.\nFinally, return to Richmond from Virginia Beach (100 miles).\nRichmond -&gt; Petersburg -&gt; Newport News -&gt; Norfolk -&gt; Chesapeake -&gt; Virginia Beach -&gt; Richmond\nTotal distance traveled: = 25+65+30+10+15+100 = 245 miles\n\n\n\n\nTSP Feasibility Map",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "benchmark.html",
    "href": "benchmark.html",
    "title": "Benchmark Optimization Problems",
    "section": "",
    "text": "Benchmark Problems: Overview",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "benchmark.html#onemax-problem",
    "href": "benchmark.html#onemax-problem",
    "title": "Benchmark Optimization Problems",
    "section": "OneMax Problem",
    "text": "OneMax Problem\n\nIn evolutionary algorithms, the OneMax problem serves as a simple test problem where the goal is to evolve a population of binary strings towards the optimal solution (a string of all 1s). The fitness function is used to evaluate the quality of each candidate solution in the population.\nFitness Function: Imagine life had a personal ‚Äòfitness function‚Äô just for you. What variables would you include in it, and how would you weigh them? If 0 meant that you were unable to satisfy that goal, and 1 meant that you were able to satisfy that goal, wouldn‚Äôt you want all 1s.\n\n\nOne Max Formula\n\nBinary String: A binary string is generated using NumPy‚Äôs randint function, which creates a list of 0s and 1s.\nFitness Function: The one_max function calculates the ‚Äúfitness‚Äù of the binary string, which is simply the sum of all 1s in the string. This is the value that needs to be maximized.\nExample Run: If the generated binary string is [1, 0, 1, 1, 0, 1, 0, 1, 1, 0], the fitness would be 6, since there are six 1s in the string.\nThe objective is to maximize the number of 1s in a binary string.\n\n\\[\\max_{s \\in A} f(s) = \\sum_{i=1}^{n} s_i, \\quad \\text{subject to} \\ s_i \\in \\{0, 1\\}.\\]\n\n\nOptimal Solution OneMax\n\nThe optimal solution of this problem is that all the subsolutions assume the value 1; i.e., \\(s_i=1\\) for all \\(i\\). For instance, the optimal solution for \\(n=4\\) is \\(s^*=(1111)\\) and the objective value of a possible solution \\(s^*=(0111)\\) can be easily calculated as the count of the number of ones in the solution \\(s\\) as the objective function if \\(f(s) = f(0111) = 0+1+1+1 = 3\\)\n\n\n\nOneMax Pseudocode\nPseudocode:\n\n\nFUNCTION one_max(binary_string):\n# Calculate the fitness as the sum of 1s in the binary string\n    RETURN sum(binary_string)\n\n# Example usage\nSET n = 10  # Length of the binary string\n\n# Generate a random binary string of length n\nSET binary_string = generate a random list of 0s and 1s of size n\n\n# Calculate the fitness\nSET fitness = one_max(binary_string)\n\n# Print the binary string and its fitness\nPRINT \"Binary string:\", binary_string\nPRINT \"Fitness (number of 1s):\", fitness\n\n\n\nOneMax Python Implementation\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef one_max(binary_string):\n    return np.sum(binary_string)  \n\n# Example usage: # Length of the binary string\nn = 10\n\n# Generate a random binary string of length n (keeps it as a NumPy array)\nbinary_string = np.random.randint(0, 2, size=n)\n\n# Calculate the fitness using np.sum\nfitness = one_max(binary_string)\n\nprint(f\"Binary string: {binary_string}\")\nprint(f\"Fitness (number of 1s): {fitness}\")\n\n#### Example of a fitness plot with new summed fitness score\niterations = 20\nfitness_over_time = np.random.randint(0, n + 1, size=iterations)\n\n# Line plot of fitness over iterations\nplt.figure(figsize=(8, 4))\nplt.plot(range(iterations), fitness_over_time, marker='o', color='green', linestyle='-', linewidth=2)\nplt.fill_between(range(iterations), fitness_over_time, color='lightgreen', alpha=0.4)\nplt.title(f\"Fitness Evolution Over Time\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Fitness (number of 1s)\")\nplt.xticks(np.arange(0, iterations, step=1))  # The step shows whole numbers on the x-axis\n\nplt.grid(True)\nplt.show()\n\nBinary string: [1 1 0 1 0 1 0 1 0 0]\nFitness (number of 1s): 5\n\n\n\n\n\n\n\n\n\n\nThe plot above tracks how fitness improves or changes across iterations in an optimization algorithm, giving insight into the convergence of the algorithm.\n\n\n\nComparing OneMax Problem to Greedy Algorithm\n\nIf a greedy search algorithm is used and is allowed to randomly add one to or subtract one from the current solution \\(s\\) to create the next possible solution \\(v\\) for solving the one-max problem, that is, it is allowed to move one and only one step to either the left or the right of the current solution in the landscape of the solution space.\nWithout knowledge of the landscape of the solution space, the search process will easily get stuck in the peaks of this solution space.\nHence, most researchers prefer using the one-max problem as an example because it is easy to implement and also because it can be used to prove if a new concept for a search algorithm is correct.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "benchmark.html#the-knapsack-problem",
    "href": "benchmark.html#the-knapsack-problem",
    "title": "Benchmark Optimization Problems",
    "section": "The Knapsack Problem",
    "text": "The Knapsack Problem\n\nThe Knapsack Problem is a classic NP-complete optimization problem, where you are given a set of items, each with a weight and a value.\nThe goal is to determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible.\nTypes of Knapsack Problems:\n\n0/1 Knapsack Problem:\n\nEach item can be included (1) or excluded (0) in the knapsack.\nYou cannot break items into smaller parts. \\(\\max_{s \\in A} f(s) = \\sum_{i=1}^{n} s_i v_i, \\quad \\text{subject to} \\quad w(s) = \\sum_{i=1}^{n} s_i w_i \\leq W, \\quad s_i \\in \\{0, 1\\}\\) Where \\(v_i\\) is the value associated with \\(s_1\\) and \\(w_i\\) is the weight associated with \\(s_i\\)\n\nFractional Knapsack Problem:\n\nYou can break items into smaller parts and include fractions of them in the knapsack.\nRatio = \\(\\frac{v_i}{w_i}\\), where \\(v_i\\) is the value of item \\(i\\), and \\(w_i\\) is the weight of item \\(i\\).\n\n\n\n\nNP Complete\n\nNP (Nondeterministic Polynomial Time): A problem is in NP if a solution can be verified in polynomial time by a deterministic algorithm. In other words, given a solution, it is possible to check if it is correct relatively quickly (in polynomial time). However, finding the solution itself might take much longer (potentially exponential time) unless the problem can also be solved in polynomial time.\nNP-complete refers to a class of problems in computational complexity theory that are both NP (nondeterministic polynomial time) and every problem in NP can be reduced to it in polynomial time\nNP-hard problems are optimization or decision problems that are at least as difficult to solve as the hardest problems in NP (nondeterministic polynomial time).\n\nUnlike NP-complete problems, NP-hard problems do not have to be verifiable in polynomial time. This means that while it may be incredibly hard to find an optimal solution, even verifying a proposed solution might take more than polynomial time.\nEssentially, NP-hard problems are hard to solve optimally, and their complexity often prevents efficient algorithms from finding or checking solutions within a reasonable time frame.\nNP-hard problems are broader and potentially harder than NP-complete problems because they can include problems that aren‚Äôt even in NP. They may not have a polynomial-time verification process.\n\n\n\n\nKey Characteristics of NP-complete Problems\n\nDifficult to solve: No known algorithms can solve NP-complete problems efficiently (in polynomial time) for all instances.\nVerification in polynomial time: If someone provides a solution, it can be verified quickly. Equivalence to other NP-complete problems: If one NP-complete problem can be solved in polynomial time, all NP-complete problems can be solved in polynomial time.\nKnapsack Problem: The 0/1 knapsack problem is NP-complete. Finding the optimal solution is hard, but verifying if a solution meets the constraints and maximizes value can be done in polynomial time.\nThe Fractional knapsack problem is not NP-complete and can be solved in polynomial time using a greedy algorithm.\nThe Travelling Salesman is a NP-hard problem.\n\n\n\nExample Fractional Knapsack Problem\n\nItems Available: - Item 1: Value = 10, Weight = 5 kg - Item 2: Value = 40, Weight = 10 kg - Item 3: Value = 30, Weight = 15 kg\nObjective: Maximize the total value without exceeding Knapsack Capacity of 15 kg.\nThe greedy algorithm works well by prioritizing items with the highest value-to-weight ratio.\n0/1 Knapsack Problem requires more complex algorithms like dynamic programming to find the optimal solution over the Fractional Knapsack Problem\n\n\n\nGreedy Algorithm for Fractional Knapsack\n\nStep 1: Calculate Value-to-Weight Ratio:\n\nItem 1: 10/5=2\nItem 2: 40/10=4\nItem 3: 30/15=2\n\nStep 2: Sort Items by Ratio (Descending): Item 2, Item 1, Item 3\n\nStep 3: Fill the Knapsack:\nTake Item 2 (10 kg, Value = 40).\nTake as much of Item 1 as possible (5 kg, Value = 10).\n\nResults: Total Weight = 15 kg, Total Value = 50.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "benchmark.html#binary-to-decimal-b2d-problem-b2d-1",
    "href": "benchmark.html#binary-to-decimal-b2d-problem-b2d-1",
    "title": "Benchmark Optimization Problems",
    "section": "Binary to Decimal (B2D) Problem: B2D-1",
    "text": "Binary to Decimal (B2D) Problem: B2D-1\n\nThe binary to decimal model is often used in optimization problems, particularly in the context of genetic algorithms and heuristic methods.\nWith a minor modification, the solution space of the one-max problem can be simplified as the solution space of another optimization problem.\nThe model uses binary strings to represent numbers. Each string represents a decimal number when interpreted in binary form.\nThe B2D-1 problem is to maximize the value of the objective function of a binary string.\n\n\nCharacteristics and Visualization of B2D-1\n\nThese two examples are possible landscapes to the B2D problem.\nThe first chart to the left implies that there are only two possible next states (candidate solutions) that can be generated from the current solution except for solutions (0000) and (1111), which can only be moved to the right and to the left, respectively.\nIf another search algorithm can generate a new candidate solution by randomly inverting (flipping) one of the subsolutions of the current solution, the number of possible states of the new candidate solution will be \\(n\\), where \\(n\\) is the number of subsolutions.\n\n\n\n\nOneMax",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "benchmark.html#b2d-with-deception-b2d-2",
    "href": "benchmark.html#b2d-with-deception-b2d-2",
    "title": "Benchmark Optimization Problems",
    "section": "B2D with Deception: B2D-2",
    "text": "B2D with Deception: B2D-2\n\nB2D deception problems mislead optimization algorithms away from finding the global optimum by presenting local optima that seem promising but are actually suboptimal.\nUsed to test whether a search algorithm is capable of escaping local optima or not.\nDeception problems highlight the necessity of exploration in heuristic algorithms, such as introducing diversity through mutation or crossover in genetic algorithms. If the algorithm becomes too greedy and focuses only on local fitness improvements (exploitation), it may get stuck at deceptive local optima.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "benchmark.html#single-objective-optimization-problem-sop",
    "href": "benchmark.html#single-objective-optimization-problem-sop",
    "title": "Benchmark Optimization Problems",
    "section": "Single-objective Optimization Problem (SOP)",
    "text": "Single-objective Optimization Problem (SOP)\n\nA single-objective optimization problem involves finding the best solution from a set of feasible solutions based on a single objective function. The goal is to either maximize or minimize this objective function.\n\n\\[\\underset{s \\in \\mathbb{R}^n}{\\text{opt}} f(s), \\quad \\text{subject to } \\, c_i(s) \\odot b_i, \\quad i = 1, 2, \\ldots, m,\\]\n\nwhere\n\n\\({R}^n\\) and \\({R}\\) are the domain and codomain, respectively,\n\\(f(s) {R}^n\\) and \\({R}\\) is the objective function to be optimized,\n\\(c_i(s): {R}^n\\) and \\({R}\\odot b_i, \\quad i = 1, 2, \\ldots, m,\\) are the constraints,\nand \\(opt\\) and \\(\\odot\\) are as given in Definition 1 as &lt;, &gt;, =, ‚©Ω, or ‚©æ.\n\n\n\nAckley Function: A Single Optimization Problem\n\nThe Ackley Function is a widely used benchmark function for testing optimization algorithms. It is characterized by its multi-modal nature with a nearly flat outer region and a large hole at the center.\nApplications\n\nUsed as a standard test case in evaluating the performance of optimization algorithms like genetic algorithms, simulated annealing, and particle swarm optimization.\nRelevant in fields such as machine learning, control systems, and operations research.\n\nLimitations\n\nThe function‚Äôs large search space and numerous local minima make it difficult for algorithms to converge to the global minimum.\nLarge importance of balancing exploration and exploitation in optimization strategies when dealing with the Ackley Function.\n\n\n\nExplanation of Ackley Function(x, y)\n\nComputes the value of the Ackley function given a point (x, y) in the search space.\nThe optimization algorithm optimizes the Ackley function to find the point where it reaches its minimum. It initializes a population of random solutions, evaluates their fitness (using the Ackley function), and iteratively improves them using an optimization method (like gradient descent or a genetic algorithm).\nThe best solution and corresponding function value (score) are returned as the result.\n\n\n\nAckley function and B2D: Converting to Decimal\n\nThe Ackley function uses the binary representation, where the binary strings need to be converted to decimal values (i.e., real numbers). In this case, the converted decimal values correspond to points in the continuous search space.\nFor example, a binary string like 1010 can be converted into a decimal value, which can then be used as input to the Ackley function. Binary String: 1010, where the binary number is 1010_2.\nEach position in the binary number represents a power of 2, starting from the right (least significant bit):\nThe rightmost bit (0) is \\(2^0\\),The next bit (1) is \\(2^1\\) ,The next bit (0) is \\(2^2\\),The leftmost bit (1) is \\(2^3\\).\n\n\\(1010_2= 0 ‚àó 2^0+ 1 ‚àó 2^1 +0 ‚àó 2^2+1 ‚àó 2^3\\) \\(= 0 ‚àó 1+ 1 ‚àó 2 + 0 ‚àó 4 + 1 ‚àó 8\\) \\(=  0 + 2 + 0 + 8 = 10\\) Thus, the decimal equivalent of the binary string ‚Äú1010‚Äù is 10. Use this value as input for the Ackley function.\n\n\nCharacteristics and Visualization of Ackley Function\n\nThe Ackley function is evaluated in the hypercube.\nThe global optimum (minimum) of the Ackley function is ùëì(ùë†^‚àó)=0 is located at \\(s^*=(0,0,‚Ä¶0)\\).\nThis function has many local optima, which makes it hard for the search algorithm to find the global optimum.\n\n\n\n\nAckley\n\n\n\n\nAckley Function Formula\n\\[\n\\begin{array}{rl}\n\\min_{s \\in \\mathbb{R}^n} f(s) &= -20 \\exp \\left(-0.2 \\sqrt{\\frac{1}{n} \\sum_{i=1}^n s_i^2} \\right) \\\\\n& \\quad - \\exp \\left( \\frac{1}{n} \\sum_{i=1}^n \\cos(2 \\pi s_i) \\right) + 20 + e, \\\\\n\\text{subject to} & \\quad -30 \\leq s_i \\leq 30, \\quad i = 1, 2, \\dots, n.\n\\end{array}\n\\]\n\n\nAckley Function: Pseudocode\n\nThe below example uses a random function to pull a point that we want to hit the local minima. You can imagine, this might not be the best way to do this.\n\nPSEUDOCODE\nFUNCTION Ackley(s):\n    SET a = 20, b = 0.2, c = 2 * pi\n    SET n = length of s\n    COMPUTE sum_sq_term = sum of squares of all elements in s\n    COMPUTE cos_term = sum of cos(2 * pi * each element in s)\n    \n    COMPUTE term1 = -a * exp(-b * sqrt(sum_sq_term / n))\n    COMPUTE term2 = -exp(cos_term / n)\n    \n    RETURN term1 + term2 + a + e\n\n# Main Execution\nSET n = 2  # Dimension of the Ackley function\n\n# Generate random vector s with elements between -30 and 30\nSET s = random values in range [-30, 30] of length n\n\n# Compute the Ackley function result for the vector s\nSET result = Ackley(s)\n\nPRINT vector s\nPRINT Ackley function result for vector s\n\n\n\nAckley Function Python Implementation\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nnp.random.seed(5042)\n\n# Ackley function implementation\ndef ackley(s):\n    a, b, c = 20, 0.2, 2 * np.pi\n    n = len(s)\n    sum_sq_term = np.sum(s**2)\n    cos_term = np.sum(np.cos(c * s))\n    term1 = -a * np.exp(-b * np.sqrt(sum_sq_term / n))\n    term2 = -np.exp(cos_term / n)\n    return term1 + term2 + a + np.e\n\n# Example usage: \nn = 2  # Dimension \ns = np.random.uniform(-30, 30, n)  # Generate random s_i values in the range [-30, 30]\n\nresult = ackley(s)  # Evaluate the Ackley function\n\nprint(f\"Vector s: {s}\")\nprint(f\"Ackley function result: {result}\")\n\n# Visualization of the Ackley function\nx = np.linspace(-30, 30, 400)\ny = np.linspace(-30, 30, 400)\nX, Y = np.meshgrid(x, y)\n\n# Compute Z for the Ackley function\nZ = np.array([ackley(np.array([x_val, y_val])) for x_val, y_val in zip(np.ravel(X), np.ravel(Y))])\nZ = Z.reshape(X.shape)\n\n# Plotting the Ackley function surface\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none')\n\n# Customize the plot\nax.set_title(\"Ackley Function Surface\")\nax.set_xlabel(\"s_1\")\nax.set_ylabel(\"s_2\")\nax.set_zlabel(\"f(s)\")\n\n# Show the plot\nplt.show()\n\nVector s: [12.84779359  3.16468632]\nAckley function result: 17.91745666838746\n\n\n\n\n\n\n\n\n\n\nExample results: Vector s: [12.8 3.16]; Ackley function result: 17.9. These results are random, so it may vary from what you see in the plot.\nDistance from the Origin: The Ackley function reaches its global minimum of 0 at the origin (i.e., when both \\(x_1\\) and \\(x_2\\) are close to 0). Our vector values are quite far from the origin, which is why the function result is positive and relatively large at 17.9‚Äù\nThe Ackley landscape has an exponentially increasing structure as you move away from the global minimum. It has many local minima, which makes optimization algorithms prone to getting stuck in suboptimal solutions. A result like 17.9 is far from zero, and indicates that the vector is located in such a suboptimal region of the function space.\nThus, the Ackley result of 17.9 suggests that the point [12.8 3.16]; is not close to the global minimum (which is zero at the origin) and is located in a region of higher function values.\n\n\n\nDifferential Evolution with the Ackley Function\n\nTo demonstrate how an algorithm does well on the Ackley function, we can use a global optimization algorithm such as Differential Evolution, which is effective for non-convex functions with many local minima.\nDifferential Evolution is a population-based optimization algorithm used for solving complex multidimensional problems. It belongs to the family of evolutionary algorithms, where a population of candidate solutions evolves over time to find the global optimum of a function.\nThe differential_evolution function from the scipy.optimize module is a powerful optimization tool designed to solve global optimization problems. It is a type of evolutionary algorithm, which is used when the function to optimize is non-linear, has many local minima, or is not differentiable.\n\n\nimport numpy as np\nfrom scipy.optimize import differential_evolution\nimport matplotlib.pyplot as plt\n\nnp.random.seed(5042)\n\n# Define the Ackley function\n# Ackley function implementation\ndef ackley(s):\n    a, b, c = 20, 0.2, 2 * np.pi\n    n = len(s)\n    sum_sq_term = np.sum(s**2)\n    cos_term = np.sum(np.cos(c * s))\n    term1 = -a * np.exp(-b * np.sqrt(sum_sq_term / n))\n    term2 = -np.exp(cos_term / n)\n    return term1 + term2 + a + np.e\n\n# Set the bounds for the variables \nbounds = [(-30, 30), (-30, 30)]\n\n# Use differential evolution to minimize the Ackley function\nresult = differential_evolution(ackley, bounds, seed=42)\n\n# Print the result\nprint(f'Optimized parameters (x1, x2): {result.x}')\nprint(f'Function value at minimum: {result.fun}')\n\n\n# Visualization of the Ackley function\nx = np.linspace(-30, 30, 400)\ny = np.linspace(-30, 30, 400)\nX, Y = np.meshgrid(x, y)\n\n# Compute Z for the Ackley function\nZ = np.array([ackley(np.array([x_val, y_val])) for x_val, y_val in zip(np.ravel(X), np.ravel(Y))])\nZ = Z.reshape(X.shape)\n\n# Plotting the Ackley function surface\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none')\n\n# Customize the plot\nax.set_title(\"Ackley Function Surface (2D)\")\nax.set_xlabel(\"s_1\")\nax.set_ylabel(\"s_2\")\nax.set_zlabel(\"f(s)\")\n\n# Show the plot\nplt.show()\n\nOptimized parameters (x1, x2): [0. 0.]\nFunction value at minimum: 4.440892098500626e-16\n\n\n\n\n\n\n\n\n\n\nUsing a great function that is designed to solve problems with multiple local minimums, you can see that we got extremely close to the local minimum 0.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "benchmark.html#multi-objective-optimization-problem-mop",
    "href": "benchmark.html#multi-objective-optimization-problem-mop",
    "title": "Benchmark Optimization Problems",
    "section": "Multi-objective Optimization Problem (MOP)",
    "text": "Multi-objective Optimization Problem (MOP)\n\nGiven a set of functions and a set of constraints, the MOP is to find the optimal value or a set of optimal values (also called Pareto front), subject to the constraints, out of all possible solutions of these functions.\nThe Pareto front consists of solutions where no objective can be improved without worsening at least one other objective. For example, In product design, you might want to minimize cost while maximizing performance. These two objectives are often in conflict, meaning improving one leads to trade-offs in the other.\n\n\\[\\text{opt}\\left( f_1(s), f_2(s), \\dots, f_k(s) \\right),\n\\quad \\mathbf{s} \\in \\mathbb{R}^n,\n\\quad \\text{subject to } c_i(s) \\odot b_i, \\quad i = 1, 2, \\dots, m,\\]\n\nWhere\n\n\\({R}^n\\) and \\({R}\\) are the domain and codomain, respectively,\n\\(f(s) {R}^n\\) and \\({R}\\) is the objective function to be optimized,\n\\(c_i(s): {R}^n\\) and \\({R}\\odot b_i, \\quad i = 1, 2, \\ldots, m,\\) are the constraints,\nand \\(opt\\) and \\(\\odot\\) are as given in Definition 1 as &lt;, &gt;, =, ‚©Ω, or ‚©æ.\n\n\n\nThe Schaffer min-min Multi-objective Optimization Problem\n\nThe Schaffer min-min problem is a well-known test function in the field of multi-objective optimization.\nIt is often used to evaluate optimization algorithms due to its simplicity and well-defined structure. The problem is particularly famous for having a simple Pareto-optimal front.\nThe Schaffer function can be defined as a two-objective optimization problem, where the objectives are functions of a single variable x.\nThe goal is to minimize both of these objective functions simultaneously.\n\n\nCharacteristics and Visualization\n\nConvexity: The Pareto front of the Schaffer min-min problem is convex, making it relatively easy to identify the trade-off surface between the two objectives.\nGraphically, the Pareto front of the Schaffer min-min problem can be visualized as a curve in the objective space, where \\(f_1(x)\\) is plotted against \\(f_2(x)\\) and the curve represents the set of optimal trade-offs between the two objectives.\n\n\n\n\n\n\n\n\n\n\n\n\nscipy.optimize import minimize\n\nminimize is a general-purpose function from scipy.optimize used for finding the minimum of a scalar function.\nUses the BFGS (Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno) algorithm when no specific method is provided. This method is a quasi-Newton optimization algorithm, particularly useful for smooth unconstrained problems.\n\nIt can handle different types of optimization problems, including: Unconstrained minimization\nConstrained minimization (equality and inequality constraints) Bounded minimization (where variables are limited to a certain range)\n\nBasic Workflow:\n\nDefine the objective function (the function to minimize). Choose an initial guess for the variables.\nRun the minimize function with the desired method.\nAnalyze the results: returns optimized variables, the function value, and other diagnostic information.\n\n\n\n\nSchaffer Min-Min Formula\n\\[\n\\min_{s \\in \\mathbb{R}^n}\n\\begin{cases}\nf_1(s) = s^2, \\\\\nf_2(s) = (s - 2)^2,\n\\end{cases}\n\\quad \\text{subject to} \\quad s \\in [-10^3, 10^3].\n\\]\n\n\nSchaffer Min-Min Pseudocode\nPseudocode\n\n# FUNCTION to calculate f1(s)\nFUNCTION f1(s):\n    RETURN s^2\n\n# FUNCTION to calculate f2(s)\nFUNCTION f2(s):\n    RETURN (s - 2)^2\n\n# FUNCTION for combined objective, weighted sum of f1 and f2\nFUNCTION combined_objective(s, w1=0.5, w2=0.5):\n    RETURN w1 * f1(s) + w2 * f2(s)\n\n# MAIN EXECUTION\n# Step 1: Set up the bounds for the solution (s ‚àà [-1000, 1000])\nSET bounds = [-1000, 1000]\n\n# Step 2: Initialize a starting guess for the solution\nSET initial_guess = 0\n\n# Step 3: Minimize the combined objective function using an optimization algorithm\nCALL minimize function with combined_objective, initial_guess, and bounds\nSTORE the result in result\n\n# Step 4: Print the optimization result\nPRINT \"Optimal value of s:\", result.x\nPRINT \"f1(s):\", f1(result.x)\nPRINT \"f2(s):\", f2(result.x)\nPRINT \"Combined objective:\", combined_objective(result.x)\n\n# Step 5: Visualization - Create a range of values for s from -1000 to 1000\n\n\n\nSchaffer Min-Min Python Implementation\n\nPopulation Initialization: A population of random solutions is initialized within the bounds [‚àí1000,1000]. Objective Function Evaluation: For each solution, both objective functions are evaluated.\nScore Combination: The results of the two functions are combined into a single score, which can be minimized. In this case, the combination is a simple sum of f1 and f2.\nOptimization Loop: Iteratively updates the solutions to find the minimum combined score using an optimization technique (e.g., gradient descent, genetic algorithm).\nThe plot uses the weighted sum minimization, w1 * f1(s) + w2 * f2(s) instead of plotting the Pareto front. The plot prints The optimal \\(s\\) value and the corresponding function values \\(f1(s)\\) and \\(f2(s)\\) at that point. The minimum combined objective function value.\n\n\nimport numpy as np\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\n\n\n# Define the two objective functions for the Schaffer problem\ndef f1(s):\n    return s**2\n\ndef f2(s):\n    return (s - 2)**2\n\n# Combined objective function: weighted sum of f1 and f2\n# You can adjust the weights to explore different trade-offs between the two objectives\ndef combined_objective(s, w1=0.5, w2=0.5):\n    return w1 * f1(s) + w2 * f2(s)\n\n# Bounds for the solution (s ‚àà [-1000, 1000])\nbounds = [(-1000, 1000)]\n\n# Initial guess for the solution\ninitial_guess = np.array([0])\n\n# Use scipy's minimize function to find the solution\nresult = minimize(combined_objective, initial_guess, bounds=bounds)\n\n# Print the result\nprint(\"Optimal value of s:\", result.x[0])\nprint(\"f1(s):\", f1(result.x[0]))\nprint(\"f2(s):\", f2(result.x[0]))\nprint(\"Combined objective:\", combined_objective(result.x[0]))\n\n# Visualization of the objective functions and combined objective\ns_values = np.linspace(-1000, 1000, 400)\nf1_values = f1(s_values)\nf2_values = f2(s_values)\ncombined_values = combined_objective(s_values)\n\n# Plotting\nplt.figure(figsize=(10, 6))\n\n# Plot f1(s), f2(s), and combined objective\nplt.plot(s_values, f1_values, label=\"f1(s) = s^2\", color='blue')\nplt.plot(s_values, f2_values, label=\"f2(s) = (s - 2)^2\", color='green')\nplt.plot(s_values, combined_values, label=\"Combined Objective\", color='red', linestyle='--')\n\n# Mark the optimal solution found\nplt.axvline(x=result.x[0], color='black', linestyle=':', label=f\"Optimal s = {result.x[0]:.2f}\")\n\n# Customize the plot\nplt.title(\"Schaffer Min-Min Problem Visualization\")\nplt.xlabel(\"s\")\nplt.ylabel(\"Objective Function Value\")\nplt.legend()\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\nOptimal value of s: 1.0000000134831095\nf1(s): 1.0000000269662193\nf2(s): 0.9999999730337812\nCombined objective: 1.0000000000000002\n\n\n\n\n\n\n\n\n\n\n\nLooking at the results\n\nThe results you achieved for the Schaffer Min-Min problem look excellent, as they closely approximate the optimal solution.\nOptimal value of \\(s\\) we found is nearly exactly \\(1\\), the known optimal solution for the Schaffer function.\n\nObjective function values: \\(f_1(s) = s^2 = 1.000000027\\)\n: \\(f_2(s) = (s-2)^2 = 0.999999973\\)\nThese values are very close to 1 for both \\(f_1\\) and \\(f_2\\), indicating that the function values at this \\(s\\) are near-optimal.\n\nCombined objective: The combined objective (likely calculated as a weighted sum or some other combination of \\(f_1\\) and \\(f_2\\) is 1.00000, which is extremely close to the expected combined optimal value of 1. This negligible difference suggests that the optimization algorithm has performed very well.\nHere is the code used to generate the Pareto Front for the Schaffer Min-Min problem, filtering out only the non-dominated solutions and plotting the trade-off curve. This allows us to see \\(f_1\\) and \\(f_2\\) on the x and y axis.\nThere are so many ways to graph this, but the multiple dimensions makes it more difficult to see. Explore some options using generative AI.\n\n\n# Generate s values\ns_values = np.linspace(-10, 10, 400)\n\n# Identifying the non-dominated solutions (Pareto front)\npareto_f1 = []\npareto_f2 = []\n\nfor s in s_values:\n    f1_val = f1(s)\n    f2_val = f2(s)\n\n    # A point (f1, f2) is Pareto-optimal if no other point dominates it\n    if not any(other_f1 &lt;= f1_val and other_f2 &lt;= f2_val for other_f1, other_f2 in zip(pareto_f1, pareto_f2)):\n        pareto_f1.append(f1_val)\n        pareto_f2.append(f2_val)\n\n# Sorting to ensure a smooth Pareto front curve\npareto_f1, pareto_f2 = zip(*sorted(zip(pareto_f1, pareto_f2)))\n\n# Plotting the Pareto front with a more visible optimal point\nplt.figure(figsize=(8, 6))\nplt.plot(pareto_f1, pareto_f2, marker='o', linestyle='-', color='red', label=\"Pareto Front\")\n\n# Mark the optimal solution (1,1) with a larger marker and annotation\nplt.scatter(1, 1, color='black', marker='o', s=100, label=\"Optimal (1,1)\")\nplt.annotate(\"Optimal (1,1)\", xy=(1, 1), xytext=(10, 20),\n             textcoords=\"offset points\", fontsize=12, color='black',\n             arrowprops=dict(facecolor='black', shrink=0.05))\n\n# Customize the plot\nplt.title(\"Pareto Front of the Schaffer Min-Min Problem\", fontsize=14)\nplt.xlabel(\"f1(s) = s^2\", fontsize=12)\nplt.ylabel(\"f2(s) = (s - 2)^2\", fontsize=12)\nplt.legend(fontsize=12)\nplt.grid(True)\n\n# Show the plot\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "es.html",
    "href": "es.html",
    "title": "Exhaustive Search",
    "section": "",
    "text": "Advantages and Disadvantages of ES\nIf you had unlimited time to search through every possible hobby in the world, which one do you think you‚Äôd end up pursuing?",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "es.html#advantages-and-disadvantages-of-es",
    "href": "es.html#advantages-and-disadvantages-of-es",
    "title": "Exhaustive Search",
    "section": "",
    "text": "Advantages:\n\nGuaranteed Optimal Solution: Finds the best possible solution, ensuring optimality.\nSimplicity: Easy to understand and implement for small-scale problems.\n\nDisadvantages:\n\nScalability Issues: Infeasible for large datasets due to exponential time complexity.\nResource Intensive: High computational cost in terms of time and memory.\n\n\n\nExhaustive Search Algorithm\n\nCheck all the candidate solutions in the solution space of the problem in question; therefore, it is always capable of finding the best solution for the problem in question.\n\n\n\n\nes algorithm",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "es.html#exhaustive-search-in-continuous-vs.-discrete-domains",
    "href": "es.html#exhaustive-search-in-continuous-vs.-discrete-domains",
    "title": "Exhaustive Search",
    "section": "Exhaustive Search in Continuous vs.¬†Discrete Domains",
    "text": "Exhaustive Search in Continuous vs.¬†Discrete Domains\n\nDiscrete Domains: Solutions can be explicitly enumerated without discretization.\n\nExample: Knapsack problem.\n\nContinuous Domains: Require discretization (using a step size) to make exhaustive search feasible.\n\nExample: Ackley function.\n\nChallenges in Continuous Domains:\n\nInfinite solution space makes direct enumeration impossible.\nStep size is critical for balancing resolution and computational cost.\n\nNote that Discretization is the process of converting continuous data or variables into discrete categories or bins.\n\n\nRole of Step Size in Continuous Optimization\n\nStep size in exhaustive search is the distance between consecutive sample points in the search space. A small step size results in a finer grid with higher precision but increases computation time, while a larger step size reduces computation time but may miss optimal solutions.\nWhy Step Size Matters: Controls the granularity of the search.\n\nSmaller steps increase resolution but significantly raise computation time.\nLarger steps reduce computational cost but risk missing the optimal solution.\n\nPractical Considerations:\n\nOptimal step size depends on the problem‚Äôs scale and the required precision.\nTrade-off between efficiency and accuracy.\n\nWhen is Exhaustive Search Practical?\n\nDiscrete Domains:\n\nFeasible if the solution space is small (manageable number of combinations).\nGuarantees finding the exact optimal solution.\n\nContinuous Domains:\n\nOnly practical when the domain is small and can be discretized effectively.\n\nHigher dimensions increase complexity exponentially (curse of dimensionality).",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "es.html#the-search-strategy-of-es",
    "href": "es.html#the-search-strategy-of-es",
    "title": "Exhaustive Search",
    "section": "The Search Strategy of ES",
    "text": "The Search Strategy of ES",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "es.html#algorithmic-steps-of-es",
    "href": "es.html#algorithmic-steps-of-es",
    "title": "Exhaustive Search",
    "section": "Algorithmic Steps of ES",
    "text": "Algorithmic Steps of ES\n\nFor each binary solution (up to num_bits length), the code evaluates the number of 1‚Äôs in its binary representation (which is used as the ‚Äúfitness‚Äù value).\nIt then finds the solution with the highest number of 1‚Äôs (the one with the most bits set to 1).\nImport necessary tools: time for timing and matplotlib for plotting results.\nSet up run function\n\nLoop through all possible solutions.\nEvaluate each one and track its fitness.\nUpdate the best solution as needed.\nReturn the best solution and fitness progress.\n\nDefine helper functions:\n\nTransit: Move to the next solution.\nEvaluate: Count the number of 1s in the binary version of the solution (higher is better).\nDetermine: Compare fitness in order to track the best solution found so far.",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "es.html#imports",
    "href": "es.html#imports",
    "title": "Exhaustive Search",
    "section": "Imports",
    "text": "Imports\n\nimport time\nimport matplotlib.pyplot as plt",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "es.html#run_exhaustive_search-function",
    "href": "es.html#run_exhaustive_search-function",
    "title": "Exhaustive Search",
    "section": "run_exhaustive_search function",
    "text": "run_exhaustive_search function\n\nrun_exhaustive_search(num_bits):\nCalculates the total number of possible solutions max_sol is the maximum possible solution, which is \\(2^{numbits}\\). \\(2 ** numbits\\): This correctly represents the total number of possible combinations when you have numbits binary bits. For example, if numbits = 3, the possible solutions range from \\(000 (0)\\) to \\(111 (7)\\), resulting in \\(2^3=8\\) solutions.\nInitializes \\(s\\) is the current solution, initially set to 0.\nInitializes \\(best_fitness\\) and \\(best_solution\\) to track the best results.\nInitializes an empty list \\(fitness_over_time\\) to track the fitness at each step.\n\n\n# Function for initialization (I)\ndef init_es(num_bits=10):\n    max_sol = 2 ** num_bits  # Total number of solutions\n    s = 0  # Step 1: Set the initial solution s = (0)\n    f_s = evaluate(s)  # Step 2: Evaluate initial solution\n    return s, f_s, max_sol",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "es.html#helper-functions",
    "href": "es.html#helper-functions",
    "title": "Exhaustive Search",
    "section": "Helper Functions",
    "text": "Helper Functions\n\ntransit(s): Simply increments the current solution by 1.\n\n\n# Function for transit (T)\ndef transit(s):\n    s += 1\n    return s\n\n\nevaluate(s): Evaluates the ‚Äúfitness‚Äù of the solution by counting how many 1‚Äôs are present in its binary representation. The more 1‚Äôs, the better the solution.\n\n\n# Function for evaluation (E)\ndef evaluate(s):\n    return bin(s).count(\"1\")  # Counts the number of 1s in the binary representation of s\n\n\ndetermine(fv, v, fs, s): This method checks whether the fitness of the new solution (fv) is greater than the current best fitness (fs). If so, it updates the current best solution and fitness.\n\n\n# Function for determine (D)\ndef determine(fv, v, fs, s):\n    if fv &gt; fs:\n        fs, s = fv, v\n    return fs, s",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "es.html#main-loop",
    "href": "es.html#main-loop",
    "title": "Exhaustive Search",
    "section": "Main Loop",
    "text": "Main Loop\n\nA while loop iterates through all possible solutions (represented by s). For each solution s, it calculates the fitness using the evaluate() function.\n\nFor each iteration, the solution is updated (transit method), evaluated (evaluate method), and the best solution is determined (determine method).\nThe loop continues until all possible solutions (v from 0 to max_sol) have been evaluated.\n\nfitness is the ‚Äúfitness‚Äù score of the current solution, calculated using evaluate(s) (which counts the number of 1‚Äôs in the binary representation of s).\nAppends the fitness value to fitness_over_time.\ndetermine() updates fs (fitness) and s (solution) if the new solution (v) has a higher fitness.\nIf the current solution has a better fitness than the best found so far, it updates the best fitness and solution.\nReturn Values: Returns fitness_over_time and best_solution after completing the exhaustive search.\n\n\n# Run function for exhaustive search\ndef run_exhaustive_search(num_bits=10):\n    # Initialize using init_es\n    s, f_s, max_sol = init_es(num_bits)\n    \n    #Set v = s\n    best_solution = s\n    best_fitness = f_s\n    \n    fitness_over_time = []\n    \n    # While the termination criterion is not met\n    while s &lt; max_sol:\n        # Generate the next solution v = GenNext(v)\n        v = s\n        \n        # Evaluate the new solution\n        f_v = evaluate(v)\n        \n        # Track fitness over time for plotting\n        fitness_over_time.append(f_v)\n        \n        # Print current solution and its fitness: Print command commented out for simplicity because it prints all the comparisons. \n        #print(f\"{f_v} # {v:0{num_bits}b}\")\n        \n        #If f_v is better than f_s, update s = v, f_s = f_v\n        best_fitness, best_solution = determine(f_v, v, best_fitness, best_solution)\n\n        #continued: Move to the next solution\n        s += 1  # Increment solution\n    \n    #Returned for Output the best solution\n    return fitness_over_time, best_solution",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "es.html#main-execution",
    "href": "es.html#main-execution",
    "title": "Exhaustive Search",
    "section": "Main Execution",
    "text": "Main Execution\n\nIn exhaustive search, the ‚Äúmain execution‚Äù often focuses on setting the number of bits because exhaustive search involves systematically evaluating all possible solutions in the search space. When the solutions are represented as binary strings, the number of bits directly determines the size of the search space.\nWhy it matters:\n\nDefining the Search Space: The number of bits defines how many unique binary strings (or configurations) are possible. For \\(2^n\\) possible combinations. So, by setting the number of bits, you are effectively defining the boundaries of the search space.\nEnumerating All Possibilities: In exhaustive search, the algorithm needs to evaluate every possible configuration to ensure the optimal solution is found. Each unique binary string corresponds to a specific candidate solution, so having the number of bits set enables the algorithm to enumerate all possible configurations.\nComputational Complexity: The number of bits directly affects the computational complexity of exhaustive search. With \\(n\\) bits, the search space grows exponentially, which is manageable for small \\(n\\) but quickly becomes infeasible for larger \\(n\\). By controlling the number of bits, you also control the practical feasibility of the exhaustive search.\n\n\nSimplicity: Since exhaustive search doesn‚Äôt involve complex heuristics or probabilistic methods, setting the number of bits becomes the main task. The rest of the algorithm is straightforward: generate each binary string, evaluate its objective value, and keep track of the best solution.\n\n# Main execution\nnum_bits = 10  # You can adjust the number of bits",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "es.html#output",
    "href": "es.html#output",
    "title": "Exhaustive Search",
    "section": "Output",
    "text": "Output\n\n# Run the exhaustive search and get fitness values\nstart_time = time.time()\nfitness_over_time, best_solution = run_exhaustive_search(num_bits)\nend_time = time.time()\nexecution_time = end_time - start_time  # Calculate elapsed time\n\n# Output (O)\nprint(f\"Exhaustive Search One-Max Time elapsed: {execution_time:.6f} seconds\")  # Print the elapsed time\nprint(f\"# name of the search algorithm: Exhaustive Search\")\nprint(f\"# number of bits: {num_bits}\")\n\n# Plot the evolution of fitness over time\nplt.figure(figsize=(10, 6))\nplt.plot(fitness_over_time, label='Fitness over time', color='blue', linewidth=2)\nplt.title(f'Exhaustive Search: Fitness Evolution ({num_bits} bits)')\nplt.xlabel('Evaluations')\nplt.ylabel('Fitness (Number of 1s)')\nplt.grid(True)\nplt.legend()\nplt.show()\n\nExhaustive Search One-Max Time elapsed: 0.001505 seconds\n# name of the search algorithm: Exhaustive Search\n# number of bits: 10",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Exhaustive Search</span>"
    ]
  },
  {
    "objectID": "hc.html",
    "href": "hc.html",
    "title": "Hill Climbing",
    "section": "",
    "text": "Hill Climbing Algorithm\nIf life is like hill climbing, what‚Äôs the moment where you took a single small step that ended up leading you to your biggest peak?",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#search-strategy-of-hc",
    "href": "hc.html#search-strategy-of-hc",
    "title": "Hill Climbing",
    "section": "Search Strategy of HC >",
    "text": "Search Strategy of HC &gt;\n\nThe search strategy of HC that accepts only a better solution as the next solution.\nSuppose HC has a 50/50 chance to move either left or right in solving the one-max problem; then, in addition to the global optimum (1111), HC may end up in one of the seven or three local optima. \\(f(v)&gt;f(s)\\).\n\n\n\n\nhc search",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#search-strategy-of-hc-1",
    "href": "hc.html#search-strategy-of-hc-1",
    "title": "Hill Climbing",
    "section": "Search Strategy of HC >=",
    "text": "Search Strategy of HC &gt;=\n\nThe search strategy of HC accepts both better solutions and solutions that are equally good as the next solution.\nThe possible result if HC accepts not only a better solution but also a solution that is equally good (i.e., \\(f(v)&gt;=f(s)\\) as the next solution; then only the solutions \\(x_4\\) \\((0011)\\), \\(x_8\\) \\((0111)\\), and \\(x_12\\) \\((1011)\\) will remain as the local optima, while \\(x_16\\) \\((1111)\\) is the global optimum.\n\n\n\n\nhc search",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#hill-climbing-algorithm",
    "href": "hc.html#hill-climbing-algorithm",
    "title": "Hill Climbing",
    "section": "",
    "text": "Start: Initialize with a random solution or a predefined starting point.\nEvaluate: Assess the quality of the current solution using a heuristic function.\nGenerate Neighbors: Produce a set of neighboring solutions by making small changes.\nSelect Best Neighbor: Choose the neighbor that has the highest heuristic value.\nMove: Replace the current solution with the selected neighbor.\nRepeat: Continue the process until no better neighbors are found or a stopping criterion is met.\n\n\n\n\nhc algo",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#challenges-of-hc",
    "href": "hc.html#challenges-of-hc",
    "title": "Hill Climbing",
    "section": "Challenges of HC",
    "text": "Challenges of HC\n\nLocal Maximum: Hill climbing may get stuck at a local maximum, where no neighboring solution improves, but better solutions exist further away.\nPlateaus: It can struggle on flat regions where no clear direction of improvement is evident.\nRidges: Difficulties in navigating narrow ridges that require moving sideways to find a better peak.\nVariations to Overcome Limitations\n\nStochastic Hill Climbing: Introduces randomness to avoid local maxima.\nSimulated Annealing: Uses probabilistic decisions to escape local maxima and explore a larger solution space.\nSteepest-Ascent Hill Climbing: Considers all neighbors and selects the one with the steepest ascent.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#set-up-run-function",
    "href": "hc.html#set-up-run-function",
    "title": "Hill Climbing",
    "section": "Set up Run Function",
    "text": "Set up Run Function\n\nEvaluate the initial solution by counting the number of 1s, which serves as the ‚Äúfitness‚Äù value.\nSearch for Neighbors: Generate neighboring solutions by flipping one bit at a time in the current solution. For each neighbor, calculate its fitness (number of 1s in its binary representation).\nSelect Best Neighbor: Identify the neighboring solution with the highest fitness.If this neighbor‚Äôs fitness is better than the current solution‚Äôs fitness, update the current solution to this neighbor.\nRepeat Until Convergence: Continue generating and evaluating neighbors until no neighboring solution improves the current fitness (local maximum reached).Track the fitness of the best solution at each iteration for plotting or analysis.\nReturn Best Solution and Fitness Progress: Output the best solution found along with a list of fitness values over iterations.\n\n\n# Run function for hill climbing\ndef run_hc(num_bits=10, max_evals=2 ** num_bits):\n    sol = init_hc(num_bits)  # Initialize random solution\n    fitness = evaluate(sol)  # Evaluate the initial solution\n    fitness_over_time = []\n    \n    eval_count = 0\n\n    # Main loop of evaluations\n    while eval_count &lt; max_evals:\n        tmp_sol = transit(sol)  # Make a random change (flip a bit)\n        tmp_fitness = evaluate(tmp_sol)  # Evaluate the new solution\n        sol, fitness = determine(tmp_sol, tmp_fitness, sol, fitness)  # Determine if we accept the new solution\n        \n        fitness_over_time.append(fitness)  # Track the fitness over time\n        eval_count += 1\n        \n    return fitness_over_time, sol\n\n\nThe main execution section initiates the hill climbing algorithm, recording the time taken to execute it. It captures the progression of fitness values over each iteration and identifies the best solution found, while calculating the total execution time.\n\n\n# Main Execution\n# Run hill climbing and get fitness values\nstart_time = time.time()\nfitness_over_time, best_solution = run_hc(num_bits)\nend_time = time.time()\nexecution_time = end_time - start_time  # Calculate elapsed time\n\n\nThe output section displays key details about the hill climbing algorithm, including its name, the number of bits used, and the time taken for execution. It then visualizes the fitness evolution across evaluations, providing a clear plot that shows how fitness values change over time as the algorithm progresses toward the best solution.\n\n\n# Output and visualize\nprint(f\"# Name of the search algorithm: Hill Climbing\")\nprint(f\"# number of bits: {num_bits}\")\nprint(f\"Time elapsed: {execution_time:.6f} seconds\")  # Print the elapsed time\n\n# Plot the evolution of fitness over time\nplt.figure(figsize=(10, 6))\nplt.plot(fitness_over_time, label='Fitness over time', color='blue', linewidth=2)\nplt.title(f'Hill Climbing: Fitness Evolution ({num_bits} bits)')\nplt.xlabel('Evaluations')\nplt.ylabel('Fitness (Number of 1s)')\nplt.grid(True)\nplt.legend()\nplt.show()\n\n# Name of the search algorithm: Hill Climbing\n# number of bits: 10\nTime elapsed: 0.007042 seconds\n\n\n\n\n\n\n\n\n\n\nIn the chart above, the fitness score, which measures the number of 1s in a 10-bit solution, is shown over a series of evaluations. Initially, the fitness rapidly increases, suggesting that the hill climbing algorithm quickly finds improvements in the solution. Once it reaches the maximum fitness of 10, it stabilizes and remains flat, indicating that the algorithm has found an optimal solution and no further improvements are being made.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#helper-functions",
    "href": "hc.html#helper-functions",
    "title": "Hill Climbing",
    "section": "Helper Functions:",
    "text": "Helper Functions:\n\nInitiate: Set Initial Solution: Randomly choose an initial binary solution of the specified bit length (num_bits).\n\n\n# Function for initialization (I)\ndef init_hc(num_bits):\n    return np.random.randint(0, 2, size=num_bits)\n\n\nTransit (Generate Neighbors): Create a function that generates all neighbors of a binary solution by flipping each bit one by one.\nA random bit in the current solution is flipped, representing a small change or ‚Äúmove.‚Äú\n\n\n# Function for transit: (T) \n# Flipping a random bit in the solution\n\ndef transit(sol):\n     new_sol = sol.copy() # A copy of the current solution (sol) is created\n     flip_index = np.random.randint(len(sol)) # A random index in the solution is chosen. \n     new_sol[flip_index] = 1 - new_sol[flip_index]# Flip the bit at the index (0 to 1, or 1 to 0)\n     return new_sol\n\n\nEvaluate: Count the number of 1s in the binary representation of a solution (fitness value for One Max).\nA higher number of 1s means a better solution.\n\n\n# Function for evaluation: (E) \n# Sum the number of 1s (OneMax problem)\ndef evaluate(sol):\n    return np.sum(sol)\n\n\nDetermine: Track and update the current best solution and its fitness as the search progresses.\nIf the new solution (tmp_fitness) is better (i.e., has a higher fitness than the current solution, (fitness)), we accept it. Otherwise, we keep the current solution.\nIf the new solution (tmp_fitness) is worse or equal to the current fitness, we keep the current one.\n\n\n# Function for determine: (D) \n# Decide whether to accept the new solution\ndef determine(tmp_sol, tmp_fitness, sol, fitness):\n    if tmp_fitness &gt; fitness:\n        return tmp_sol, tmp_fitness  # Accept the new solution\n    return sol, fitness  # Keep the current solution",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#comparison-of-es-and-hc-for-the-one-max-problem-of-size-n10",
    "href": "hc.html#comparison-of-es-and-hc-for-the-one-max-problem-of-size-n10",
    "title": "Hill Climbing",
    "section": "Comparison of ES and HC for the one-max Problem of size \\(n=10\\)",
    "text": "Comparison of ES and HC for the one-max Problem of size \\(n=10\\)\n\nHC-LR and HC-Rand to denote a similar thing. HC-Rand and HC-LR differ primarily in how they choose the next solution and handle exploration, impacting their likelihood of getting stuck in local optima.\nHC-LR (Hill Climbing with Limited Range): HC-LR‚Äôs transition operator restricts it to move only to adjacent solutions, which makes it less flexible. The solution v of HC-LR will be the one that is one smaller or one larger than the current solution s (i.e., \\(v=s‚àí1\\) or \\(v=s+1\\)). As described in the example, if HC-LR starts with the solution ‚Äú1000,‚Äù it can only move to ‚Äú0111‚Äù or ‚Äú1001.‚Äù If neither of these options provides an improvement, HC-LR is likely to get stuck there, leading to a local optimum. Because HC-LR only accepts moves that improve the solution, it has a high risk of getting trapped in suboptimal points without exploring further.\nHC-Rand (Hill Climbing with Randomization): HC-Rand randomly selects a part of the current solution to invert, allowing it to explore a broader range of possible next solutions. This random approach reduces the chance of getting stuck in a local optimum since the algorithm has the flexibility to try different options, even if they are not immediately better. HC-Rand‚Äôs randomness increases the probability of escaping local optima, thus improving the chances of finding a global optimum.\n\nThe solution \\(v\\) of HC-Rand will be created by inverting a randomly chosen subsolution of s (i.e., ‚Äú1‚Äù becomes ‚Äú0‚Äù and ‚Äú0‚Äù becomes ‚Äú1‚Äù). The transition operator in the HC-Rand (Hill Climbing with Randomization) algorithm randomly selects a part (subsolution) of the current solution and inverts the chosen bit(s). For a ‚Äúone-max problem‚Äù of a specific size (where the goal is to maximize the number of 1s in the solution), inverting different bits can produce several possible next solutions.\nIn the example given, there are four possible new solutions because there are four different bits that could be inverted from the current solution. If inverting one bit doesn‚Äôt improve the objective value (e.g., the number of 1s), HC-Rand will consider other options, allowing it to avoid getting stuck in local optima. This random inversion helps HC-Rand explore other solutions, increasing the chances of finding the global maximum (an optimal solution with the highest number of 1s).\n\nIn both cases, only a better solution will be accepted as the next solution \\(v\\).\n\n\n\n\nhc search",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#deciphering-probability-of-switching",
    "href": "hc.html#deciphering-probability-of-switching",
    "title": "Hill Climbing",
    "section": "Deciphering Probability of Switching",
    "text": "Deciphering Probability of Switching\n\nThe landscape of the ‚Äúsolution space‚Äù or ‚Äúsearch space‚Äù of an optimization problem seen by a search can be different when:\n\ndifferent ways are used to represent the solutions or\ndifferent ways are used to generate new candidate solutions from current solutions.\n\nThe solution space that a search algorithm sees can also be called the ‚Äúsearch space.‚Äù\nHC-Rand-M: If we use ‚Äú&gt;=‚Äù instead of ‚Äú&gt;‚Äù in the comparison of the current solution and the possible next solution for solving the BSD-2 problem of sizes n=4 and n=10.\nThe results of HC-Rand-M show that with this modification, HC-Rand will be able to find the optimal solution in most cases.\n\n ## Comparison of ES and HC for the one-max Problem of size \\(n=10\\)\n\nHC-LR and HC-Rand to denote a similar thing. HC-Rand and HC-LR differ primarily in how they choose the next solution and handle exploration, impacting their likelihood of getting stuck in local optima.\nHC-LR (Hill Climbing with Limited Range): HC-LR‚Äôs transition operator restricts it to move only to adjacent solutions, which makes it less flexible. The solution v of HC-LR will be the one that is one smaller or one larger than the current solution s (i.e., \\(v=s‚àí1\\) or \\(v=s+1\\)). As described in the example, if HC-LR starts with the solution ‚Äú1000,‚Äù it can only move to ‚Äú0111‚Äù or ‚Äú1001.‚Äù If neither of these options provides an improvement, HC-LR is likely to get stuck there, leading to a local optimum. Because HC-LR only accepts moves that improve the solution, it has a high risk of getting trapped in suboptimal points without exploring further.\nHC-Rand (Hill Climbing with Randomization): HC-Rand randomly selects a part of the current solution to invert, allowing it to explore a broader range of possible next solutions. This random approach reduces the chance of getting stuck in a local optimum since the algorithm has the flexibility to try different options, even if they are not immediately better. HC-Rand‚Äôs randomness increases the probability of escaping local optima, thus improving the chances of finding a global optimum.\n\nThe solution \\(v\\) of HC-Rand will be created by inverting a randomly chosen subsolution of s (i.e., ‚Äú1‚Äù becomes ‚Äú0‚Äù and ‚Äú0‚Äù becomes ‚Äú1‚Äù). The transition operator in the HC-Rand (Hill Climbing with Randomization) algorithm randomly selects a part (subsolution) of the current solution and inverts the chosen bit(s). For a ‚Äúone-max problem‚Äù of a specific size (where the goal is to maximize the number of 1s in the solution), inverting different bits can produce several possible next solutions.\nIn the example given, there are four possible new solutions because there are four different bits that could be inverted from the current solution. If inverting one bit doesn‚Äôt improve the objective value (e.g., the number of 1s), HC-Rand will consider other options, allowing it to avoid getting stuck in local optima. This random inversion helps HC-Rand explore other solutions, increasing the chances of finding the global maximum (an optimal solution with the highest number of 1s).\n\nIn both cases, only a better solution will be accepted as the next solution \\(v\\).\n\n\n\n\nhc search",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#load-imports",
    "href": "hc.html#load-imports",
    "title": "Hill Climbing",
    "section": "Load Imports",
    "text": "Load Imports\n\nimport numpy as np\nimport time\nimport random\nimport matplotlib.pyplot as plt\n\nnp.random.seed(5042) ##for consistency",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#define-the-ackley-function-1d",
    "href": "hc.html#define-the-ackley-function-1d",
    "title": "Hill Climbing",
    "section": "Define the Ackley Function (1D)",
    "text": "Define the Ackley Function (1D)\n\nFunction f(x): This is the function we want to optimize.\nIn this case, Ackley function with provided formula.\n\n\n# Ackley function in 1D\ndef ackley(x):\n    a = 20\n    b = 0.2\n    c = 2 * np.pi\n    term1 = -a * np.exp(-b * np.sqrt(np.mean(np.square(x))))\n    term2 = -np.exp(np.mean(np.cos(c * np.array(x))))\n    return term1 + term2 + a + np.exp(1)",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#main-loop",
    "href": "hc.html#main-loop",
    "title": "Hill Climbing",
    "section": "Main Loop",
    "text": "Main Loop\n\nThis is where the hill climbing algorithm repeatedly evaluates candidate solutions, compares them with the current solution, and decides whether to update the current solution. This loop is crucial because it drives the optimization process by iterating over a fixed number of steps or until a stopping criterion is met.\nThe current solution is set to start_x, which was initialized earlier. The Ackley value for this starting solution is calculated and stored in current_value.x_history and value_history keep track of all the solutions and function values (fitness) encountered during the optimization process.\nThe hill climbing loop runs for a fixed number of iterations (max_iters), which controls how many optimization steps are performed. Each iteration represents one optimization step, where two new candidate solutions are generated and evaluated.\nThe transit function generates two neighboring solutions, left_x and right_x, by subtracting and adding the step_size (0.1 by default) to the current solution (current_x). These represent potential moves to the left and right in the search space.\nBoth neighboring solutions are evaluated using the Ackley function, resulting in their corresponding values (left_value and right_value). These values represent how ‚Äúgood‚Äù the new solutions are (lower is better in this minimization problem). Then we compare all three solutions (current, left, and right).\n\nThe np.argmin(values) function finds the index of the minimum value (best solution) from the three candidates.\nThe determine function then decides whether to update the current solution (current_x) to the new best solution.\n\nThe current solution and its corresponding function value are appended to the history lists (x_history and value_history). These lists will be used later to plot the progress of the algorithm.\nFinally, we return the results.\n\n\n# Main hill climbing loop for Ackley function in 1D\ndef hc_ackley(start_x, max_iters=100, step_size=0.1):\n    current_x = start_x\n    current_value = evaluate(current_x)\n    x_history = [current_x]\n    value_history = [current_value]\n\n    for _ in range(max_iters):\n        left_x, right_x = transit(current_x, step_size)\n        left_value = evaluate(left_x)\n        right_value = evaluate(right_x)\n\n        values = [current_value, left_value, right_value]\n        x_candidates = [current_x, left_x, right_x]\n\n        best_index = np.argmin(values)\n        current_x, current_value = determine(values[best_index], current_value, x_candidates[best_index], current_x)\n\n        x_history.append(current_x)\n        value_history.append(current_value)\n\n    return current_x, current_value, x_history, value_history",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#helper-functions-1",
    "href": "hc.html#helper-functions-1",
    "title": "Hill Climbing",
    "section": "Helper Functions",
    "text": "Helper Functions\n\nInitialize: This function initializes the hill climbing process by selecting a random starting point (start_x) within the given range [-10, 10]. This represents the initial solution that hill climbing will start with.\n\n\n# Initialization function (I) to set the starting point\ndef init_hc(range_min=-10, range_max=10):\n    return random.uniform(range_min, range_max)\n\n\nTransition: This function generates two new candidate solutions by taking small steps (of size 0.1) to the left and right of the current solution (current_x). These represent possible transitions to neighboring points in the search space.\n\n\n# Transition function (T)\ndef transit(current_x, step_size=0.1):\n    return current_x - step_size, current_x + step_size\n\n\nEvaluation: This function evaluates the fitness of a solution (sol) using the Ackley function. The goal of the hill climbing algorithm is to minimize this value. The Ackley function is typically used for testing optimization algorithms due to its complex landscape with many local minima.\n\n\n# Evaluation function (E) to evaluate fitness (Ackley value in this case)\ndef evaluate(sol):\n    return ackley([sol])\n\n\nDetermination: This function compares the current solution‚Äôs value (current_value) with the value of a new candidate solution (new_value). If the new solution is better (i.e., has a lower Ackley value), it becomes the new current solution. Otherwise, the current solution is retained.\n\n\n# Determination (D)\n# Function to decide whether to accept the new solution\ndef determine(new_value, current_value, new_x, current_x):\n    return (new_x, new_value) if new_value &lt; current_value else (current_x, current_value)",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#main-execution",
    "href": "hc.html#main-execution",
    "title": "Hill Climbing",
    "section": "Main Execution",
    "text": "Main Execution\n\nThe process starts by initializing start_x using the Initialization function (init_hc). The main function (hc_ackley) then runs for a fixed number of iterations (100 by default) using Transition to generate candidate solutions, Evaluation to evaluate them, and Determination to decide whetherr to accept a new solution.\n\n\n# Main execution\nstart_x = init_hc()\nstart_time = time.time()\noptimal_x, optimal_value, x_history, value_history = hc_ackley(start_x)\nend_time = time.time()\nexecution_time = end_time - start_time",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#output",
    "href": "hc.html#output",
    "title": "Hill Climbing",
    "section": "Output",
    "text": "Output\n\nThe first part prints the optimal solution found (optimal_x), the corresponding function value (optimal_value), and the total time taken for the execution.\nThe second part plots the Ackley function over the range [-10, 10] and overlays the hill climbing progress. The points in red represent the steps taken by the hill climbing algorithm as it navigates through the search space.\n\n\n# Output (O)\nprint(f\"Optimal x: {optimal_x}\")\nprint(f\"Optimal value: {optimal_value}\")\nprint(f\"Execution time: {execution_time:.6f} seconds\")\n\n\n# Plot the Ackley function and hill climbing progress\nx_range = np.linspace(-10, 10, 1000)\ny_values = [ackley([x]) for x in x_range]\n\nplt.plot(x_range, y_values, label=\"Ackley Function\")\nplt.plot(x_history, value_history, 'ro--', label=\"Hill Climbing Progress\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.title(\"Hill Climbing on Ackley Function in 1D\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nOptimal x: 8.981200099425264\nOptimal value: 16.70044477517878\nExecution time: 0.003003 seconds\n\n\n\n\n\n\n\n\n\n\nAn example run of this put the points Optimal x: 5.03 and Optimal Value: 12.75, are suboptimal for the Ackley function.\nFor the Ackley function in 1D, the global minimum occurs at \\(x=0\\), and the function value at this point is exactly \\(f(0)=0\\).\nOur results are not close to the global minimum of \\(x=0\\). Any value greater than zero indicates that the algorithm has not found the true global minimum.\nThe blue line represents the Ackley function over a range of \\(x\\) values from -10 to 10.The red dots connected by dashed lines represent the progress of the hill climbing algorithm as it iterates through neighboring solutions.\nThe Ackley function is known for its many local minima, and hill climbing is a greedy optimization method that tends to get stuck in local minima because it only moves to the immediate best neighboring solution. Once it finds a local minimum, it stops, even if it hasn‚Äôt found the global minimum.\nIn this case, the result you obtained is a local minimum but not the global minimum. Since \\(f(0)=0\\) is the global minimum, and your result is \\(f(5.03)= 12.75\\), the algorithm likely got stuck in a local minimum during its search.\nSimulated Annealing is a more advanced version of hill climbing that sometimes accepts worse solutions to escape local optima. Will discuss later.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#imports",
    "href": "hc.html#imports",
    "title": "Hill Climbing",
    "section": "Imports",
    "text": "Imports\n\n## Hill Climbing Finance Example Using yfinance\nimport yfinance as yf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport time",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#fetching-data",
    "href": "hc.html#fetching-data",
    "title": "Hill Climbing",
    "section": "Fetching Data",
    "text": "Fetching Data\n\n# Selecting a few popular stocks with start date and end date\nstocks = ['AAPL', 'GOOGL', 'MSFT', 'AMZN', 'TSLA', 'NFLX', 'NVDA', 'META', 'DIS', 'BA'] \nstart_date = '2023-10-01'\nend_date = '2024-10-1'\n\n# Fetch historical stock data\ndef fetch_data(stocks, start_date, end_date):\n    return yf.download(stocks, start=start_date, end=end_date, auto_adjust=True)\nstock_data = fetch_data(stocks, start_date, end_date)\n\n[                       0%                       ][                       0%                       ][                       0%                       ][                       0%                       ][                       0%                       ][**********************60%****                   ]  6 of 10 completed[**********************70%*********              ]  7 of 10 completed[**********************70%*********              ]  7 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\n\n10 Failed downloads:\n['NFLX', 'META', 'GOOGL', 'DIS', 'AMZN', 'AAPL', 'TSLA', 'NVDA', 'MSFT', 'BA']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#calculating-performance-via-sharpe-ratio",
    "href": "hc.html#calculating-performance-via-sharpe-ratio",
    "title": "Hill Climbing",
    "section": "Calculating performance via Sharpe Ratio",
    "text": "Calculating performance via Sharpe Ratio\n\ndef portfolio_performance(weights, mean_returns, cov_matrix):\n    returns = np.sum(mean_returns * weights) * 252  # Annualized returns\n    risk = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights))) * np.sqrt(252)  # Annualized risk\n    sharpe_ratio = returns / risk  # Sharpe ratio\n    return returns, risk, sharpe_ratio",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#main-loop-1",
    "href": "hc.html#main-loop-1",
    "title": "Hill Climbing",
    "section": "Main Loop",
    "text": "Main Loop\n\ndef hc(mean_returns, cov_matrix, max_iterations=1000):\n    num_assets = len(mean_returns)\n    # Initial random portfolio\n    current_weights = np.random.random(num_assets)\n    current_weights /= np.sum(current_weights)  \n    _, _, current_sharpe = portfolio_performance(current_weights, mean_returns, cov_matrix)\n    \n    # Track progress for visualization\n    score_history = [current_sharpe]\n    iteration = 0\n\n    # Hill Climbing Loop\n    while iteration &lt; max_iterations:\n        # Generate neighbor: Slight random change in weights\n        neighbor_weights = current_weights + np.random.normal(0, 0.01, num_assets)\n        neighbor_weights = np.clip(neighbor_weights, 0, 1)  # Ensure weights are between 0 and 1\n        neighbor_weights /= np.sum(neighbor_weights)  # Normalize weights to sum to 1\n        \n        neighbor_returns, neighbor_risk, neighbor_sharpe = portfolio_performance(neighbor_weights, mean_returns, cov_matrix)\n        \n        # If the neighbor is better, move to the neighbor solution\n        if neighbor_sharpe &gt; current_sharpe:\n            current_weights, _, _, current_sharpe = neighbor_weights, neighbor_returns, neighbor_risk, neighbor_sharpe\n            score_history.append(current_sharpe)\n        \n        iteration += 1  \n    \n    return current_weights, current_sharpe, score_history\n\nstock_data = fetch_data(stocks, start_date, end_date)\nreturns = stock_data['Close'].pct_change().dropna()\nmean_returns = returns.mean()\ncov_matrix = returns.cov()\n\n[                       0%                       ][                       0%                       ][                       0%                       ][                       0%                       ][**********************50%                       ]  5 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************50%                       ]  5 of 10 completed[**********************80%*************          ]  8 of 10 completed[**********************90%******************     ]  9 of 10 completed[*********************100%***********************]  10 of 10 completed\n\n10 Failed downloads:\n['TSLA', 'GOOGL', 'NVDA', 'AAPL', 'BA', 'AMZN', 'NFLX', 'META', 'DIS', 'MSFT']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\nc:\\Users\\pamel\\Documents\\BUAD5042-Heuristics\\Project\\.conda\\Lib\\site-packages\\numpy\\lib\\function_base.py:520: RuntimeWarning:\n\nMean of empty slice.\n\nc:\\Users\\pamel\\Documents\\BUAD5042-Heuristics\\Project\\.conda\\Lib\\site-packages\\numpy\\core\\_methods.py:121: RuntimeWarning:\n\ninvalid value encountered in divide\n\nc:\\Users\\pamel\\Documents\\BUAD5042-Heuristics\\Project\\.conda\\Lib\\site-packages\\pandas\\core\\frame.py:11211: RuntimeWarning:\n\nDegrees of freedom &lt;= 0 for slice\n\nc:\\Users\\pamel\\Documents\\BUAD5042-Heuristics\\Project\\.conda\\Lib\\site-packages\\numpy\\lib\\function_base.py:2748: RuntimeWarning:\n\ndivide by zero encountered in divide\n\nc:\\Users\\pamel\\Documents\\BUAD5042-Heuristics\\Project\\.conda\\Lib\\site-packages\\numpy\\lib\\function_base.py:2748: RuntimeWarning:\n\ninvalid value encountered in multiply",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#main-execution-1",
    "href": "hc.html#main-execution-1",
    "title": "Hill Climbing",
    "section": "Main Execution",
    "text": "Main Execution\n\nstart_time = time.time()\n# Run hill climbing\nbest_weights, best_sharpe, score_history = hc(mean_returns, cov_matrix)\nend_time = time.time()\nexecution_time = end_time - start_time  # Calculate elapsed time",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#output-1",
    "href": "hc.html#output-1",
    "title": "Hill Climbing",
    "section": "Output",
    "text": "Output\n\n# Output the best results\nprint(\"Optimized Portfolio using Hill Climbing:\")\nfor i, stock in enumerate(stocks):\n    print(f\"{stock}: {best_weights[i]:.4f}\")\nprint(f\"Best Sharpe Ratio: {best_sharpe:.4f}\")\nprint(f\"Execution time: {execution_time:.6f} seconds\")\n\n# Plot the progress of the hill climbing algorithm (Sharpe ratio improvement)\nplt.figure(figsize=(10, 6))\nplt.plot(score_history, marker='o', linestyle='-', color='b', label='Sharpe Ratio Progress')\nplt.title(\"Hill Climbing Progress for Portfolio Optimization (Sharpe Ratio)\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Sharpe Ratio\")\nplt.grid(True)\nplt.legend()\nplt.show()\n\nOptimized Portfolio using Hill Climbing:\nAAPL: 0.1328\nGOOGL: 0.1028\nMSFT: 0.1192\nAMZN: 0.1132\nTSLA: 0.0975\nNFLX: 0.0664\nNVDA: 0.0533\nMETA: 0.0360\nDIS: 0.1582\nBA: 0.1205\nBest Sharpe Ratio: nan\nExecution time: 0.080156 seconds\n\n\n\n\n\n\n\n\n\n\nThe plot visualizes how the Sharpe ratio evolves over the iterations. This helps understand the performance of the hill climbing algorithm.\nThe weights are heavily concentrated in a few stocks, particularly: AAPL (Apple), AMZN (Amazon), NFLX (Netflix), META (Facebook), and DIS (Walt Disney Co).\nA Sharpe ratio of 2.800 is generally considered very good. A Sharpe ratio above 1 is usually considered acceptable, above 2 is very good, and above 3 is excellent, indicating that your portfolio offers a high return per unit of risk.\nOur portfolio is highly concentrated in just 5 out of the 10 assets. The algorithm has effectively zeroed out the remaining assets, which can either be a good or bad thing depending on the actual data.\n\nPros: Concentration might mean that the algorithm has found a combination of assets that offer the best Sharpe ratio, meaning it is maximizing returns while minimizing risk.\nCons: Concentrating in a few assets can increase the overall risk due to lack of diversification.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "hc.html#items-to-consider",
    "href": "hc.html#items-to-consider",
    "title": "Hill Climbing",
    "section": "Items To Consider",
    "text": "Items To Consider\n\nA Sharpe ratio of 2.800 is very good, suggesting that the portfolio has a favorable balance of return vs.¬†risk. This ratio indicates that for each unit of risk, your portfolio earns more than two times the return.\nWhether this Sharpe ratio is truly impressive depends on the quality and volatility of the returns in the dataset. Financial markets rarely produce consistently high Sharpe ratios, so the high value might indicate the presence of some anomaly, or it could be due to a limited timeframe or low volatility in the assets during this period.\nGiven the high concentration in a few stocks, there‚Äôs a risk that the algorithm might be overfitting to the specific data in the training period. This means the portfolio might perform very well during the period used for the analysis but might not generalize well to future periods.\nThings to Try Next:\n\nBacktesting: Test this portfolio over a different time period or a longer time horizon to see if it maintains its high Sharpe ratio.\nRebalancing: Consider rebalancing the portfolio periodically (monthly or quarterly) to see if the high Sharpe ratio persists.\nDiversification: If you are concerned about high concentration risk, you might want to introduce constraints to the hill climbing algorithm to enforce a minimum weight for diversification or limit the maximum weight of any asset.",
    "crumbs": [
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Hill Climbing</span>"
    ]
  },
  {
    "objectID": "sa.html",
    "href": "sa.html",
    "title": "Simulated Annealing",
    "section": "",
    "text": "What is a Metaheuristic Algorithm?",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#history-of-metaheuristics",
    "href": "sa.html#history-of-metaheuristics",
    "title": "Simulated Annealing",
    "section": "History of Metaheuristics",
    "text": "History of Metaheuristics\n\nAlthough the term ‚Äúmetaheuristic‚Äù was introduced in the 1980s, several metaheuristic algorithms were actually presented in the 1960s or even earlier.\nThe year 1990 can be regarded as the first watershed in the development of metaheuristic algorithms. However, the available computing power may not satisfy the requirements of some complicated metaheuristic algorithms.\nSince the late 1990s, the number of metaheuristic algorithms has exploded. More and more studies use ‚Äúthe number of evaluations‚Äù to replace ‚Äúthe number of iterations‚Äù for evaluating the performance of a metaheuristic algorithm because each search can be regarded as an investment of computation resource.\n\nUsing the number of evaluations provides a way that is more precise than using the number of iterations to evaluate the effect (improvement or outcome) of adding an additional unit of computation resource for the search\n\nFrom the year 2010 or even earlier, some groups have attempted to apply metaheuristic algorithms in high-performance computing environments.\n\nUsing distributed or parallel computing systems to accelerate the response time of metaheuristic algorithms is an intuitive approach adopted in some early studies.\nSome of the parallel metaheuristics are not only able to provide the end results to the user more quickly; they also can find better results than metaheuristic algorithms on a single machine because the parallel computing mechanism leads them to increase the search diversity during the convergence process.\nWhen we look at these distributed and parallel computing environments, the cloud computing platform (e.g., Hadoop, Spark, Microsoft Azure, Amazon EC2, or Google Compute Engine) can now provide an easy way to use a distributed computing system to further reduce the response time of metaheuristics.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#a-unified-framework-for-metaheuristic-algorithms-ufm",
    "href": "sa.html#a-unified-framework-for-metaheuristic-algorithms-ufm",
    "title": "Simulated Annealing",
    "section": "A Unified Framework for Metaheuristic Algorithms (UFM)",
    "text": "A Unified Framework for Metaheuristic Algorithms (UFM)\n\nThe appearance of metaheuristic algorithms has come with methods to classify them. These classification methods include:\n\n\nnature-inspired vs.¬†non-nature inspired,\ndynamic vs.¬†static objective function,\none vs.¬†various neighborhood structures,\nmemory usage vs.¬†memoryless methods,\nwith vs.¬†without local search method, and\npopulation-based vs.¬†single-solution-based search.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#ufm-5-main-operators",
    "href": "sa.html#ufm-5-main-operators",
    "title": "Simulated Annealing",
    "section": "UFM 5 Main Operators",
    "text": "UFM 5 Main Operators\n\nInitialization (I): The initialization operator normally plays the roles of reading the input file (e.g., dataset), initializing all the parameters of a metaheuristic algorithm, and determining the initial solutions, which is normally based on a random process.\nTransition (T): The transition operator usually plays the role of varying the search directions, such as perturbing a portion of the subsolutions of the current solution to generate a new candidate solution or generating a set of new candidate solutions each based on two or more of the current solutions.\nEvaluation (E): The evaluation operator is responsible for measuring the quality of solutions, such as calculating the objective value of each solution to be used by the determination operator to distinguish the quality of all the solutions. An intuitive way is to use an objective function to measure the quality of a solution for the problem in question. However, some metaheuristics do not use the ‚Äúobjective value‚Äù directly to measure their solutions; rather, the objective value of a solution has to undergo some sort of transformation to obtain the so-called ‚Äúfitness value.‚Äù\nDetermination (D): The determination operator plays the role of deciding the search directions by using information the evaluation operator provides during the convergence process. The performance of a metaheuristic algorithm depends to a large extent on the performance of this operator. A ‚Äúgood‚Äù search strategy for this operator will make it possible for the metaheuristic algorithm to find a better solution faster or to avoid falling into a local optimum at early iterations.\nOutput (O): In spite of the fact that this operator seems to be trivial, the reality is that it can be either simple or complex depending on how much information we want to display for the metaheuristic algorithm. It can be as simple as displaying only the final result of the metaheuristic algorithm, or it can be as complex as displaying the trajectory of convergence of the metaheuristic algorithm to better understand the performance of a metaheuristic algorithm.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#ufm",
    "href": "sa.html#ufm",
    "title": "Simulated Annealing",
    "section": "UFM",
    "text": "UFM\n\nIn this framework, I denotes the input dataset, s denotes the current solution, \\(\\nu\\) denotes the candidate solution, \\(f_s\\) denotes the objective value of \\(s\\), and \\(f_\\nu\\) denotes the objective value of \\(\\nu\\). Also, \\(s\\) and \\(\\nu\\) can denote either a single solution or a set of solutions, where each solution has \\(n\\) elements or is an \\(n-tuple\\).\n\n\n\n\nUFM",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#comparison-between-exhaustive-search-greedy-and-metaheuristic-algorithms",
    "href": "sa.html#comparison-between-exhaustive-search-greedy-and-metaheuristic-algorithms",
    "title": "Simulated Annealing",
    "section": "Comparison between exhaustive search, greedy, and metaheuristic algorithms",
    "text": "Comparison between exhaustive search, greedy, and metaheuristic algorithms\n\n\n\nCompare Models",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#key-components",
    "href": "sa.html#key-components",
    "title": "Simulated Annealing",
    "section": "Key Components",
    "text": "Key Components\n\nSolution Space:\n\nThis is the space of all possible solutions to the problem.\n\nObjective Function:\n\nDefines the quality and overall goal of a solution.\n\nTemperature:\n\nThis controls the probability of accepting worse solutions.\nInitially, the temperature is high, allowing the algorithm to explore the solution space more freely.\nAs the temperature lowers, the algorithm becomes more conservative, accepting only smaller degradations in the objective.\n\nCooling Schedule:\n\nThis is a function that dictates how the temperature decreases over time (iterations).\nTypically, it follows a geometric decay, where the temperature decreases by a factor on each iteration (e.g., \\(T = T_0 * \\alpha^k\\), where \\(T_0\\) is the initial temperature, alpha is a constant, and \\(k\\) is the iteration number).",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#defining-sa",
    "href": "sa.html#defining-sa",
    "title": "Simulated Annealing",
    "section": "Defining SA",
    "text": "Defining SA\n\nAn iterative algorithm that explores the solution space of an optimization problem by considering not only improvements to the current solution but also occasional, controlled acceptance of worse solutions. This allows the algorithm to escape local minima and explore a broader range of the search space in search of a global minimum.\n\nThe function \\(f(x)\\) that the algorithm seeks to minimize (or maximize)\nThe configuration or state is a point \\(x\\) in the solution space, representing a possible solution to the problem.\nThe neighboring states are the set of solutions that are reachable from the current state through small modifications.\nA control parameter temperature \\(T\\)) that regulates the likelihood of accepting worse solutions. It starts high and gradually decreases as the algorithm progresses.\nAs \\(T\\) decreases, the probability of accepting worse solutions decreases, making the search more focused on local improvements.\nA cooling schedule includes a function that controls the decrease of the temperature T over time",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#usefulness-of-sa",
    "href": "sa.html#usefulness-of-sa",
    "title": "Simulated Annealing",
    "section": "Usefulness of SA",
    "text": "Usefulness of SA\n\nThe basic idea of SA is to occasionally accept non-improving solutions, which means that SA will not always move to a better solution.\nSimulated Annealing is widely used in various fields, such as:\n\nCombinatorial Optimization: Problems like the Traveling Salesman Problem (TSP), scheduling, and circuit design.\nMachine Learning: Hyperparameter optimization, clustering.\nEngineering: Structural design, control systems.\n\nPractical Applications: real-world problems where SA shines: vehicle routing, job scheduling, or portfolio optimization.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#tuning-parameters",
    "href": "sa.html#tuning-parameters",
    "title": "Simulated Annealing",
    "section": "Tuning Parameters",
    "text": "Tuning Parameters\n\nThe success of SA heavily depends on the tuning of parameters like the cooling schedule, the initial temperature, and the size of the neighborhood.\nIn the context of tuning parameters for Simulated Annealing (SA), the exploration-exploitation trade-off means adjusting the algorithm‚Äôs behavior to balance between searching broadly (exploration) and refining promising solutions (exploitation).",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#the-search-strategy-of-sa",
    "href": "sa.html#the-search-strategy-of-sa",
    "title": "Simulated Annealing",
    "section": "The Search Strategy of SA",
    "text": "The Search Strategy of SA\n\nThe search strategy of SA is to start with a random possible solution in the solution space and then use a criteria (like the metropolis acceptance criterion) to determine whether a worse solution is to be accepted or not.\nIn this example, \\(t^{\\Delta} \\quad\\) and \\(\\quad t^{\\nabla}\\) indicate that the solution at the \\(t+1^{\\text{th}}\\) iteration is either better or not better than the solution at the \\(t^{\\text{th}}\\) iteration, respectively.\nThis example shows that if the starting point is ùë•_9 and SA accepts only a new candidate solution that is better than the current solution, i.e., it has no escape mechanism, the search will get stuck at one of the two local optima (\\(x_8\\) and \\(x_10\\)) denoted \\(L_1\\) and \\(L_2\\).\nThe temperature parameter in SA allows escape from local optima. If the search starts at \\(x_9\\), SA can potentially move to worse solutions with a certain probability based on temperature and a random probability check. At higher temperatures, SA has a higher chance of escaping local optima (\\(L_1\\)) (\\(x_8\\)) and (\\(L_2\\)) (\\(x_{10}\\)), but as the temperature decreases, it becomes less likely to accept worse moves, leading to convergence.\n\n\n\n\nSearch Strategy of SA",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#sa-algorithm",
    "href": "sa.html#sa-algorithm",
    "title": "Simulated Annealing",
    "section": "SA Algorithm",
    "text": "SA Algorithm\n\nInitialization:\n\nChoose an initial solution \\(s\\) and an initial temperature \\(T_0\\).\n\nIteration:\n\nFor each step, generate a neighboring solution \\(v\\) of the current solution \\(s\\).\nCompute the change in the objective function.\nDecide whether to move to the new solution \\(v\\) based on the acceptance probability.\nGradually reduce the temperature \\(T\\) according to the cooling schedule\n\nTermination:\n\nThe algorithm stops when the temperature \\(T\\) is sufficiently low or after a predefined number of iterations.\nThe best solution found during the process is returned.\n\n\n\n\n\nSA Algorithm",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#many-versions-of-sa-exist",
    "href": "sa.html#many-versions-of-sa-exist",
    "title": "Simulated Annealing",
    "section": "Many Versions of SA Exist",
    "text": "Many Versions of SA Exist\n\nThis framework is a little bit different from the model above because there are two loops (outer and inner loops). This implies that a certain number of new candidate solutions will be generated and evaluated before the temperature is updated.\n\n\n\n\nSA Algorithm Variation",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#advantages-and-disadvantages-of-sa",
    "href": "sa.html#advantages-and-disadvantages-of-sa",
    "title": "Simulated Annealing",
    "section": "Advantages and Disadvantages of SA",
    "text": "Advantages and Disadvantages of SA\n\nAdvantages\n\nGlobal Search Capability: Unlike simple greedy algorithms, simulated annealing can escape local minima and potentially find a global minimum.\nFlexibility: It can be applied to a wide range of optimization problems, including those with complex, multimodal landscapes.\nSimplicity: The algorithm is relatively easy to implement and does not require gradient information, making it suitable for non-differentiable problems.\n\n\n\nDisadvantages\n\nComputational Cost: The method can be slow, particularly for large problem spaces, as it requires many iterations to reach a good solution.\nParameter Sensitivity: The performance of simulated annealing depends heavily on the choice of the cooling schedule, initial temperature, and other parameters.\nNo Guarantee of Optimality: The algorithm does not guarantee finding the global optimum but rather a good approximation.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#examples-of-how-value-can-change",
    "href": "sa.html#examples-of-how-value-can-change",
    "title": "Simulated Annealing",
    "section": "Examples of How Value Can Change",
    "text": "Examples of How Value Can Change\n * Minimization: + If \\(\\Delta_f^{\\text{min}}\\) is negative, we always accept (better solution). + If \\(\\Delta_f^{\\text{min}}\\) is positive, accept with probability \\(paminp_a^{\\text{min}}pamin\\).\n\nMaximization:\n\nIf \\(\\Delta_f^{\\text{max}}\\) is positive, we always accept (better solution).\nIf \\(\\Delta_f^{\\text{max}}\\) is negative, accept with probability \\(pamaxp_a^{\\text{max}}\\).\n\nIn this case, \\(p_a^{\\text{max}}\\)=.329 and 0.007 for \\({\\Psi_t}\\) = .9 and 0.2, respectively. This indicates that a lower temperature Image implies a smaller probability Image to accept a non-improving candidate solution as the current solution.\nThe value of \\(f_s\\) goes from 2.0 to 1.1, the value of \\(p_a^{\\text{max}}\\) goes from 0.329 up to 0.895. This means that in case \\(f(\\nu)\\) is worse than \\(f(s)\\), a smaller \\(\\Delta_f^{\\text{max}}\\) implies a higher probability \\(p_a^{\\text{max}}\\) to accept a non-improving candidate solution as the current solution.\nHigher temperatures} (\\(\\Psi_t = 0.9\\)) allow higher acceptance of worse solutions.\n\nExample: For \\(\\Delta_f^{\\min} = -1.0\\), at \\(\\Psi_t = 0.9\\), \\(p_a^{\\min} = 3.038\\)(high).\n\nLower temperatures \\(\\Psi_t = 0.2\\) reject worse solutions more aggressively.\n\nExample: For \\(\\Delta_f^{\\min} = -1.0\\), at \\(\\Psi_t = 0.2\\), \\(p_a^{\\min} = 148.413\\), which results in almost no acceptance.\n\nAcceptance of better solutions} \\(p_a^{\\max}\\) is much higher in general but varies based on temperature.\n\nFor \\(\\Psi_t = 0.9\\), \\(p_a^{\\max} = 0.329\\) (highlighted in yellow).\nFor \\(\\Psi_t = 0.2\\), \\(p_a^{\\max} = 0.007\\), meaning lower temperatures almost never accept bad moves.\n\n\n\nSummary of Cases\n\nThe new solution v is better than the current solution s.\n\nSA will always accept the new solution owing to the fact that \\(P_A\\) will always be greater than 1.0 and \\(r \\in [0,1]\\) will always be smaller than \\(P_A\\). The new solution \\(\\nu\\) is worse than the current solution \\(s\\). SA will accept the new solution only if \\(r &lt; P_A\\) , where \\(r\\) is as defined above. The new solution \\(\\nu\\) is worse than the current solution s. SA will not accept the new solution because \\(r \\geq P_A\\) .",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#comparing-algorithms",
    "href": "sa.html#comparing-algorithms",
    "title": "Simulated Annealing",
    "section": "Comparing Algorithms",
    "text": "Comparing Algorithms\n\nSimulated Annealing Algorithms\n\nExploration: In the early stages, SA accepts worse solutions with high probability, which allows it to explore the search space more broadly and escape local optima.\nExploitation: As the temperature decreases, the algorithm becomes more conservative and starts focusing on refining the current solution. SA balances exploration and exploitation dynamically as the temperature cools.\nTrade-off: SA is particularly good when the solution space is rugged (many local optima) because it has a built-in mechanism (temperature) to transition from exploration to exploitation.\n\nGenetic Algorithms\n\nExploration: GA uses processes like mutation and crossover to generate new solutions from the current population. Mutation allows for exploration by introducing random changes, while crossover exploits good solutions by combining them.\nExploitation: GA exploits the best solutions through selection and crossover, where solutions with higher fitness are more likely to survive and reproduce.\nTrade-off: GA maintains a population of solutions, which helps with exploration, but can sometimes suffer from premature convergence if the population becomes too homogeneous, leading to poor exploitation of potentially better solutions.\n\nGreedy Algorithms\n\nExploration: Greedy algorithms have very limited exploration. They make the best immediate choice (locally optimal) at each step without considering the broader solution space.\nExploitation: Greedy heuristics are purely exploitative‚Äîthey focus solely on improving the current state as much as possible. They tend to get stuck in local optima because they don‚Äôt explore alternative solutions that might initially look worse but could lead to better outcomes.\nTrade-off: A greedy algorithm doesn‚Äôt balance exploration and exploitation well. It is fast and simple but can fail when the problem has many local optima, making it inappropriate for complex problems like TSP or scheduling.\n\n\n\nWhen SA Is More Appropriate\n\nRugged Solution Spaces: When there are many local optima (like in the Ackley function or complex scheduling), SA‚Äôs ability to accept worse solutions helps it explore more and avoid local traps.\nTime-Constrained Search: SA can be more useful when you don‚Äôt need an exact global optimum but want a good solution within a reasonable amount of time. The cooling schedule can be adjusted to control how fast the algorithm converges.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#imports-global-variables",
    "href": "sa.html#imports-global-variables",
    "title": "Simulated Annealing",
    "section": "Imports, Global Variables",
    "text": "Imports, Global Variables\n\n##Simulated Annealing One-Max Example\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport time\n\n# Define the necessary global variables\nnum_bits = 10\nmax_evals = 1000\ninitial_temp = 10\ncooling_rate = 0.99\nmin_temp = 0.0001",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#initialize",
    "href": "sa.html#initialize",
    "title": "Simulated Annealing",
    "section": "Initialize",
    "text": "Initialize\n\nThe init_sa function initializes a random solution (sol) for the One-Max problem by generating a binary array of size num_bits, where each element is randomly set to 0 or 1.\nThe initial solution is evaluated using the One-Max function (evaluate) to calculate its fitness (i.e., the sum of 1‚Äôs in the array).\nThe function returns the initial solution and its evaluated fitness.\n\n\n# Initialization function (I)\ndef init_sa(num_bits):\n    sol = np.random.randint(0, 2, num_bits)\n    return sol, evaluate(sol)\n\n\nTransition (T)\n\nThe transit function generates a neighboring solution by flipping a random bit in the current solution.\nThis is done by randomly selecting an index in the binary array and toggling the bit (changing 1 to 0 or 0 to 1).\nThis new solution represents the neighboring candidate that will be evaluated next, which is a small random change to the current solution.\n\n\n# Transition function (T)\ndef transit(sol):\n    new_sol = sol.copy()\n    index = np.random.randint(len(sol))\n    new_sol[index] = 1 - new_sol[index]  # Flip a random bit\n    return new_sol\n\n\nExample: 1011011001‚Ä¶1100100110 (100 bits total)\nRandomly Selected Bit to Flip: Assume random index was 25.\n\nNew Solution After Flip: If the bit at index = 25 was 1, it flips to 0, and vice versa.\nBefore: 1011011001‚Ä¶1100100110 (Bit at index 25 is 0)\nAfter: 1011011001‚Ä¶1101100110 (Bit at index 25 is flipped to 1)\n\nSince np.random.randint(len(sol)) randomly selects a bit, every execution results in different positions being flipped.\n\n\n\nEvaluate (E)\n\nThe evaluate function calculates the fitness of the current solution by summing up the number of 1‚Äôs in the binary array (sol).The One-Max problem aims to maximize this value, with the goal being to find the solution with all bits set to 1.\n\n\n# Evaluation Function (E): One-max function\ndef evaluate(sol):\n    return np.sum(sol)\n\n\n\nDetermination (D)\n\nThe determine function decides whether to accept the new neighboring solution (neighbor_value), based on its value compared to the current solution (current_value).\nIf the neighboring solution is better (i.e., it has a higher One-Max value), it is accepted.\nIf the neighboring solution is worse, it may still be accepted based on the probability calculated by the simulated annealing algorithm:\n\nThe acceptance probability depends on the temperature and the difference between the new and current values.\nA higher temperature allows for more exploration of worse solutions, while a lower temperature makes the algorithm more selective.\n\nThe function returns True if the new solution is accepted, otherwise False.\n\n\n# Determination function for SA (D)\ndef determine(neighbor_value, current_value, temperature):\n    if neighbor_value &gt; current_value:\n        return True\n    else:\n        acceptance_probability = np.exp((neighbor_value - current_value) / temperature)\n        return random.random() &lt; acceptance_probability\n\n\n\n\nExample of Acceptance Calculations\n\n\n\n\nRun Function\n\nThe init_sa function generates a random initial solution (current_sol) with a binary array of length num_bits.\nThis solution is evaluated using the evaluate function, which counts the number of 1‚Äôs in the array.\nThe current solution is also set as the best solution initially, since no other solutions have been explored yet.\nThe temperature is set to initial_temp, which controls the probability of accepting worse solutions in the early stages of the algorithm.\nThe list value_history is initialized to store the value (fitness) of the current solution over time.\nThis main loop runs until the temperature drops below min_temp (0.0001) or the maximum number of evals (max_evals, default 1000) is reached. Each evaluation represents one step in the simulated annealing process.\nA neighboring solution (neighbor_sol) is generated by the transit function, which randomly flips one bit in the current solution.\nThis represents exploring a new area of the search space close to the current solution.\nThe neighboring solution is evaluated using the evaluate function, which calculates its fitness (i.e., the number of 1‚Äôs in the binary array). This value is compared to the current solution‚Äôs fitness.\nThe determine function decides whether to accept the neighboring solution\nThe current solution‚Äôs fitness is appended to value_history to keep a record of the solution‚Äôs fitness across evaluations. This will later be used to plot the progress of the algorithm.\nAfter each evaluation, the temperature is reduced by multiplying it by the cooling_rate (default 0.99). This cooling process gradually reduces the probability of accepting worse solutions, making the algorithm behave more like greedy hill climbing toward the end.\nThe number of evals is incremented.\nAfter the loop terminates (either because the temperature has cooled sufficiently or the maximum number of evaluations has been reached), the function returns: 1) best_sol: The best solution found during the process. 2) best_value: The fitness value of the best solution. 3) value_history: A list of the fitness values over time, useful for visualizing the algorithm‚Äôs progress.\n\n\n# Simulated Annealing (SA) function\ndef simulated_annealing(num_bits):\n    current_sol, current_value = init_sa(num_bits)\n    best_sol, best_value = current_sol, current_value\n    temperature = initial_temp\n    value_history = [current_value]\n\n    evals = 0\n    while temperature &gt; min_temp and evals &lt; max_evals:\n        neighbor_sol = transit(current_sol)\n        neighbor_value = evaluate(neighbor_sol)\n\n        if determine(neighbor_value, current_value, temperature):\n            current_sol, current_value = neighbor_sol, neighbor_value\n            if current_value &gt; best_value:\n                best_sol, best_value = current_sol, current_value\n\n        # Store history of values for plotting\n        value_history.append(current_value)\n\n        # Cool down the temperature\n        temperature *= cooling_rate\n        evals += 1\n\n    return best_sol, best_value, value_history\n\n\n\n\nExample Cooling Schedule Calculations\n\n\n\n\nMain Execution\n\nThe process starts by initializing the solution using Initialization (I) (init_sa), followed by repeated transitions (T) and evaluations (E) to explore the search space.\nAfter each transition, the Determination (D) function decides whether to accept the new solution based on the current temperature and fitness values.\nThe temperature gradually cools down, reducing the chance of accepting worse solutions as the algorithm progresses.\nFinally, the results are printed and plotted for visual analysis.\n\n\n# Main execution\nstart_time = time.time()\nbest_sol, best_value, value_history = simulated_annealing(num_bits)\nend_time = time.time()\nexecution_time = end_time - start_time",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#output",
    "href": "sa.html#output",
    "title": "Simulated Annealing",
    "section": "Output",
    "text": "Output\n\n# Output results (O)\nprint(f\"Best solution: {best_sol}\")\nprint(f\"Best value (number of 1s): {best_value}\")\nprint(f\"Execution time: {execution_time:.6f} seconds\")\n\n# Plot the simulated annealing progress\nplt.figure(figsize=(10, 6))\nplt.plot(value_history, marker='o', linestyle='--', color='blue', label='SA Progress')\nplt.title(\"One-Max Problem with Simulated Annealing Progress\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Objective Value (Number of 1s)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nBest solution: [1 1 1 1 1 1 1 1 1 1]\nBest value (number of 1s): 10\nExecution time: 0.008037 seconds",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#example-results",
    "href": "sa.html#example-results",
    "title": "Simulated Annealing",
    "section": "Example Results",
    "text": "Example Results\n\nimport numpy as np\nimport random\nimport math\nimport matplotlib.pyplot as plt\nimport time\n\n# Define the necessary global variables\nnum_evals = 1000\ninitial_temp = 10\ncooling_rate = 0.99\nmin_temp = 0.00001\n\n# Ackley function (1D)\ndef ackley(x):\n    a = 20\n    b = 0.2\n    c = 2 * np.pi\n    term1 = -a * np.exp(-b * np.sqrt(np.mean(np.square(x))))\n    term2 = -np.exp(np.mean(np.cos(c * np.array(x))))\n    return term1 + term2 + a + np.exp(1)\n\n# Initialization function (I) to set the starting point\ndef init_sa():\n    start_x = random.uniform(-10, 10)\n    return start_x, ackley([start_x])\n\n\nHere, the new candidate solution (neighbor_x) is generated by adding a random perturbation from the uniform distribution random.uniform(-1, 1). This means the step size varies randomly between -1 and 1.\nIf you want more control over the step size, you could introduce a parameter, step_size, and modify the function.\n\n\n# Transition function (T)\ndef transit(current_x):\n    neighbor_x = current_x + random.uniform(-1, 1)\n    return neighbor_x\n\n\nThe determine function is the same as the One Max problem except that because the Ackley is a min problem the neighbor_value and current_value switch places.\n\n\n# Determination function for SA (D)\ndef determine(neighbor_value, current_value, temperature):\n    if neighbor_value &lt; current_value:\n        return True\n    else:\n        acceptance_probability = np.exp((current_value - neighbor_value) / temperature)\n        return np.random.rand() &lt; acceptance_probability\n\n\n# Simulated Annealing (SA) function\ndef simulated_annealing():\n     #Initialize (I)\n    current_x, current_value = init_sa()\n    best_x, best_value = current_x, current_value\n    temperature = initial_temp\n    x_history, value_history = [current_x], [current_value]\n\n    evals = 0\n   #Transit (T) \n    while temperature &gt; min_temp and evals &lt; max_evals:\n        neighbor_x = transit(current_x)\n        # Evaluate (E)\n        neighbor_value = ackley([neighbor_x])\n        \n        # Determine (D)\n        if determine(neighbor_value, current_value, temperature):\n            current_x, current_value = neighbor_x, neighbor_value\n            if current_value &lt; best_value:\n                best_x, best_value = current_x, current_value\n        \n        # Store history of x and values for plotting\n        x_history.append(current_x)\n        value_history.append(current_value)\n\n        # Cool down the temperature\n        temperature *= cooling_rate\n        evals += 1\n\n    return best_x, best_value, x_history, value_history\n\n# Main execution\nstart_time = time.time()\nbest_x, best_value, x_history, value_history = simulated_annealing()\nend_time = time.time()\nexecution_time = end_time - start_time\n\n# Output (O)\nprint(f\"Optimal x: {best_x}\")\nprint(f\"Optimal value: {best_value}\")\nprint(f\"Execution time: {execution_time:.6f} seconds\")\n\n# Plot the Ackley function and simulated annealing progress\nx_values = np.linspace(-10, 10, 1000)\ny_values = [ackley([x]) for x in x_values]\n\nplt.figure(figsize=(10, 6))\nplt.plot(x_values, y_values, label=\"Ackley Function\", color='b')\nplt.plot(x_history, value_history, marker='o', linestyle='--', color='red', label='SA Progress')\nplt.scatter(best_x, best_value, color='green', s=100, zorder=5, label='Optimal Value')\nplt.title(\"Ackley Function in 1D with Simulated Annealing Progress\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nOptimal x: 0.0015065151496616025\nOptimal value: 0.006146927888476927\nExecution time: 0.014532 seconds\n\n\n\n\n\n\n\n\n\n\nOptimal x: -0.0006396808092359318\n\nThe value -0.00063968 represents a point very close to 0 on the x-axis, where the Ackley function achieves its minimum value in the 1D case.The true global minimum of the Ackley function occurs at x=0, so this value is nearly optimal.\n\nOptimal value: 0.0025805153299995887\n\nThe Ackley function‚Äôs global minimum is 0, which occurs exactly at x=0. The value 0.0025805153299995887 is very close to this, showing that the algorithm successfully minimized the function but did not reach the exact minimum. This small difference can be due to the stochastic nature of simulated annealing and the stopping criteria (temperature and iterations).\n\nExecution time: 0.012161 seconds\n\nThis indicates the total time it took for the simulated annealing algorithm to run and find the optimal solution.\nThe process completed in just 0.012 seconds, which is very fast. This fast execution time suggests that the algorithm quickly converged to a near-optimal solution, likely because the problem space (1D) is simple and small, and the Ackley function‚Äôs shape guides the algorithm efficiently toward the global minimum.\n\n\n\nComparing SA to HC with Ackley Function\n\nIn an example run, I found the following results depicted in the png image below. The simulated annealing algorithm performed well, finding a solution very close to the global minimum of the Ackley function in a short time.\nThe slight deviation from the exact minimum value (0) is expected due to the stochastic exploration nature of simulated annealing. You can compare these results to the ones above under a different set seed.\n\n\n\n\nComparing HA with Ackley\n\n\n\nOur SA model was much better than our HC model with the Ackley Function. Why?\n\nThe Ackley function is known for its multimodal landscape‚Äîit has many local minima and a global minimum at ùë•=0. The landscape consists of a broad plateau followed by sharp drops toward the global minimum, making it challenging for optimization algorithms to find the true global minimum.\nHill climbing can get stuck in one of the many local minima because it only moves to a better neighboring solution. Once it reaches a local minimum, it can‚Äôt escape because no better solution is immediately available in its neighborhood.\nSimulated annealing, by contrast, has the ability to accept worse solutions early in the process, which allows it to escape local minima and continue searching for the global minimum.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#our-hill-climbing-finance-model-was-better.-why",
    "href": "sa.html#our-hill-climbing-finance-model-was-better.-why",
    "title": "Simulated Annealing",
    "section": "Our Hill Climbing Finance Model was Better. Why?",
    "text": "Our Hill Climbing Finance Model was Better. Why?\n\nHill Climbing\n\nDeterministic: In each iteration, the algorithm strictly accepts a new portfolio only if it improves the Sharpe ratio (a measure of risk-adjusted returns). There‚Äôs no acceptance of worse solutions.\nThis results in a steady and consistent increase in the Sharpe ratio because the model always moves toward better solutions without exploring worse ones.\nSince hill climbing is greedy and deterministic, it focuses on continuously improving the portfolio weights in a direct manner, which can be more efficient for problems where the optimization landscape is relatively smooth or doesn‚Äôt have too many local minima.\n\nSimulated Annealing:\n\nStochastic Exploration: Simulated annealing, on the other hand, allows for the acceptance of worse solutions, especially early in the process when the temperature is high. This stochastic exploration helps to avoid getting stuck in local minima, but it can sometimes lead to suboptimal moves that temporarily reduce performance.\nSA slowly improves the Sharpe ratio by lowering the temperature and becoming more selective over time. However, this can result in slower convergence compared to hill climbing, which directly improves the Sharpe ratio with each iteration.\nWhile simulated annealing balances exploration and exploitation, its performance might be slightly worse in this case because it explores a broader range of solutions, some of which may be worse. The stochastic nature may cause delays in reaching the global optimum in cases where the optimization problem is less prone to getting stuck in local minima.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "sa.html#conclusions",
    "href": "sa.html#conclusions",
    "title": "Simulated Annealing",
    "section": "Conclusions",
    "text": "Conclusions\n\nStable and Simple Search Space: In portfolio optimization, especially with a limited number of assets, the optimization landscape might not be highly rugged, making hill climbing‚Äôs greedy approach more effective at converging quickly to a good solution.\nDirect Progress: Hill climbing consistently increases the Sharpe ratio by only accepting better solutions, leading to slightly higher risk-adjusted returns over the SA model, which accepts suboptimal solutions early on.",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Simulated Annealing</span>"
    ]
  },
  {
    "objectID": "ga.html",
    "href": "ga.html",
    "title": "Genetic Algorithms",
    "section": "",
    "text": "Genetic Algorithms Defined\nIf you could ‚Äòmutate‚Äô one skill you have to be even better, which one would it be? And if you could ‚Äòcrossover‚Äô with someone else‚Äôs skill, what would you pick?",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#key-steps",
    "href": "ga.html#key-steps",
    "title": "Genetic Algorithms",
    "section": "Key Steps",
    "text": "Key Steps\n\nInitialization: Randomly generate an initial population of individuals (solutions).\nSelection: Choose the fittest individuals based on a fitness function that measures solution quality.\nCrossover (Recombination): Combine parts of two parent solutions to create offspring (new solutions).\nMutation: Introduce random changes to an individual to maintain diversity and explore new parts of the solution space.\nReplacement: Replace the old population with the new one, ensuring improvement over generations.\nTermination: Continue until a stopping condition (e.g., number of generations or convergence) is met.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#advantages",
    "href": "ga.html#advantages",
    "title": "Genetic Algorithms",
    "section": "Advantages",
    "text": "Advantages\n\nGlobal Search: Capable of exploring large solution spaces and avoiding local optima.\nFlexibility: Can be applied to many types of optimization problems, including those with non-linear or non-differentiable objectives.\nHeuristic Nature: Useful when problem-solving methods like calculus-based optimization are not feasible.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#genetic-algorithms-defined",
    "href": "ga.html#genetic-algorithms-defined",
    "title": "Genetic Algorithms",
    "section": "",
    "text": "Objective: Evolve the population toward better solutions by mimicking evolutionary processes through selection, crossover, and mutation.\nPopulation: A set of candidate solutions (individuals) to the problem.\nGenes & Chromosomes:\n\nEach individual is represented by a chromosome, which is a string of genes.\nChromosomes can be encoded in binary, real numbers, or symbolic representations.\n\nGenerations & Evolution:\n\nEach iteration in the process is called a generation.\nA genetic algorithm evaluates multiple candidate solutions per generation.\n\nFitness & Selection:\n\nFitness represents the objective function value.\nSelection favors solutions with better fitness to pass their genes to the next generation.\n\nAlgorithm Flow: Initial Population ‚Üí Selection ‚Üí Crossover ‚Üí Mutation ‚Üí New Population",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#the-relationship-between-population-s-chromosome-si-and-gene-sij",
    "href": "ga.html#the-relationship-between-population-s-chromosome-si-and-gene-sij",
    "title": "Genetic Algorithms",
    "section": "The relationship between population \\(s\\), chromosome \\(si\\), and gene \\(si,j\\)",
    "text": "The relationship between population \\(s\\), chromosome \\(si\\), and gene \\(si,j\\)\n\n\\(s\\) represents the population (a set of solutions),\n\\(s_i\\) represents a chromosome (a solution),\n\\(s_{ij}\\) represents a gene (a subsolution),\n\\(m\\) represents the number of chromosomes in the population (also called population size, and\n\\(n\\) represents the number of subsolutions in a chromosome.\nThe objective value of each chromosome will also be transformed to a value called the fitnes value by a so-called fitness function.\n\n\n\n\nGenetic Algorithm\n\n\n\nThe Relationships between Parents and Children\n\nEach current solution selected by the selection operator is called a parent, while each new candidate solution is called an offspring during the convergence process.\nThe parents s‚Äô will be selected by the selection operator from the current population s at iteration t. The fitness values of all the chromosomes will also be calculated by the selection operator.\nOnce the parents are selected by the selection operator, the crossover and mutation operators are used to generate a new population \\(v\\), which is called the offspring of the parents \\(s‚Äô\\).\nThe offspring at iteration \\(t\\) will become the current population s of iteration \\(t+1\\). This is referred to as the reproduction process.\n\n\n\n\nGenetic Algorithm\n\n\n\nThe strategies of the selection, crossover, and mutation operators of GA: (a) using only the selection operator and (b) using all the three operators.\nGA will first randomly select some of the chromosomes based on the fitness value of each chromosome.\nShows the situation in which only the selection operator is used, i.e., no other transition operators are used. In this case, the distribution of all the chromosomes will be shifted and changed from left to right on the x-axis from generation t=1 to generation t=3.\nThis means that the average objective value of the population will be increased while the variance is decreased. If GA uses only the selection operator but none of the transition operators (e.g., crossover or mutation), it will not generate any new candidate solutions even though the average objective value of all the chromosomes is raised from 1.5 to 2.5.\nShows that if GA uses not only the selection operator to select better chromosomes for the next generation but also the crossover and mutation operators as the transition operators to generate new chromosomes for the population, this makes it possible for GA to find better candidate solutions. In this example, the average objective value of all the chromosomes will be increased from 1.5 to 3, while the best objective value will exceed 3.\n\n\n\n\nGenetic Algorithm\n\n\n\nThe strategy of the transition operator of GA. (a) How the crossover operator works. (b) How the mutation operator works.\nGA will typically apply the crossover and mutation operators to the chromosomes selected by the selection operator.\nTwo-dimensional landscape will be used as compared to one dimensional used in traditional methods, simulated annealing or tabu search.\n\n\n\n\nGenetic Algorithm",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#refining-genetic-algorithms",
    "href": "ga.html#refining-genetic-algorithms",
    "title": "Genetic Algorithms",
    "section": "Refining Genetic Algorithms",
    "text": "Refining Genetic Algorithms\n\nThere‚Äôs no perfect setup for GA parameters. Experiment with combinations and monitor performance to find the best results for your specific problem.\nGA Parameters Are Crucial: The effectiveness of GA depends heavily on carefully tuning parameters like population size, mutation rate, and the number of generations.\nPopulation Size: For more complex problems, use larger populations (e.g., 1000‚Äì2000 individuals)\nMutation Rate: A low probability, typically 0.001 to 0.002 per gene.\nNumber of Generations: For more generations (e.g., 1000‚Äì2000 generations).\nFitness Function: Carefully tailored to the specific problem being solved, as it guides the selection process.\nSelection Method: rank-based linear selection or proportional selection can be used.\nCrossover Mechanism: Single-point crossover chosen at a random location",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#fitness-function-alternatives-in-heuristic-models",
    "href": "ga.html#fitness-function-alternatives-in-heuristic-models",
    "title": "Genetic Algorithms",
    "section": "Fitness Function Alternatives in Heuristic Models",
    "text": "Fitness Function Alternatives in Heuristic Models\n\nFitness measures and selection mechanisms together determine the quality of offspring (solutions) in evolutionary or heuristic algorithms.\n\nWhile fitness measures are problem-specific, several reasonable alternatives often exist.\nIt‚Äôs essential to tailor the fitness function to the specific goals of the problem (e.g., precision, speed, or robustness).\n\nSelecting the Right Fitness Function: Some fitness measures work significantly better than others depending on the problem.\nLeverage prior knowledge or insights from similar problems to guide fitness function selection.\nTrial and Error: Expect to experiment with different fitness measures to find the one that best suits your specific problem.\nInstead of a OneMax that counts the number of 1s, if our problem involved finding the closest match to a target solution (e.g., an image, a numeric function, or a specific binary pattern), you would need to replace or modify this fitness function using distance-based measures.\nCommon Distance Measures for Comparing Solutions to a Target Using an Image as an Example:\n\nSum of Squared Differences: A measure of the total squared deviation of each pixel from the target, commonly used when larger errors should be penalized more.\nEuclidean Distance: The square root of the sum of squared differences, providing a more intuitive ‚Äúdistance‚Äù measure in the image space.\nSum of Absolute Pixel Differences: A simpler alternative that sums the absolute differences for each pixel, often less sensitive to outliers.\nMaximum Absolute Pixel Difference: Focuses on the largest deviation, highlighting the worst pixel match.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#approaches-to-optimize-the-mutation-rate",
    "href": "ga.html#approaches-to-optimize-the-mutation-rate",
    "title": "Genetic Algorithms",
    "section": "Approaches to Optimize the Mutation Rate",
    "text": "Approaches to Optimize the Mutation Rate\n\nHyperparameter Tuning: Use various fixed mutation rates and evaluate the performance of the GA over multiple runs to identify the best one. This is the simplest method but can be time-consuming as it involves manual experimentation.\nDynamic Mutation Rate: Adjust the mutation rate dynamically during the evolution process. For example, you can start with a high mutation rate to encourage exploration and gradually reduce it as the algorithm converges.\nSelf-Adaptive Mutation Rate: Introduce a mechanism in the GA where each individual in the population has its own mutation rate, which evolves over time. The mutation rate itself becomes part of the genetic material.\nCross-Validation: Use techniques like k-fold cross-validation to evaluate the impact of different mutation rates and find the one that generalizes the best.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#comparing-ga-to-sa",
    "href": "ga.html#comparing-ga-to-sa",
    "title": "Genetic Algorithms",
    "section": "Comparing GA to SA",
    "text": "Comparing GA to SA\n\nMultiple search directions: Compared to the single-solution-based metaheuristic algorithms (e.g., SA) that search only one solution at a time, GA searches for more than one solution at a time during the convergence process. Since GA will search for multiple directions or regions at a time, its search diversity will normally be much higher than single-solution-based metaheuristics that search for only one direction or region at a time during the convergence process. Selection operator: Another characteristic of GA is that it uses the selection and fitness function operators to determine solutions to be searched, not just based on the objective value of each solution. This kind of mechanism keeps the search process of GA from looking for the best solution in the population all the time so that it will not always choose the solution with the best objective or fitness value to search its neighbors again and again. Consequently, GA will not easily get stuck in local optima at early iterations.\nCrossover operator: This operator is one of the transition operators of GA, which plays the role of exchanging information between parent chromosomes, such as moving portions of the genes of a chromosome to another. This kind of mechanism allows GA to restructure its solutions to form new solutions in such a way that the structure of the new solutions is not confined to the structure of the initial solutions and may even inherit partial structures from their parents; as a consequence, the search process of GA will quickly jump from one region to another in the solution space during the convergence process.\nMutation operator: This operator is another transition operator of GA, which ensures that the search process of GA is capable of escaping from a local optimum by changing the value of some genes randomly. Of course, this kind of mechanism will also play the role of fine-tuning the chromosomes of GA because only a few genes will be changed at a time.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#startup-commands",
    "href": "ga.html#startup-commands",
    "title": "Genetic Algorithms",
    "section": "Startup Commands",
    "text": "Startup Commands\n\n# Genetic Algorithm Ackley Example (1D)\n\nimport numpy as np\nimport random\nimport time\nimport matplotlib.pyplot as plt\n\n# Define the necessary global variables\npop_size = 50\nnum_generations = 100\ncrossover_rate = 0.7\nmutation_rate = 0.01\nnum_players = 3\n\n\nbest_obj_val = float('inf')\nbest_sol = None\n\n# Ackley function (1D)\ndef ackley(x):\n    a = 20\n    b = 0.2\n    c = 2 * np.pi\n    term1 = -a * np.exp(-b * np.sqrt(np.mean(np.square(x))))\n    term2 = -np.exp(np.mean(np.cos(c * np.array(x))))\n    return term1 + term2 + a + np.exp(1)",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#initiation-function-i",
    "href": "ga.html#initiation-function-i",
    "title": "Genetic Algorithms",
    "section": "Initiation Function (I)",
    "text": "Initiation Function (I)\n\nA population of solutions is initialized using the init_ga function, where each solution is a random number between -10 and 10, as per the problem domain.\nThe inclusion of the pop_size argument means that the function generates 50 rows, each containing one randomly chosen value between -10 and 10.\n\n\n# Initialization function (I) to set the starting point\ndef init_ga(pop_size):\n    return np.random.uniform(-10, 10, (pop_size, 1))",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#evaluation-function-e",
    "href": "ga.html#evaluation-function-e",
    "title": "Genetic Algorithms",
    "section": "Evaluation Function (E)",
    "text": "Evaluation Function (E)\n\nThe evaluate function is used to compute fitness of each individual in every generation.\nThis function Iterates over each individual (ind) in the population. It then computes the Ackley function value for that individual and produces a list of fitness values. The np.array() ensures that the output is a NumPy array for efficient computations.\n\n\n# Evaluation function (E)\ndef evaluate(pop):\n    return np.array([ackley(ind) for ind in pop])",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#determination-d-update-function",
    "href": "ga.html#determination-d-update-function",
    "title": "Genetic Algorithms",
    "section": "Determination (D): Update Function",
    "text": "Determination (D): Update Function\n\nThis function, update_best_sol, is responsible for keeping track of the best solution found so far in a genetic algorithm. It compares the fitness values of the current population (curr_obj_vals) to the global best objective value (best_obj_val).\n\nIt identifies the index (best_idx) of the individual in the current population with the lowest objective value (best fitness) using np.argmin.\nIf this individual‚Äôs objective value is better (lower) than the current global best (best_obj_val), it updates best_obj_val to this new lower value and updates best_sol to the corresponding individual from the current population.\n\nThis ensures that best_sol and best_obj_val always store the best solution and its fitness value found across all generations during the algorithm‚Äôs execution. The use of global allows the function to modify these variables outside its local scope.\n\n\n# Update best solution function\ndef update_best_sol(curr_pop, curr_obj_vals):\n    global best_sol, best_obj_val\n    best_idx = np.argmin(curr_obj_vals)\n    if curr_obj_vals[best_idx] &lt; best_obj_val:\n        best_obj_val = curr_obj_vals[best_idx]\n        best_sol = curr_pop[best_idx]",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#determination-d-selection-function",
    "href": "ga.html#determination-d-selection-function",
    "title": "Genetic Algorithms",
    "section": "Determination (D): Selection Function",
    "text": "Determination (D): Selection Function\n\nA subset of the population is selected for reproduction using a tournament selection process. This selection favors individuals with better fitness, helping to propagate good solutions.\n\nThis code implements a tournament selection mechanism in a genetic algorithm to select individuals for reproduction based on their fitness values.\nFor each individual in the current population (curr_pop), a tournament is conducted by randomly selecting a subset of individuals (of size num_players) without replacement.\nWithin this subset, the individual with the best fitness value (lowest objective value in curr_obj_vals) is identified using np.argmin and added to the selected population (selected_pop). This process ensures that fitter individuals have a higher chance of being chosen, promoting the propagation of good solutions while maintaining diversity through randomness.\nThe function returns the newly selected population as a NumPy array.\n\n\n\n# Determination (D)\n# Selection function using tournament selection\ndef select(curr_pop, curr_obj_vals, num_players):\n    selected_pop = []\n    for _ in range(len(curr_pop)):\n        tournament = np.random.choice(len(curr_pop), num_players, replace=False)\n        best_idx = tournament[np.argmin(curr_obj_vals[tournament])]\n        selected_pop.append(curr_pop[best_idx])\n    return np.array(selected_pop)",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#transition-t-crossover-function",
    "href": "ga.html#transition-t-crossover-function",
    "title": "Genetic Algorithms",
    "section": "Transition (T): Crossover Function",
    "text": "Transition (T): Crossover Function\n\nIn genetic algorithms, the transition function refers to the combined operations of crossover and mutation that generate a new population (offspring) from the current population. The transition function first applies crossover to exchange genetic material between selected parent solutions, creating new offspring, and then applies mutation to introduce small random changes in the offspring, maintaining diversity and enabling exploration of the solution space. This ensures a balance between exploitation of good solutions (via crossover) and exploration of new solutions (via mutation).\nCrossover: Pairs of solutions from the selected population are combined to create new offspring by mixing parts of the parent solutions.\nA new population (new_pop) is initialized as a copy of the current population (pop). The loop iterates over the population in pairs. For each pair, a random number is generated using np.random.rand(). If this number is less than the crossover_rate, crossover occurs:\n\nA crossover point is chosen randomly (here, using np.random.randint(0, pop.shape[1]), which specifies where the two parent solutions will exchange segments. The segments of the two parent solutions from the crossover point onward are swapped, creating two new offspring solutions.\nIf the random number is greater than the crossover_rate, no crossover occurs, and the parent solutions remain unchanged. The function returns the new_pop containing the modified population after applying the crossover operation.\n\n\n# Transition function (T): Crossover and Mutation\n# Crossover function (T)\ndef crossover(pop, crossover_rate):\n    new_pop = pop.copy()\n    for i in range(0, len(pop) - 1, 2):\n        if np.random.rand() &lt; crossover_rate:\n            crossover_point = np.random.randint(0, pop.shape[1])\n            new_pop[i, crossover_point:], new_pop[i + 1, crossover_point:] = (\n                pop[i + 1, crossover_point:], \n                pop[i, crossover_point:]\n            )\n    return new_pop",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#transition-t-mutation-function",
    "href": "ga.html#transition-t-mutation-function",
    "title": "Genetic Algorithms",
    "section": "Transition (T): Mutation Function",
    "text": "Transition (T): Mutation Function\n\nMutation: Random mutations are introduced in the offspring solutions to maintain diversity and avoid premature convergence.\nIt introduces random changes to a population of solutions in a genetic algorithm. It iterates over each individual in the population (pop) and, with a probability specified by the mutation_rate, replaces the current individual with a new value randomly drawn from a uniform distribution between -10 and 10. This random alteration helps maintain diversity in the population, enabling the genetic algorithm to explore new areas of the solution space and avoid premature convergence. The modified population is returned at the end of the function. This introduces large jumps (not a small step size) since any value can be replaced.\n\n\n# Mutation function (M)\ndef mutation(pop, mutation_rate):\n    for i in range(len(pop)):\n        if np.random.rand() &lt; mutation_rate:\n            pop[i] = np.random.uniform(-10, 10)\n    return pop",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#main-loop",
    "href": "ga.html#main-loop",
    "title": "Genetic Algorithms",
    "section": "Main Loop",
    "text": "Main Loop\n\nThe algorithm iterates through a fixed number of generations (iterations). Each generation represents an iteration of evolving the population to improve the solutions.\nThe new population (after crossover and mutation) replaces the old one for the next generation.\n\n\n# Genetic Algorithm function (D)\ndef genetic_algorithm():\n    global best_sol, best_obj_val\n    pop = init_ga(pop_size)\n    \n    for generation in range(num_generations):\n        obj_vals = evaluate(pop)\n        update_best_sol(pop, obj_vals)\n        \n        selected_pop = select(pop, obj_vals, num_players)\n        offspring_pop = crossover(selected_pop, crossover_rate)\n        mutated_pop = mutation(offspring_pop, mutation_rate)\n        \n        pop = mutated_pop  \n    return best_sol, best_obj_val",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#main-execution",
    "href": "ga.html#main-execution",
    "title": "Genetic Algorithms",
    "section": "Main Execution",
    "text": "Main Execution\n\n# Main execution\nstart_time = time.time()\nbest_solution, best_value = genetic_algorithm()\nend_time = time.time()\nexecution_time = end_time - start_time",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#output-o",
    "href": "ga.html#output-o",
    "title": "Genetic Algorithms",
    "section": "Output (O)",
    "text": "Output (O)\n\n# Output (O)\nprint(f\"Optimal solution: {best_solution}\")\nprint(f\"Optimal value: {best_value}\")\nprint(f\"Execution time for Genetic Algorithm: {execution_time:.6f} seconds\")\n\n# Plot the Ackley function and genetic algorithm progress\nx_values = np.linspace(-10, 10, 1000)\ny_values = [ackley([x]) for x in x_values]\n\nplt.figure(figsize=(10, 6))\nplt.plot(x_values, y_values, label=\"Ackley Function\", color='b')\nplt.scatter(best_solution[0], best_value, color='green', label='GA Best Solution', s=100)\nplt.title(\"Ackley Function in 1D with Genetic Algorithm Best Solution\")\nplt.xlabel(\"x\")\nplt.ylabel(\"f(x)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\nOptimal solution: [0.07283306]\nOptimal value: 0.5550096936246942\nExecution time for Genetic Algorithm: 0.116501 seconds\n\n\n\n\n\n\n\n\n\n\nComparison GA/SA with Ackley FUnction\n\nTo compare Simulated Annealing (SA) with Genetic Algorithms (GA), we can implement a Genetic Algorithm for the same optimization problem (Ackley function) and then compare the two approaches based on:\n\nPerformance: Compare the final optimized values.\nConvergence Speed: How fast each algorithm converges to a solution.\nExploration vs.¬†Exploitation: How each method balances searching new areas (exploration) versus refining the current solution (exploitation).\n\nPopulation-based vs.¬†Single Solution:\n\nGA maintains and evolves a population of solutions.\nSA operates on a single solution and modifies it over time.\n\nSA Model:\n\nOptimal x: 0.0021715903903449796\nOptimal value: 0.00893749448894754\nExecution time: 0.014006 seconds\n\nGA Model:\n\nOptimal solution: [-0.06420558]\nOptimal value: 0.4648604493269599\nExecution time for Genetic Algorithm: 0.099029 seconds",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#the-relationships-between-populations-chromosomes-and-genes",
    "href": "ga.html#the-relationships-between-populations-chromosomes-and-genes",
    "title": "Genetic Algorithms",
    "section": "The Relationships between Populations, Chromosomes, and Genes",
    "text": "The Relationships between Populations, Chromosomes, and Genes\n\n\\(s\\) represents the population (a set of solutions),\n\\(s_i\\) represents a chromosome (a solution),\n\\(s_{ij}\\) represents a gene (a subsolution),\nThe number of chromosomes in the population (also called population size), and the number of subsolutions in a chromosome are also considered.\nThe objective value of each chromosome will also be transformed to a value called the fitnes value calculated by a fitness function.\n\n\n\n\nGenetic Algorithm\n\n\n\nThe Relationships between Parents and Children\n\nEach current solution selected by the selection operator is called a parent, while each new candidate solution is called an offspring during the convergence process.\nThe parents s‚Äô will be selected by the selection operator from the current population s at iteration t. The fitness values of all the chromosomes will also be calculated by the selection operator.\nOnce the parents are selected by the selection operator, the crossover and mutation operators are used to generate a new population \\(v\\), which is called the offspring of the parents \\(s‚Äô\\).\nThe offspring at iteration \\(t\\) will become the current population s of iteration \\(t+1\\). This is referred to as the reproduction process.\n\n\n\n\nGenetic Algorithm\n\n\n\nThe strategies of the selection, crossover, and mutation operators of GA: (a) using only the selection operator and (b) using all the three operators.\nGA will first randomly select some of the chromosomes based on the fitness value of each chromosome.\n\nshows the situation in which only the selection operator is used, i.e., no other transition operators are used. In this case, the distribution of all the chromosomes will be shifted and changed from left to right on the x-axis from generation t=1 to generation t=3.\n\nThis means that the average objective value of the population will be increased while the variance is decreased. If GA uses only the selection operator but none of the transition operators (e.g., crossover or mutation), it will not generate any new candidate solutions even though the average objective value of all the chromosomes is raised from 1.5 to 2.5.\n\nshows that if GA uses not only the selection operator to select better chromosomes for the next generation but also the crossover and mutation operators as the transition operators to generate new chromosomes for the population, this makes it possible for GA to find better candidate solutions. In this example, the average objective value of all the chromosomes will be increased from 1.5 to 3, while the best objective value will exceed 3.\n\n\n\n\n\nGenetic Algorithm",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#tournament-selection",
    "href": "ga.html#tournament-selection",
    "title": "Genetic Algorithms",
    "section": "Tournament Selection",
    "text": "Tournament Selection\n\nRandom Selection of Competitors\n\nFor each individual to be selected, a random subset of num_players individuals is chosen from the population.\nA random choice function selects a number of players without replacement from the population.\n\nChoosing the Best Individual\n\nThe fitness values of the selected individuals are compared.\nThe individual with the highest fitness is chosen as the winner.\nThis is done using np.argmax(fitness_vals[tournament]), which finds the index of the best individual in the subset.\n\nBuilding the New Population\n\nThis process is repeated until a new population of the same size is formed.\nThe best individuals from each tournament are added to selected_pop.\n\nReturning the Selected Population\n\nThe function returns a new population consisting of individuals selected through tournament selection.\n\n\n\nTournament Selection Example\n\n1st Selection Round\n\nRandomly select two individuals for the tournament: Suppose the tournament picks F1 and F2.\nCompare their fitness values:F1 (6) vs.¬†F2 (4) ‚Üí F1 wins.\nF1 is added to the new population.\n\n2nd Selection Round\n\nAnother tournament is held, randomly selecting F2 and F3.\nCompare their fitness values:\nF2 (4) vs.¬†F3 (5) ‚Üí F3 wins.\nF3 is added to the new population.\n\n3rd Selection Round\n\nAnother tournament is held, randomly selecting F1 and F3.\nCompare their fitness values:\nF1 (6) vs.¬†F3 (5) ‚Üí F1 wins.\nF1 is added again.\n\n\n ### Controlling Selection Pressure * Selection Pressure refers to the degree to which fitter individuals are favored for reproduction over less fit ones. * Larger tournaments (e.g., num_players &gt;= 3) favor stronger individuals. + Increases selection pressure because there‚Äôs a higher chance that fitter individuals will be included and selected. + This can lead to faster convergence but may reduce genetic diversity, risking premature convergence to suboptimal solutions. * Smaller tournaments (e.g., num_players = 2) introduce more randomness. + Decreases selection pressure, allowing less fit individuals a better chance to be selected. This maintains diversity and promotes exploration but may slow down convergence.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#tournament-selection-example",
    "href": "ga.html#tournament-selection-example",
    "title": "Genetic Algorithms",
    "section": "Tournament Selection Example",
    "text": "Tournament Selection Example\n\n1st Selection Round\n\nRandomly select two individuals for the tournament: Suppose the tournament picks F1 and F2.\nCompare their fitness values:F1 (6) vs.¬†F2 (4) ‚Üí F1 wins.\nF1 is added to the new population.\n\n2nd Selection Round\n\nAnother tournament is held, randomly selecting F2 and F3.\nCompare their fitness values:\nF2 (4) vs.¬†F3 (5) ‚Üí F3 wins.\nF3 is added to the new population.\n\n3rd Selection Round\n\nAnother tournament is held, randomly selecting F1 and F3.\nCompare their fitness values:\nF1 (6) vs.¬†F3 (5) ‚Üí F1 wins.\nF1 is added again.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#disadvantages",
    "href": "ga.html#disadvantages",
    "title": "Genetic Algorithms",
    "section": "Disadvantages",
    "text": "Disadvantages\n\nComputationally Expensive ‚Äì GAs often require many function evaluations, making them slow for complex problems.\nNo Guarantee of Optimality ‚Äì They may converge to suboptimal solutions or get stuck in local optima.\nParameter Sensitivity ‚Äì Performance depends on careful tuning of mutation rate, crossover rate, and population size.\nLack of Interpretability ‚Äì The evolutionary process doesn‚Äôt always provide clear insights into why a solution was chosen.\nPremature Convergence ‚Äì The algorithm may lose diversity and settle on a poor solution early.\nInfeasibility for Some Problems ‚Äì Constraints can be difficult to handle, leading to invalid solutions.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#controlling-selection-pressure",
    "href": "ga.html#controlling-selection-pressure",
    "title": "Genetic Algorithms",
    "section": "Controlling Selection Pressure",
    "text": "Controlling Selection Pressure\n\nLarger tournaments (e.g., num_players = 3) favor stronger individuals.\nSmaller tournaments (num_players = 2) introduce more randomness.\n\n\n\n\nGA Selection Example",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#fitness-proportionate-selection",
    "href": "ga.html#fitness-proportionate-selection",
    "title": "Genetic Algorithms",
    "section": "Fitness-Proportionate Selection",
    "text": "Fitness-Proportionate Selection\n\nThe ‚ÄúRoulette Wheel‚Äù Strategy to Selection\nImagine a roulette wheel where each individual occupies a section proportional to its fitness. A random value (between 0 and 1) is generated, and the wheel is spun. The individual whose section the random value falls into is selected.\nThe probability of each chromosome being elected as a parent can be computed as follows: \\[p_i = \\frac{f_i}{\\sum_{j=1}^{m} f_j}\\]\nFitness \\(f_i\\) of member i as a proportion of sum of fitness of all population members, suggesting that \\(p_i = \\frac{\\text{fit}}{\\sum \\text{fit}}\\)\nAssume three individuals with fitness values:\n\n\\(F1 = 10\\)\n\\(F2 = 20\\)\n\\(F3 = 30\\)\n\nTheir survival probabilities are as follows:\n\np1= 10/((10+20+30))= .166$\np2= 20/((10+20+30)) = .333$\np3= 30/((10+20+30)) = .5$\n\nThus, individual 3 has the highest chance of being retained, but it‚Äôs still possible for less-fit individuals to survive.\nWe then generate a random number r in the range [0,1] and find the first individual whose cumulative probability exceeds r. For example, if we generate r=.4, we select F2, if we generate .7, we would select .5",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#crossover",
    "href": "ga.html#crossover",
    "title": "Genetic Algorithms",
    "section": "Crossover",
    "text": "Crossover\n\nThe chart is structured as a grid, where each point represents a potential solution encoded in binary form.\n\nThe labels Bit 1, Bit 2, Bit 3, and Bit 4 represent binary values in a 4-bit chromosome.\nThe fitness function values, denoted as f(X), are assigned to each solution.\nEach solution (chromosome) is represented as a 4-bit string (e.g., 0000, 1100, etc.).\nThe fitness function f(X) is computed for each chromosome and shown in the grid.\n\nEvolution Path Without Crossover & Mutation: The arrows represent the movement of solutions through selection. Selection: The GA picks better solutions based on fitness. The search path (arrows) shows the improvement in fitness as the GA selects better candidates. In this case, movement is limited, leading to potential stagnation in local optima.\n\nEvolution Path With Crossover & Mutation: This plot includes crossover and mutation, introducing more diversity. Some arrows jump to different areas of the grid, indicating a recombination of genetic material. Crossover: Two parent solutions combine to produce offspring with mixed genes. Mutation: A small random change is applied to a gene, allowing exploration of new regions. This helps escape local optima and find better solutions.\n\nWithout crossover and mutation (a), the algorithm might get stuck in a local optimum.(b) With crossover and mutation, the search space is explored more effectively, leading to better solutions\n\n\n\n\nGenetic Algorithm",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#mutation",
    "href": "ga.html#mutation",
    "title": "Genetic Algorithms",
    "section": "Mutation",
    "text": "Mutation\n\nThe mutate function introduces random changes (mutations) in the genetic algorithm to maintain diversity in the population. This prevents premature convergence to local optima.\nWe generate a random number between [0,1] and compare it to the mutation_rate. If this random number is less than the mutation_rate, mutation occurs (bit flips).\n\n\n\n\nGA Mutation Example\n\n\n\nHow Often Mutation Occurs\n\nCrossover is the primary driver of improvement in Genetic Algorithms, as it combines successful traits from parents.\nMutation acts as a secondary mechanism that introduces small random changes to explore new regions of the search space.\nIf mutation happens too frequently, the algorithm behaves more like a random search rather than an evolutionary process.\nSetting a small mutation rate (~0.01 or lower) ensures that improvements from selection and crossover are not lost due to excessive randomness.\nNote that in the example, I used .1, but we typically set a lower threshold than that.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#how-often-mutation-occurs",
    "href": "ga.html#how-often-mutation-occurs",
    "title": "Genetic Algorithms",
    "section": "How Often Mutation Occurs",
    "text": "How Often Mutation Occurs\n\nCrossover is the primary driver of improvement in Genetic Algorithms, as it combines successful traits from parents.\nMutation acts as a secondary mechanism that introduces small random changes to explore new regions of the search space.\nIf mutation happens too frequently, the algorithm behaves more like a random search rather than an evolutionary process.\nSetting a small mutation rate (~0.01 or lower) ensures that improvements from selection and crossover are not lost due to excessive randomness.\nNote that in the example, I used .1, but we typically set a lower threshold than that.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "The course presents a structured approach to introducing heuristic algorithms and optimization techniques. Beginning with a broad overview, we highlight the significance of algorithms in diverse industries and introduce foundational heuristic methods, emphasizing their value for complex problem-solving where traditional approaches fall short due to scalability or computational limits: (1Intro), (2Algorithms), (3NumpyBasics).\nThe content progresses through various algorithm types: constructive methods like greedy algorithms, exhaustive search, and iterative improvement approaches, including hill climbing. Key concepts like local optima, plateaus, and ridges are explained alongside examples such as the Knapsack Problem and Traveling Salesman Problem, laying a foundation for understanding optimization challenges and strategies for improving solutions iteratively: (4Greedy); (6ExhaustiveSearch); and (7HillClimbing).\nThe later course material delves into advanced metaheuristic algorithms, specifically Simulated Annealing (SA) and Genetic Algorithms (GA). SA is explained with its cooling schedule and probabilistic acceptance of suboptimal solutions, which help avoid local optima. GA is described through its evolutionary process‚Äîselection, crossover, and mutation‚Äîillustrating how populations of solutions evolve toward optimality over generations.\nThe advantages and disadvantages of each approach are covered, along with the importance of tuning parameters for effective convergence. This framework not only familiarizes students with the mechanics of each method but also stresses the trade-offs involved, preparing them to apply these techniques to benchmark problems such as the Ackley and OneMax functions: (5BenchmarkProblems); (8-9SimulatedAnnealing); and (10-11GeneticAlgorithms)",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "ga.html#fitness-proportional-selection",
    "href": "ga.html#fitness-proportional-selection",
    "title": "Genetic Algorithms",
    "section": "Fitness-Proportional Selection",
    "text": "Fitness-Proportional Selection\n\nThe ‚ÄúRoulette Wheel‚Äù Strategy to Selection\nImagine a roulette wheel where each individual occupies a section proportional to its fitness. A random value (between 0 and 1) is generated, and the wheel is spun. The individual whose section the random value falls into is selected.\nThe probability of each chromosome being elected as a parent can be computed as follows: \\[p_i = \\frac{f_i}{\\sum_{j=1}^{m} f_j}\\]\nFitness \\(f_i\\) of member i as a proportion of sum of fitness of all population members, suggesting that \\(p_i = \\frac{\\text{fit}}{\\sum \\text{fit}}\\)\nAssume three individuals with fitness values:\n\n\\(F1 = 10\\)\n\\(F2 = 20\\)\n\\(F3 = 30\\)\n\nTheir survival probabilities are as follows:\n\np1= 10/((10+20+30))= .166$\np2= 20/((10+20+30)) = .333$\np3= 30/((10+20+30)) = .5$\n\nThus, individual 3 has the highest chance of being retained, but it‚Äôs still possible for less-fit individuals to survive.\nWe then generate a random number r in the range [0,1] and find the first individual whose cumulative probability exceeds r. For example, if we generate r=.4, we select F2 because .166 + .333 is around .5 and that is the first cumulative probability above .4. If we generate .7, we would select F3 because 1 is the nearest cumulative probability that exceeds it.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#example-of-crossover",
    "href": "ga.html#example-of-crossover",
    "title": "Genetic Algorithms",
    "section": "Example of Crossover",
    "text": "Example of Crossover\n\nF1 (Parent 1) = 10101 | 01010\nF3 (Parent 2) = 11001 | 10011\nLet‚Äôs choose crossover point or index = 5 , which means after the fifth bit\n\nChild 1 = 10101 | 10011 ‚Üí ‚Äú1010110011‚Äù\nChild 2 = 11001 | 01010 ‚Üí ‚Äú1100101010‚Äú\n\nNow, these offspring can replace some individuals in the population or undergo mutation before the next iteration.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#population-based-ga-vs.-single-solution-sa",
    "href": "ga.html#population-based-ga-vs.-single-solution-sa",
    "title": "Genetic Algorithms",
    "section": "Population-based (GA) vs.¬†Single Solution (SA)",
    "text": "Population-based (GA) vs.¬†Single Solution (SA)\n\nCompared to the single-solution-based metaheuristic algorithms (e.g., SA) that search only one solution at a time, GA searches for more than one solution at a time during the convergence process.\nSince GA will search for multiple directions or regions at a time, its search diversity will normally be much higher than single-solution-based metaheuristics that search for only one direction or region at a time during the convergence process.\nThese two approaches are a fundamental distinction in how the search space is explored.\n\n\nPopulation-based\n\nGA maintains and evolves a population of solutions. Works with a group of candidate solutions (a population) at each step.\n\nEvolutionary operations such as selection, crossover (recombination), and mutation are applied to create new generations of solutions.\nThe search process involves diversity maintenance because multiple solutions evolve simultaneously.\n\n\n\n\nSingle Solution\n\nSA operates on a single solution and modifies it over time.\n\nWorks with one candidate solution at a time.\nStarts with a single initial solution and iteratively moves to a new solution by applying small changes (perturbations).\nAccepts new solutions based on a probability function that depends on a temperature parameter (allowing uphill moves early in the process).",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#exploration-strategy",
    "href": "ga.html#exploration-strategy",
    "title": "Genetic Algorithms",
    "section": "Exploration Strategy",
    "text": "Exploration Strategy\n\nGA uses crossover and mutation to explore the solution space, encouraging diversity.\n\nExploration and exploitation happen naturally through genetic operators‚Äîmutation introduces diversity (exploration), and selection rewards better candidates (exploitation).\n\nSA explores the space by making probabilistic changes to a single solution, allowing uphill moves early on (controlled by the temperature).\n\nExploration and exploitation happen through a balance of random jumps (exploration) and greedy selection (exploitation).",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#deterministic-vs.-probabilistic",
    "href": "ga.html#deterministic-vs.-probabilistic",
    "title": "Genetic Algorithms",
    "section": "Deterministic vs.¬†Probabilistic:",
    "text": "Deterministic vs.¬†Probabilistic:\n\nGA uses deterministic selection mechanisms, such as tournament selection, and a fixed mutation rate.\n\nUnlike purely stochastic methods (e.g., roulette wheel selection), tournament selection always picks the best candidate from the subset, making it more deterministic.\nThe mutation rate is predefined and remains constant throughout the GA process (e.g., 1% or 2% of genes randomly mutate). This ensures steady convergence toward better solutions while maintaining some diversity.\n\nSA uses a temperature mechanism to probabilistically accept worse solutions early in the search.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#convergence",
    "href": "ga.html#convergence",
    "title": "Genetic Algorithms",
    "section": "Convergence:",
    "text": "Convergence:\n\nGA uses generations and selection pressure to converge on the best solution over time.\n\nGAs evolve solutions through multiple generations, where each generation consists of a population of candidate solutions.\nSelection pressure refers to how strongly better solutions are favored during reproduction.\n\nHigher selection pressure (e.g., elitism, tournament selection with large num_players) quickly promotes fitter solutions but risks premature convergence.\n\nLower selection pressure (e.g., roulette wheel selection, or tournament with lower num_players) maintains more diversity but may slow down convergence.\n\n\nOver successive generations, poor solutions are eliminated, and better solutions dominate the population.\n\nSA cools down gradually, converging based on the temperature schedule.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  },
  {
    "objectID": "ga.html#summary",
    "href": "ga.html#summary",
    "title": "Genetic Algorithms",
    "section": "Summary",
    "text": "Summary\n\nThe design of GA is much more complex than that of SA; as a consequence, it is not only more difficult to implement than SA, it also takes much more memory space. Even Holland himself was surprised that the GA field was quiet for decades, as described in the preface of the 1992 edition of his book.\n\nWhen this book was originally published I was very optimistic, envisioning extensive reviews and a kind of ‚Äúbest seller‚Äù in the realm of monographs. Alas! That did not happen. After five years I did regain some optimism because the book did not ‚Äúdie,‚Äù as is usual with monographs, but kept on selling at 100‚Äì200 copies a year. Still, research in the area was confined almost entirely to my students and their colleagues, and it did not fit into anyone‚Äôs categories\n\nWhere John H. Holland (1929‚Äì2015) was a pioneering figure in the field of genetic algorithms (GAs). He is widely credited with formalizing and popularizing GAs as a computational approach for solving optimization and search problems.",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Genetic Algorithms</span>"
    ]
  }
]