[
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2¬† Introduction to Heuristic Algorithms",
    "section": "",
    "text": "3 Course Overview",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Introduction to Heuristic Algorithms</span>"
    ]
  },
  {
    "objectID": "intro.html#course-goals",
    "href": "intro.html#course-goals",
    "title": "2¬† Introduction to Heuristic Algorithms",
    "section": "3.1 Course Goals",
    "text": "3.1 Course Goals\n\nDevelop a solid process for algorithm development.\nEnhance Python programming skills.\nUnderstand the structure of heuristic models, focusing on:\n\nHill climbing\nSimulated annealing\nGenetic algorithms",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Introduction to Heuristic Algorithms</span>"
    ]
  },
  {
    "objectID": "intro.html#required-book",
    "href": "intro.html#required-book",
    "title": "2¬† Introduction to Heuristic Algorithms",
    "section": "3.2 Required Book",
    "text": "3.2 Required Book\nHandbook of Metaheuristic Algorithms\nAuthors: Chun-Wei Tsai & Ming-Chao Chiang\nPublisher: Academic Press\n- Access the book free through O‚ÄôReilly‚Äôs website with school credentials. - Python code is available on the author‚Äôs GitHub.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Introduction to Heuristic Algorithms</span>"
    ]
  },
  {
    "objectID": "algo.html",
    "href": "algo.html",
    "title": "2¬† Algorithm Design and Pseudocode",
    "section": "",
    "text": "3 Automate This by Christopher Steiner",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "numpy.html",
    "href": "numpy.html",
    "title": "3¬† Python for Heuristics & NumPy",
    "section": "",
    "text": "4 Coding Know-How\nWhat problems does this code have?  * Multiple instantiations of the same code within a program cause problems + More opportunities for errors + More maintenance * Blocks of reusable code solve this problem + These are called functions",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "greedy.html",
    "href": "greedy.html",
    "title": "4¬† Introduction to Greedy Algorithms",
    "section": "",
    "text": "5 Optimization Problem",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "benchmark.html",
    "href": "benchmark.html",
    "title": "5¬† Benchmark Optimization Problems",
    "section": "",
    "text": "6 Benchmark Problems: Overview",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "es.html",
    "href": "es.html",
    "title": "6¬† About",
    "section": "",
    "text": "About this site",
    "crumbs": [
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>About</span>"
    ]
  },
  {
    "objectID": "hc.html",
    "href": "hc.html",
    "title": "8¬† About",
    "section": "",
    "text": "About this site",
    "crumbs": [
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>About</span>"
    ]
  },
  {
    "objectID": "sa.html",
    "href": "sa.html",
    "title": "9¬† About",
    "section": "",
    "text": "About this site",
    "crumbs": [
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>About</span>"
    ]
  },
  {
    "objectID": "ga.html",
    "href": "ga.html",
    "title": "10¬† About",
    "section": "",
    "text": "About this site",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>About</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "10¬† Summary",
    "section": "",
    "text": "The course presents a structured approach to introducing heuristic algorithms and optimization techniques. Beginning with a broad overview, we highlight the significance of algorithms in diverse industries and introduce foundational heuristic methods, emphasizing their value for complex problem-solving where traditional approaches fall short due to scalability or computational limits: (1Intro), (2Algorithms), (3NumpyBasics).\nThe content progresses through various algorithm types: constructive methods like greedy algorithms, exhaustive search, and iterative improvement approaches, including hill climbing. Key concepts like local optima, plateaus, and ridges are explained alongside examples such as the Knapsack Problem and Traveling Salesman Problem, laying a foundation for understanding optimization challenges and strategies for improving solutions iteratively: (4Greedy); (6ExhaustiveSearch); and (7HillClimbing).\nThe later course material delves into advanced metaheuristic algorithms, specifically Simulated Annealing (SA) and Genetic Algorithms (GA). SA is explained with its cooling schedule and probabilistic acceptance of suboptimal solutions, which help avoid local optima. GA is described through its evolutionary process‚Äîselection, crossover, and mutation‚Äîillustrating how populations of solutions evolve toward optimality over generations.\nThe advantages and disadvantages of each approach are covered, along with the importance of tuning parameters for effective convergence. This framework not only familiarizes students with the mechanics of each method but also stresses the trade-offs involved, preparing them to apply these techniques to benchmark problems such as the Ackley and OneMax functions: (5BenchmarkProblems); (8-9SimulatedAnnealing); and (10-11GeneticAlgorithms)",
    "crumbs": [
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "11¬† Recommended Further Reading Materials",
    "section": "",
    "text": "Tsai, C.-W., & Chiang, M.-C. (2023). Handbook of metaheuristic algorithms. Academic Press. Retrieved from https://learning.oreilly.com/library/view/handbook-of-metaheuristic/9780443191091/",
    "crumbs": [
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Recommended Further Reading Materials</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Heuristic Modelling",
    "section": "",
    "text": "1 Course Overview",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Heuristic Algorithms</span>"
    ]
  },
  {
    "objectID": "index.html#course-goals",
    "href": "index.html#course-goals",
    "title": "Heuristic Modelling",
    "section": "1.1 Course Goals",
    "text": "1.1 Course Goals\n\nDevelop a solid process for algorithm development.\nEnhance Python programming skills.\nUnderstand the structure of heuristic models, focusing on:\n\nHill climbing\nSimulated annealing\nGenetic algorithms",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Heuristic Algorithms</span>"
    ]
  },
  {
    "objectID": "index.html#required-book",
    "href": "index.html#required-book",
    "title": "Heuristic Modelling",
    "section": "1.2 Required Book",
    "text": "1.2 Required Book\nHandbook of Metaheuristic Algorithms\nAuthors: Chun-Wei Tsai & Ming-Chao Chiang\nPublisher: Academic Press\n- Access the book free through O‚ÄôReilly‚Äôs website with school credentials. - Python code is available on the author‚Äôs GitHub.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Introduction to Heuristic Algorithms</span>"
    ]
  },
  {
    "objectID": "algo.html#summation-and-loop-example",
    "href": "algo.html#summation-and-loop-example",
    "title": "2¬† Algorithm Design and Pseudocode",
    "section": "6.3 Summation and Loop Example",
    "text": "6.3 Summation and Loop Example\n\nSummation Formula: \\(S = \\sum_{i=1}^{n} i = 1 + 2 + 3 + \\dots + n\\)\n\nExplanation: This formula calculates the sum of the first \\(n\\) natural numbers. The symbol \\(\\sum\\) represents the summation, and \\(i\\) is the index that runs from 1 to \\(n\\).\n\nNested Loop Calculation: \\(T = \\sum_{i=1}^{n} \\sum_{j=1}^{m} (i \\times j)\\)\n\nExplanation: This formula represents a nested summation where \\(i\\) runs from 1 to \\(n\\) and \\(j\\) runs from 1 to \\(m\\). It calculates the sum of the product of \\(i\\) and \\(j\\) over these ranges.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#conditional-statements-and-recursion",
    "href": "algo.html#conditional-statements-and-recursion",
    "title": "2¬† Algorithm Design and Pseudocode",
    "section": "6.4 Conditional Statements and Recursion",
    "text": "6.4 Conditional Statements and Recursion\n\nConditional Formula: \\[\nf(x) =\n\\begin{cases}\n0 & \\text{if } x &lt; 0 \\\\\n1 & \\text{if } x \\geq 0\n\\end{cases}\n\\]\n\nExplanation: This piecewise function returns 0 if ( x ) is less than 0, and 1 if ( x ) is greater than or equal to 0. It‚Äôs an example of a conditional statement in algorithmic form.\n\nRecursive Formula: \\[\nF(n) =\n\\begin{cases}\n1 & \\text{if } n = 1 \\\\\nn \\times F(n-1) & \\text{if } n &gt; 1\n\\end{cases}\n\\]\n\nExplanation: This is a recursive definition of the factorial function. For ( n = 1 ), ( F(n) = 1 ). For ( n &gt; 1 ), ( F(n) ) is defined as ( n ) times the factorial of ( n-1 ).",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#big-o-notation-and-algorithm-complexity",
    "href": "algo.html#big-o-notation-and-algorithm-complexity",
    "title": "2¬† Algorithm Design and Pseudocode",
    "section": "6.5 Big-O Notation and Algorithm Complexity",
    "text": "6.5 Big-O Notation and Algorithm Complexity\n\nBig-O Notation Example: \\(T(n) = O(n^2)\\)\n\nExplanation: This formula describes the time complexity of an algorithm, where\\(T(n)\\) represents the runtime as a function of input size \\(n\\). The notation \\(O(n^2)\\) indicates that the algorithm‚Äôs runtime grows quadratically with the size of the input.\n\nLogarithmic Complexity: \\(T(n) = O(\\log n)\\)\n\nExplanation: This formula represents an algorithm with logarithmic time complexity. The runtime increases logarithmically as the input size \\(n\\) increases, which is common in algorithms like binary search.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#basic-tsp-formulation",
    "href": "algo.html#basic-tsp-formulation",
    "title": "2¬† Algorithm Design and Pseudocode",
    "section": "6.6 Basic TSP Formulation",
    "text": "6.6 Basic TSP Formulation\n\nObjective Function: \\[\n\\text{Minimize} \\quad Z = \\sum_{i=1}^{n} \\sum_{j=1, j \\neq i}^{n} c_{ij} x_{ij}\n\\]\n\nExplanation: This formula represents the objective function of the TSP, where \\(c_{ij}\\) is the cost (or distance) of traveling from city \\(i\\) to city \\(j\\), and \\(x_{ij}\\) is a binary variable that equals 1 if the path from \\(i\\) to \\(j\\) is included in the solution and 0 otherwise. The goal is to minimize the total travel cost.\n\nConstraints:\n\\(\\sum_{j=1, j \\neq i}^{n} x_{ij} = 1 \\quad \\forall i\\)\n\\(\\sum_{i=1, i \\neq j}^{n} x_{ij} = 1 \\quad \\forall j\\)\n\\(x_{ij} \\in \\{0, 1\\}\\)\n\nExplanation: The first constraint ensures that each city \\(i\\) is exited exactly once, and the second constraint ensures that each city \\(j\\) is entered exactly once. The binary constraint on \\(x_{ij}\\) ensures that the solution only includes valid paths.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#tsp-approximation-algorithm",
    "href": "algo.html#tsp-approximation-algorithm",
    "title": "2¬† Algorithm Design and Pseudocode",
    "section": "6.7 TSP Approximation Algorithm",
    "text": "6.7 TSP Approximation Algorithm\n\nApproximation Algorithm Cost: \\(Z \\leq 2 \\times \\text{OPT}\\)\n\nExplanation: This formula represents the performance guarantee of a 2-approximation algorithm for the TSP, where \\(Z\\) is the cost of the approximate solution and \\(\\text{OPT}\\) is the cost of the optimal solution. It guarantees that the approximate solution will be at most twice as costly as the optimal solution.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#heuristic-nearest-neighbor-example",
    "href": "algo.html#heuristic-nearest-neighbor-example",
    "title": "2¬† Algorithm Design and Pseudocode",
    "section": "6.8 Heuristic Nearest Neighbor Example:",
    "text": "6.8 Heuristic Nearest Neighbor Example:\n\\(Z = \\sum_{i=1}^{n-1} c_{i, \\text{NN}(i)} + c_{n, \\text{NN}(1)}\\)\n\nExplanation: This is the cost calculation for the nearest neighbor heuristic, where \\(\\text{NN}(i)\\) denotes the nearest neighbor of city \\(i\\). The tour starts at a city, repeatedly visits the nearest unvisited city, and returns to the starting city.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#tips-for-writing-good-pseudocode",
    "href": "algo.html#tips-for-writing-good-pseudocode",
    "title": "2¬† Algorithm Design and Pseudocode",
    "section": "7.1 Tips for Writing Good Pseudocode",
    "text": "7.1 Tips for Writing Good Pseudocode\n\nMaintain consistent terms throughout: Use the same terminology for variables, functions, and processes. If you define a variable as total, always refer to it as total, not sum later on.\nBe clear on naming: Use descriptive names for variables, functions, and operations to make your pseudocode intuitive.\nPseudocode should be easy to understand after a single read-through. If it feels too complex, break it down further.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#pseudocode-for-basic-tsp-formation",
    "href": "algo.html#pseudocode-for-basic-tsp-formation",
    "title": "2¬† Algorithm Design and Pseudocode",
    "section": "7.2 Pseudocode for Basic TSP Formation",
    "text": "7.2 Pseudocode for Basic TSP Formation\n\nGeneric Inputs: The pseudocode is now generic and works for any TSP model where you have a set of cities and their corresponding distances.\nNearest Neighbor Heuristic: The algorithm works by greedily choosing the nearest unvisited city until all cities are visited, then returning to the start city.\nOutput: It outputs the route and the total distance traveled.\n\n#inputs\nSET cities = &lt;list of cities&gt;\nSET distances = &lt;distance matrix or dictionary&gt;\nSET start_city = &lt;initial city&gt;\n\n\n# FUNCTION to find the nearest unvisited city\nFUNCTION find_nearest_neighbor(current_city, unvisited, distances):\n    SET nearest_city = None\n    SET min_distance = infinity\n    \n    FOR each city IN unvisited:\n        IF distances[current_city][city] &lt; min_distance:\n            SET min_distance = distances[current_city][city]\n            SET nearest_city = city\n    \n    RETURN nearest_city, min_distance\n\n# FUNCTION to solve TSP using Nearest Neighbor algorithm\nFUNCTION nearest_neighbor_tsp(start_city, cities, distances):\n# Initialize list of unvisited cities\n    SET unvisited = list of all cities EXCEPT start_city",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#algorithms-key-concepts-and-examples",
    "href": "algo.html#algorithms-key-concepts-and-examples",
    "title": "2¬† Algorithm Design and Pseudocode",
    "section": "3.1 Algorithms Key Concepts and Examples",
    "text": "3.1 Algorithms Key Concepts and Examples\n\nAlgorithms in Finance: High-Frequency Trading (HFT) revolutionized Wall Street by executing trades at lightning speeds, leading to both massive profits and new risks.\nAlgorithms in Healthcare: Algorithms that diagnose diseases faster and more accurately than doctors.\nMusic: Algorithms used by platforms like Pandora to predict and recommend songs.\nImpact on Jobs: Automation‚Äôs role in replacing jobs traditionally done by humans, particularly in industries like finance, journalism, and even art.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#future-implications-and-ethical-considerations",
    "href": "algo.html#future-implications-and-ethical-considerations",
    "title": "2¬† Algorithm Design and Pseudocode",
    "section": "3.2 Future Implications and Ethical Considerations",
    "text": "3.2 Future Implications and Ethical Considerations\n\nExpansion of Algorithms: The growing reach of algorithms in decision-making processes, from hiring practices to legal judgments.\nEthical Concerns: The potential for bias in algorithms and the importance of transparency in their design. The need for regulation and oversight as algorithms increasingly influence critical aspects of life.\nLooking Ahead: Steiner‚Äôs call for society to adapt to the new algorithm-driven world, balancing innovation with ethical responsibility.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#bias-in-algorithms-explored",
    "href": "algo.html#bias-in-algorithms-explored",
    "title": "2¬† Algorithm Design and Pseudocode",
    "section": "3.3 Bias in Algorithms Explored",
    "text": "3.3 Bias in Algorithms Explored\n\nFacial Recognition Technology algorithms have shown significant biases, particularly in accurately identifying people of different races and genders.\n\nStudies have found that some facial recognition systems have higher error rates when identifying individuals with darker skin tones and women. This can lead to discriminatory outcomes, such as misidentifying people of color at higher rates than white individuals.\nImpact: This bias can result in wrongful accusations or the exclusion of certain groups from services that rely on facial recognition technology.\n\nHiring algorithms are used by companies to screen job applicants, but they can unintentionally perpetuate biases present in the training data.\n\nA famous case involved an AI hiring tool developed by Amazon, which was found to be biased against female applicants. The algorithm was trained on resumes submitted over the previous decade, which were predominantly from male applicants, leading the AI to favor male candidates.\nImpact: This bias can reinforce gender inequalities in the workplace by systematically disadvantaging qualified female applicants.\n\nPredictive Policing algorithms analyze historical crime data to predict where future crimes are likely to occur, influencing law enforcement patrols.\n\nThese algorithms often reflect existing biases in policing practices, such as disproportionately targeting minority neighborhoods. Because the training data may contain biased policing patterns, the algorithm can perpetuate over-policing in certain communities.\nImpact: This can lead to a cycle of increased surveillance and criminalization of specific racial or ethnic groups, reinforcing systemic biases in the criminal justice system.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#algorithm-development-process",
    "href": "algo.html#algorithm-development-process",
    "title": "2¬† Algorithm Design and Pseudocode",
    "section": "4.1 Algorithm Development Process:",
    "text": "4.1 Algorithm Development Process:\n\nWhat should you do first?\n\n0: Think of a conceptual approach 1: Write step-by-step outline + In words (maybe pseudo code) + Focus on sound logic 2: Plan programming implementation + Choose appropriate data structures + Speed, memory, convenience + Consider functions, which kind of loops 3: Write the program and debug + Use the outline as comment statements + Write your algorithms in chunks, write a line, then test it\n\n\n\nProcess",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#understanding-the-structure-of-algorithms",
    "href": "algo.html#understanding-the-structure-of-algorithms",
    "title": "2¬† Algorithm Design and Pseudocode",
    "section": "4.2 Understanding the Structure of Algorithms",
    "text": "4.2 Understanding the Structure of Algorithms\n\nIdentify the Purpose:\n\nStart by understanding what the algorithm is supposed to achieve.\nLook for a brief description or goal at the beginning.\n\nBreak Down the Steps:\n\nAlgorithms are typically presented as a sequence of steps.\nEach step corresponds to a specific action or decision.\nin words. Focus on sound logic.\n\nRecognize Input and Output:\n\nDetermine what inputs the algorithm requires.\nIdentify the expected output(s) after the algorithm is executed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#understanding-the-structure-of-algorithms-flow-control",
    "href": "algo.html#understanding-the-structure-of-algorithms-flow-control",
    "title": "2¬† Algorithm Design and Pseudocode",
    "section": "4.3 Understanding the Structure of Algorithms: Flow Control",
    "text": "4.3 Understanding the Structure of Algorithms: Flow Control\n\nDetermine how the algorithm progresses through its steps and notice the flow control structures like loops (for, while) and conditionals (if, else).\n\nWhich data types?\n\nBegin to consider speed, memory usage, convenience tradeoffs\n\nFiner details of data organization\n\nWhat fields for dictionary labels?\nHow should lists be sorted, if at all?\n\nLoop types\n\nfor or while?\nEase of coding and development\n\nUse functions?",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#deciphering-algorithmic-notation-pseudocode-vs.-code",
    "href": "algo.html#deciphering-algorithmic-notation-pseudocode-vs.-code",
    "title": "2¬† Algorithm Design and Pseudocode",
    "section": "5.1 Deciphering Algorithmic Notation: Pseudocode vs.¬†Code",
    "text": "5.1 Deciphering Algorithmic Notation: Pseudocode vs.¬†Code\n\nRecognize that many algorithms are written in pseudocode, a high-level description that isn‚Äôt tied to any specific programming language.\nSome feel that with Python, the pseudo-code step would not be necessary anymore\nTranslate pseudocode into actual code if needed.\n\nProgram in chunks\nTest each chunk before moving on\nUse functions where reasonable\n\nEasier to test\n\n\nCheck code where solutions are known\n\nSmall problems\nObvious solutions\n\nCheck code as you write it\nDebug Often\n\nBreakpoints\nCheck variables in console\nVariable explorer\nPrint statements",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#deciphering-algorithmic-notation-mathematical-symbols",
    "href": "algo.html#deciphering-algorithmic-notation-mathematical-symbols",
    "title": "2¬† Algorithm Design and Pseudocode",
    "section": "5.2 Deciphering Algorithmic Notation: Mathematical Symbols",
    "text": "5.2 Deciphering Algorithmic Notation: Mathematical Symbols\n\nMathematical Symbols:\n\nAlgorithms often include mathematical notation, such as sums (\\(\\sum\\)) or products (\\(\\prod\\)).\nUnderstand these as they relate to the algorithm‚Äôs operations.\n\nBig-O Notation:\n\nLook for references to Big-O notation, which indicates the algorithm‚Äôs efficiency in terms of time or space.\nUnderstand the implications for performance, especially with large inputs.\n\nCommenting\n\nCommenting improves code readability, reuse, and maintainability.\nCan transfer algorithm outline to a program as comments.\nThen, you have comments that provide an outline for coding.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#design-paradigms",
    "href": "algo.html#design-paradigms",
    "title": "2¬† Algorithm Design and Pseudocode",
    "section": "5.3 Design Paradigms",
    "text": "5.3 Design Paradigms\n\nAlgorithms can be based on:\n\nIntuitive ideas\nMath\n\nOptimization\n\nPossibly with calculus analysis (e.g., gradients)\nHeuristic model analysis\nDynamic programming",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#common-symbols-in-algorithms",
    "href": "algo.html#common-symbols-in-algorithms",
    "title": "2¬† Algorithm Design and Pseudocode",
    "section": "6.1 Common Symbols in Algorithms",
    "text": "6.1 Common Symbols in Algorithms\n\nMathematical Symbols:\n\n\\(\\forall\\): ‚ÄúFor all‚Äù, used in universal quantification.\n\\(\\exists\\): ‚ÄúThere exists‚Äù, used in existential quantification.\n\\(\\sum_{i=1}^n x_i\\): Summation from \\(i = 1\\) to \\(n\\).\n\\(\\prod_{i=1}^n x_i\\): Product from \\(i = 1\\) to \\(n\\).\n\nLogical Symbols:\n\n\\(\\land\\): Logical AND.\n\\(\\lor\\): Logical OR.\n\\(\\neg\\): Logical NOT.\n\\(\\implies\\): Logical implication.\n\nAlgorithm-Specific Notation:\n\n\\(O(n)\\): Big-O notation, representing algorithm complexity.\n\\(P \\leftarrow Q\\): Assign the value of \\(Q\\) to \\(P\\).\nfor \\(i = 1\\) to \\(n\\): A loop from \\(i = 1\\) to \\(n\\).\nif \\((condition)\\): A conditional statement.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "algo.html#examples-of-math-notation",
    "href": "algo.html#examples-of-math-notation",
    "title": "2¬† Algorithm Design and Pseudocode",
    "section": "6.2 Examples of Math Notation",
    "text": "6.2 Examples of Math Notation",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Algorithm Design and Pseudocode</span>"
    ]
  },
  {
    "objectID": "numpy.html#importing-numpy",
    "href": "numpy.html#importing-numpy",
    "title": "3¬† Python for Heuristics & NumPy",
    "section": "8.1 Importing Numpy",
    "text": "8.1 Importing Numpy\n\nimport numpy as np",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#numpy-examples",
    "href": "numpy.html#numpy-examples",
    "title": "3¬† Python for Heuristics & NumPy",
    "section": "9.1 Numpy Examples",
    "text": "9.1 Numpy Examples\n\n9.1.1 Basic Array Creation\n\n# Creating a simple numpy array from a Python list\narray = np.array([1, 2, 3, 4])\nprint(\"Array:\", array)\n\nArray: [1 2 3 4]\n\n\n\n\n9.1.2 Element-Wise Operations\n\nElement-wise operators are mathematical or logical operations applied independently to corresponding elements in arrays or matrices of the same shape.\nEach element in one array is combined with the corresponding element in the other array using the operator.\nIn the context of arrays (such as in NumPy), common element-wise operators include basic arithmetic operators:\n\nElement-wise addition +\nElement-wise subtraction -\nElement-wise multiplication *\nElement-wise division /\nElement-wise exponentiation **\n\n\n\n# Performing element-wise addition\narray = np.array([1, 2, 3, 4])\nadded_array = array + 5\nprint(\"Added Array:\", added_array)\n\nAdded Array: [6 7 8 9]\n\n\n\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\nc = a + b\nprint(c)\n\n[5 7 9]\n\n\n\n\n9.1.3 Taking an Exponent: np.exp\n\nnp.exp is a function in the NumPy library that calculates the exponential of all elements in an input array. Specifically, it computes the base-e exponential function, which is ùëí^ùë•, where ùëí is Euler‚Äôs number (approximately 2.71828), and ùë• is the input array or scalar.\n\n\n# Applying np.exp to the array\narray = np.array([1, 2, 3, 4])\nexp_array = np.exp(array)\nprint(\"Exponential Array:\", exp_array)\n\nExponential Array: [ 2.71828183  7.3890561  20.08553692 54.59815003]\n\n\n\nnp.exp from Simulated Annealing example\nThis function is part of a Simulated Annealing algorithm, specifically handling the temperature decay mechanism to decide whether to accept a new solution, even if it‚Äôs worse than the current one. Here‚Äôs a breakdown of the function based on the np.exp command and the logic:\n\ntmp_obj_val: The objective value of a new (temporary) solution.\nobj_val: The objective value of the current solution. temperature: The current temperature in the simulated annealing process, which controls how likely the algorithm is to accept worse solutions.\nA random number r between 0 and 1 is generated. This represents a threshold for whether the new solution will be accepted using random.rand()\nThe probability p of accepting the new solution is computed using the exponential function.\nIf the random value r is less than the calculated probability p, the function returns True, meaning the new solution is accepted (even if it‚Äôs worse). If r is greater than p, the new solution is rejected, and the current solution is maintained.\n\nThe function decides whether to accept a new solution in simulated annealing, balancing exploration and exploitation based on the temperature and objective values of the solutions. The np.exp() function ensures that worse solutions have a chance to be accepted, particularly early in the process, fostering a broader search space.\n\n\n# Simulated annealing temperature decay\ndef determine(self, tmp_obj_val, obj_val, temperature):\n     r = np.random.rand()\n     p = np.exp((tmp_obj_val - obj_val) / temperature)\n     return r &lt; p\n\n\n\n9.1.4 Taking a square root: np.sqrt()\n\nnp.sqrt is a function in NumPy that returns the non-negative square root of an element-wise input array. It operates on each element of the array and computes the square root.\n\n\n# Applying np.sqrt to the array\nsqrt_array = np.sqrt(array)\nprint(\"Square Root Array:\", sqrt_array)\n\nSquare Root Array: [1.         1.41421356 1.73205081 2.        ]\n\n\n\nThe Ackley function is commonly used as a benchmark problem in optimization, and is known for its many local minima. The Ackley function uses the np.sqrt within its formula.\n\n\ndef ackley(s):\n     a, b, c = 20, 0.2, 2 * np.pi\n     n = len(s)\n     sum_sq_term = np.sum(s**2)\n     cos_term = np.sum(np.cos(c * s))\n     term1 = -a * np.exp(-b * np.sqrt(sum_sq_term / n))\n     term2 = -np.exp(cos_term / n)\n     return term1 + term2 + a + np.e",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#sorting-the-data-np.argsort",
    "href": "numpy.html#sorting-the-data-np.argsort",
    "title": "3¬† Python for Heuristics & NumPy",
    "section": "9.2 Sorting the data: np.argsort",
    "text": "9.2 Sorting the data: np.argsort\n\nIn various evolutionary algorithms (such as genetic algorithms or simulated annealing), selecting the most ‚Äúfit‚Äù or optimal solutions from a population is crucial for convergence toward the global optimum.\nBy sorting individuals based on fitness, the algorithm can efficiently identify the most promising candidates for further exploration (e.g., crossover, mutation) or intensify the search around high-quality solutions.\nThe use of np.argsort allows for a fast, reliable way to rank individuals, ensuring that the evolutionary process focuses on refining the best candidates and discarding those with lower potential.\n\n\n# Dummy population and fitness values\npopulation = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n\n# Assign dummy fitness values\nfitness = np.array([10, 30, 20, 40, 50]) \n\n# Sort population based on fitness\nindices = np.argsort(fitness)\nprint(indices) \nsorted_population = population[indices]\n\n# Select top 3 individuals\ntop_individuals = sorted_population[:3] \nprint(top_individuals)\n\n[0 2 1 3 4]\n[[1 2]\n [5 6]\n [3 4]]\n\n\n\n9.2.1 Selecting the Max: np.argmax\n\nFinding the Index of the Maximum Element in a 1D Array: The np.argmax function returns the index of the first occurrence of the maximum value in the array. In this case, the maximum value is 7, and it occurs at index 2.\n\n\narr = np.array([1, 3, 7, 2, 5])\nindex = np.argmax(arr)\nprint(\"Array:\", arr)\nprint(\"Index of max element:\", index)\nprint(\"Max element:\", arr[index])\n\nArray: [1 3 7 2 5]\nIndex of max element: 2\nMax element: 7\n\n\nArray: [1 3 7 2 5]\n\nUsing np.argmax with a 2D Array (Row-wise & Column-wise): np.argmax can work on multi-dimensional arrays. By specifying axis=0 or axis=1, you can find the maximum values column-wise or row-wise, respectively. For axis=0, you get the indices of the maximum elements for each column, and for axis=1, you get them for each row.\n\n\narr_2d = np.array([[1, 2, 3], [4, 5, 1], [0, 6, 2]])\n\n# Find the index of the max element in the flattened array\nmax_index_flat = np.argmax(arr_2d)\nprint(\"Flattened array index:\", max_index_flat)\n\nFlattened array index: 7\n\n\n\nNumber 6 is in index 7, starting at index 0 and counting up across each row.\n[[1 2 3]\n[4 5 1]\n[0 6 2]]\n\n\n# Find the index of the max element along each column (axis=0)\nmax_index_col = np.argmax(arr_2d, axis=0)\nprint(\"Max element index for each column:\", max_index_col)\n\nMax element index for each column: [1 2 0]\n\n\n\n4 is in index 1, 6 is in index 2, and 3 is in index 0, counting across each column starting at index 0. [[1 2 3]\n[4 5 1]\n[0 6 2]]\n\n\n# Find the index of the max element along each row (axis=1)\nmax_index_row = np.argmax(arr_2d, axis=1)\nprint(\"Max element index for each row:\", max_index_row)\n\nMax element index for each row: [2 1 1]\n\n\n\n3 is in index 2 in the row, 5 is in index 1, and 6 is in index 1, counting across each row starting at index 0. [[1 2 3]\n[4 5 1]\n[0 6 2]]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#random-number-generation",
    "href": "numpy.html#random-number-generation",
    "title": "3¬† Python for Heuristics & NumPy",
    "section": "9.3 Random Number Generation",
    "text": "9.3 Random Number Generation\n\nRandom numbers are key to both genetic algorithms (mutation, crossover) and simulated annealing (random perturbations). Basic example using np.random.rand() to generate uniform random numbers between 0 and 1.\n\n\nrand_nums = np.random.rand(5)\nprint(rand_nums)\n\n[0.49819701 0.57076305 0.3662303  0.20012644 0.55134872]\n\n\n\n9.3.1 Standard Normal Distribution\n\nGenerating random numbers from a standard normal distribution (mean=0, std=1).\n\n\n# Generating random values from the standard normal distribution\nrandom_values = np.random.standard_normal(5)\nprint(\"Random Standard Normal Values:\", random_values)\n\nRandom Standard Normal Values: [ 0.86594689  0.01918829 -2.04885417 -0.79720448 -0.52109404]\n\n\n\n\n9.3.2 np.random.uniform\n\nIn hill climbing, the algorithm often starts with a random solution. This can be simulated with np.random.uniform, which generates random numbers between a specified range.\n\n\n# Generate a random starting point for the hill climbing algorithm\nrandom_start = np.random.uniform(low=-10, high=10, size=5)\nprint(f\"Random start: {random_start}\")\n\nRandom start: [ 8.54616701  3.09601167  3.54704847  3.21511143 -3.07427524]\n\n\n\n\n9.3.3 np.random.randint\n\nnp.random.randint(low, high=None, size=None, dtype=int)\n\nlow: The lower boundary of the random integers (inclusive).\nhigh: The upper boundary of the random integers (exclusive). If not provided, random integers are generated between 0 and low.\nsize: The shape of the output array (optional). If not provided, a single integer is returned.\ndtype: The desired data type of the output array, by default int.\n\n\n\n# Generate 5 random integers between 10 and 20\nrandom_integers = np.random.randint(10, 20, size=5)\nprint(random_integers)\n\n[11 17 11 18 18]\n\n\n\n\n9.3.4 np.random.randint from Simulated Annealing\n\nThis function, transit(), is used to modify a solution sol as part of a heuristic search process, likely for algorithms like genetic algorithms, hill climbing, or simulated annealing. The goal is to explore the solution space by introducing a small, random change (or ‚Äútransition‚Äù) to the current solution.\n\nThe function takes a single argument, sol, which is likely a binary array or list (a list of 0s and 1s).\nt = sol.copy(): A copy of the solution sol is made, named t. This is important because we don‚Äôt want to modify the original solution directly; instead, we work on the copy t.\ni = np.random.randint(len(sol)): The randint function from NumPy is used to randomly select an index i between 0 and the length of sol - 1. This selects a random position in the solution array.\nt[i] ^= 1: This is a bitwise XOR operation. In the context of a binary solution (a list of 0s and 1s), it flips the value at index i:If t[i] is 0, it becomes 1.If t[i] is 1, it becomes 0. This operation introduces a small, random change to the solution by flipping one bit.\nreturn t: After flipping one bit, the modified solution t is returned.\n\n\n\n# Transition function (T)\ndef transit(sol):\n    new_sol = sol.copy()\n    index = np.random.randint(len(sol))\n    new_sol[index] = 1 - new_sol[index]  # Flip a random bit\n    return new_sol",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#numpy-definition",
    "href": "numpy.html#numpy-definition",
    "title": "3¬† Python for Heuristics & NumPy",
    "section": "8.1 Numpy Definition",
    "text": "8.1 Numpy Definition\n\nNumPy stands for numerical Python, suggesting that it targets scenarios that are numerically demanding. The base Python interpreter tries to be as general as possible in many areas, which often leads to quite a bit of overhead at run-time.\nNumPy uses specialization as its major approach to avoid overhead and to be as good and as fast as possible in certain application scenarios.\nVectorization is a powerful concept for writing concise, easy-to-read, and easy-to-maintain code in fields such as finance and algorithmic trading. With NumPy, vectorized code does not only make code more concise, but it also can speed up code execution considerably (by a factor of about eight in the Monte Carlo simulation, for example).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#basic-array-creation",
    "href": "numpy.html#basic-array-creation",
    "title": "3¬† Python for Heuristics & NumPy",
    "section": "8.2 Basic Array Creation",
    "text": "8.2 Basic Array Creation\n\n# Creating a simple numpy array from a Python list\narray = np.array([1, 2, 3, 4])\nprint(\"Array:\", array)\n\nArray: [1 2 3 4]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#element-wise-operations",
    "href": "numpy.html#element-wise-operations",
    "title": "3¬† Python for Heuristics & NumPy",
    "section": "8.3 Element-Wise Operations",
    "text": "8.3 Element-Wise Operations\n\nElement-wise operators are mathematical or logical operations applied independently to corresponding elements in arrays or matrices of the same shape.\nEach element in one array is combined with the corresponding element in the other array using the operator.\nIn the context of arrays (such as in NumPy), common element-wise operators include basic arithmetic operators:\n\nElement-wise addition +\nElement-wise subtraction -\nElement-wise multiplication *\nElement-wise division /\nElement-wise exponentiation **\n\n\n\n# Performing element-wise addition\narray = np.array([1, 2, 3, 4])\nadded_array = array + 5\nprint(\"Added Array:\", added_array)\n\nAdded Array: [6 7 8 9]\n\n\n\na = np.array([1, 2, 3])\nb = np.array([4, 5, 6])\nc = a + b\nprint(c)\n\n[5 7 9]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#taking-an-exponent-np.exp",
    "href": "numpy.html#taking-an-exponent-np.exp",
    "title": "3¬† Python for Heuristics & NumPy",
    "section": "8.4 Taking an Exponent: np.exp",
    "text": "8.4 Taking an Exponent: np.exp\n\nnp.exp is a function in the NumPy library that calculates the exponential of all elements in an input array. Specifically, it computes the base-e exponential function, which is ùëí^ùë•, where ùëí is Euler‚Äôs number (approximately 2.71828), and ùë• is the input array or scalar.\n\n\n# Applying np.exp to the array\narray = np.array([1, 2, 3, 4])\nexp_array = np.exp(array)\nprint(\"Exponential Array:\", exp_array)\n\nExponential Array: [ 2.71828183  7.3890561  20.08553692 54.59815003]\n\n\n\nnp.exp from Simulated Annealing example\nThis function is part of a Simulated Annealing algorithm, specifically handling the temperature decay mechanism to decide whether to accept a new solution, even if it‚Äôs worse than the current one. Here‚Äôs a breakdown of the function based on the np.exp command and the logic:\n\ntmp_obj_val: The objective value of a new (temporary) solution.\nobj_val: The objective value of the current solution. temperature: The current temperature in the simulated annealing process, which controls how likely the algorithm is to accept worse solutions.\nA random number r between 0 and 1 is generated. This represents a threshold for whether the new solution will be accepted using random.rand()\nThe probability p of accepting the new solution is computed using the exponential function.\nIf the random value r is less than the calculated probability p, the function returns True, meaning the new solution is accepted (even if it‚Äôs worse). If r is greater than p, the new solution is rejected, and the current solution is maintained.\n\nThe function decides whether to accept a new solution in simulated annealing, balancing exploration and exploitation based on the temperature and objective values of the solutions. The np.exp() function ensures that worse solutions have a chance to be accepted, particularly early in the process, fostering a broader search space.\n\n\n# Simulated annealing temperature decay\ndef determine(self, tmp_obj_val, obj_val, temperature):\n     r = np.random.rand()\n     p = np.exp((tmp_obj_val - obj_val) / temperature)\n     return r &lt; p",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#taking-a-square-root-np.sqrt",
    "href": "numpy.html#taking-a-square-root-np.sqrt",
    "title": "3¬† Python for Heuristics & NumPy",
    "section": "8.5 Taking a square root: np.sqrt()",
    "text": "8.5 Taking a square root: np.sqrt()\n\nnp.sqrt is a function in NumPy that returns the non-negative square root of an element-wise input array. It operates on each element of the array and computes the square root.\n\n\n# Applying np.sqrt to the array\nsqrt_array = np.sqrt(array)\nprint(\"Square Root Array:\", sqrt_array)\n\nSquare Root Array: [1.         1.41421356 1.73205081 2.        ]\n\n\n\nThe Ackley function is commonly used as a benchmark problem in optimization, and is known for its many local minima. The Ackley function uses the np.sqrt within its formula.\n\n\ndef ackley(s):\n     a, b, c = 20, 0.2, 2 * np.pi\n     n = len(s)\n     sum_sq_term = np.sum(s**2)\n     cos_term = np.sum(np.cos(c * s))\n     term1 = -a * np.exp(-b * np.sqrt(sum_sq_term / n))\n     term2 = -np.exp(cos_term / n)\n     return term1 + term2 + a + np.e",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#selecting-the-max-np.argmax",
    "href": "numpy.html#selecting-the-max-np.argmax",
    "title": "3¬† Python for Heuristics & NumPy",
    "section": "9.1 Selecting the Max: np.argmax",
    "text": "9.1 Selecting the Max: np.argmax\n\nFinding the Index of the Maximum Element in a 1D Array: The np.argmax function returns the index of the first occurrence of the maximum value in the array. In this case, the maximum value is 7, and it occurs at index 2.\n\n\narr = np.array([1, 3, 7, 2, 5])\nindex = np.argmax(arr)\nprint(\"Array:\", arr)\nprint(\"Index of max element:\", index)\nprint(\"Max element:\", arr[index])\n\nArray: [1 3 7 2 5]\nIndex of max element: 2\nMax element: 7\n\n\nArray: [1 3 7 2 5]\n\nUsing np.argmax with a 2D Array (Row-wise & Column-wise): np.argmax can work on multi-dimensional arrays. By specifying axis=0 or axis=1, you can find the maximum values column-wise or row-wise, respectively. For axis=0, you get the indices of the maximum elements for each column, and for axis=1, you get them for each row.\n\n\narr_2d = np.array([[1, 2, 3], [4, 5, 1], [0, 6, 2]])\n\n# Find the index of the max element in the flattened array\nmax_index_flat = np.argmax(arr_2d)\nprint(\"Flattened array index:\", max_index_flat)\n\nFlattened array index: 7\n\n\n\nNumber 6 is in index 7, starting at index 0 and counting up across each row.\n[[1 2 3]\n[4 5 1]\n[0 6 2]]\n\n\n# Find the index of the max element along each column (axis=0)\nmax_index_col = np.argmax(arr_2d, axis=0)\nprint(\"Max element index for each column:\", max_index_col)\n\nMax element index for each column: [1 2 0]\n\n\n\n4 is in index 1, 6 is in index 2, and 3 is in index 0, counting across each column starting at index 0. [[1 2 3]\n[4 5 1]\n[0 6 2]]\n\n\n# Find the index of the max element along each row (axis=1)\nmax_index_row = np.argmax(arr_2d, axis=1)\nprint(\"Max element index for each row:\", max_index_row)\n\nMax element index for each row: [2 1 1]\n\n\n\n3 is in index 2 in the row, 5 is in index 1, and 6 is in index 1, counting across each row starting at index 0. [[1 2 3]\n[4 5 1]\n[0 6 2]]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#standard-normal-distribution",
    "href": "numpy.html#standard-normal-distribution",
    "title": "3¬† Python for Heuristics & NumPy",
    "section": "10.1 Standard Normal Distribution",
    "text": "10.1 Standard Normal Distribution\n\nGenerating random numbers from a standard normal distribution (mean=0, std=1).\n\n\n# Generating random values from the standard normal distribution\nrandom_values = np.random.standard_normal(5)\nprint(\"Random Standard Normal Values:\", random_values)\n\nRandom Standard Normal Values: [-0.74355748 -0.39906898 -0.98825145  0.1670147   0.70464814]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#np.random.uniform",
    "href": "numpy.html#np.random.uniform",
    "title": "3¬† Python for Heuristics & NumPy",
    "section": "10.2 np.random.uniform",
    "text": "10.2 np.random.uniform\n\nIn hill climbing, the algorithm often starts with a random solution. This can be simulated with np.random.uniform, which generates random numbers between a specified range.\n\n\n# Generate a random starting point for the hill climbing algorithm\nrandom_start = np.random.uniform(low=-10, high=10, size=5)\nprint(f\"Random start: {random_start}\")\n\nRandom start: [ 5.58221408  8.47507526  3.86677417 -4.54529836  0.94077938]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#np.random.randint",
    "href": "numpy.html#np.random.randint",
    "title": "3¬† Python for Heuristics & NumPy",
    "section": "10.3 np.random.randint",
    "text": "10.3 np.random.randint\n\nnp.random.randint(low, high=None, size=None, dtype=int)\n\nlow: The lower boundary of the random integers (inclusive).\nhigh: The upper boundary of the random integers (exclusive). If not provided, random integers are generated between 0 and low.\nsize: The shape of the output array (optional). If not provided, a single integer is returned.\ndtype: The desired data type of the output array, by default int.\n\n\n\n# Generate 5 random integers between 10 and 20\nrandom_integers = np.random.randint(10, 20, size=5)\nprint(random_integers)\n\n[12 15 15 19 10]",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#np.random.randint-from-simulated-annealing",
    "href": "numpy.html#np.random.randint-from-simulated-annealing",
    "title": "3¬† Python for Heuristics & NumPy",
    "section": "10.4 np.random.randint from Simulated Annealing",
    "text": "10.4 np.random.randint from Simulated Annealing\n\nThis function, transit(), is used to modify a solution sol as part of a heuristic search process, likely for algorithms like genetic algorithms, hill climbing, or simulated annealing. The goal is to explore the solution space by introducing a small, random change (or ‚Äútransition‚Äù) to the current solution.\n\nThe function takes a single argument, sol, which is likely a binary array or list (a list of 0s and 1s).\nt = sol.copy(): A copy of the solution sol is made, named t. This is important because we don‚Äôt want to modify the original solution directly; instead, we work on the copy t.\ni = np.random.randint(len(sol)): The randint function from NumPy is used to randomly select an index i between 0 and the length of sol - 1. This selects a random position in the solution array.\nt[i] ^= 1: This is a bitwise XOR operation. In the context of a binary solution (a list of 0s and 1s), it flips the value at index i:If t[i] is 0, it becomes 1.If t[i] is 1, it becomes 0. This operation introduces a small, random change to the solution by flipping one bit.\nreturn t: After flipping one bit, the modified solution t is returned.\n\n\n# Transition function (T)\ndef transit(sol):\n    new_sol = sol.copy()\n    index = np.random.randint(len(sol))\n    new_sol[index] = 1 - new_sol[index]  # Flip a random bit\n    return new_sol",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "numpy.html#comparing-model-clock-time-withwithout-numpy",
    "href": "numpy.html#comparing-model-clock-time-withwithout-numpy",
    "title": "3¬† Python for Heuristics & NumPy",
    "section": "13.1 Comparing Model Clock Time With/Without NumPy",
    "text": "13.1 Comparing Model Clock Time With/Without NumPy\n\n13.1.1 Without Numpy\n\nimport random\nfrom math import exp, sqrt\nimport time \n# Initial stock price\nS0 = 100 \n\n# Risk-free rate\nr = 0.05 \n\n# Time horizon (1 year)\nT = 1.0 \n\n# Volatility\nsigma = 0.2 \n\nvalues = []  \n\n# Start tracking wall time and CPU time\nstart_wall_time = time.time()\nstart_cpu_time = time.process_time()\n\nfor _ in range(1000000):  \n     ST = S0 * exp((r - 0.5 * sigma ** 2) * T +\n        sigma * random.gauss(0, 1) * sqrt(T))  \n     values.append(ST)  \n\n# End tracking wall time and CPU time\nend_wall_time = time.time()\nend_cpu_time = time.process_time()\n\n# Calculate time differences\nwall_time = end_wall_time - start_wall_time\ncpu_time = end_cpu_time - start_cpu_time\n\n# Print timing information\nprint(f\"CPU times: {cpu_time:.2f} s (user and sys combined)\")\nprint(f\"Wall time: {wall_time:.2f} s\")\n\nCPU times: 0.47 s (user and sys combined)\nWall time: 0.70 s\n\n\n\n\n13.1.2 With Numpy\n\nimport numpy as np\nimport time \n\n# Initial stock price\nS0 = 100 \n\n# Risk-free rate\nr = 0.05 \n\n# Time horizon (1 year)\nT = 1.0 \n\n# Volatility\nsigma = 0.2 \n\n# Start tracking wall time and CPU time\nstart_wall_time = time.time()\nstart_cpu_time = time.process_time()\n\nST = S0 * np.exp((r - 0.5 * sigma ** 2) * T +\n    sigma * np.random.standard_normal(1000000) * np.sqrt(T))\n\n# End tracking wall time and CPU time\nend_wall_time = time.time()\nend_cpu_time = time.process_time()\n\n# Calculate time differences\nwall_time = end_wall_time - start_wall_time\ncpu_time = end_cpu_time - start_cpu_time\n\n# Print timing information\nprint(f\"CPU times: {cpu_time:.2f} s (user and sys combined)\")\nprint(f\"Wall time: {wall_time:.2f} s\")\n\nCPU times: 0.00 s (user and sys combined)\nWall time: 0.02 s",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Python for Heuristics & NumPy</span>"
    ]
  },
  {
    "objectID": "greedy.html#optimization-function-other-definitions",
    "href": "greedy.html#optimization-function-other-definitions",
    "title": "4¬† Introduction to Greedy Algorithms",
    "section": "5.1 Optimization Function: Other Definitions",
    "text": "5.1 Optimization Function: Other Definitions\n\nDef 2: The optimal solution is a solution, out of all feasible candidate solutions of the optimization problem \\(P\\), that gives the optimal value. \\(f(s^*) = \\operatorname{opt} \\{f(s)\\}, \\, \\forall \\, c_i(s) \\, \\odot \\, b_i, \\, i = 1, 2, \\dots, m.\\)\nDef 3: If the optimal solution \\(s*\\) for the problem ùëÉ exists, then the optimal \\(f(s)\\) is defined as \\(min_{s \\in A} f(s)\\), subject to \\(\\forall c_i(s) \\odot b_i\\),\nwhile the maximization problem of maximizing \\(f(s)\\) subject to some constraints can be defined as \\(max_{s \\in A} f(s)\\), subject to \\(\\forall c_i(s) \\odot b_i\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "greedy.html#definition-1",
    "href": "greedy.html#definition-1",
    "title": "4¬† Introduction to Greedy Algorithms",
    "section": "5.1 Definition 1",
    "text": "5.1 Definition 1\n\nAn optimization problem \\(P\\) is to find the optimal value, possibly subject to some constraints, out of all possible solutions.\nContains the objective function, constraint(s), and solution.\n\\(opt_{s \\in A} f(s)\\) subject to \\(\\forall c_i(s) \\odot b_i, i=i, 2, ...,m\\) where\nopt is either min (for minimization) or max (for maximization),\n\ns is a candidate solution\nA and B are the domain and codomain of the problem Image, namely, A is the set of all possible solutions and B is the set of all possible outcomes of the objective function,\n\\(c_i(s) \\odot b_i\\) is the constraint, and\n\\(f(s): A-&gt;B\\) is the objective function\n\\(\\odot\\) is \\(&gt;, &lt;, =, &lt;=, &gt;=\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "greedy.html#definition-2",
    "href": "greedy.html#definition-2",
    "title": "4¬† Introduction to Greedy Algorithms",
    "section": "5.2 Definition 2",
    "text": "5.2 Definition 2\n\nThe optimal solution is a solution, out of all feasible candidate solutions of the optimization problem \\(P\\), that gives the optimal value. \\(f(s^*) = \\operatorname{opt} \\{f(s)\\}, \\, \\forall \\, c_i(s) \\, \\odot \\, b_i, \\, i = 1, 2, \\dots, m.\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "greedy.html#definition-3",
    "href": "greedy.html#definition-3",
    "title": "4¬† Introduction to Greedy Algorithms",
    "section": "5.3 Definition 3",
    "text": "5.3 Definition 3\n\nIf the optimal solution \\(s*\\) for the problem ùëÉ exists, then the optimal \\(f(s)\\) is defined as \\(min_{s \\in A} f(s)\\), subject to \\(\\forall c_i(s) \\odot b_i\\),\nWhile the maximization problem of maximizing \\(f(s)\\) subject to some constraints can be defined as \\(max_{s \\in A} f(s)\\), subject to \\(\\forall c_i(s) \\odot b_i\\)",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "greedy.html#simple-example-of-greedy-algorithm-in-action",
    "href": "greedy.html#simple-example-of-greedy-algorithm-in-action",
    "title": "4¬† Introduction to Greedy Algorithms",
    "section": "6.1 Simple Example of Greedy Algorithm in Action",
    "text": "6.1 Simple Example of Greedy Algorithm in Action\n\nThe Coin Change Problem\nGiven a set of coin denominations and a target amount, find the minimum number of coins that add up to the target amount.\nGreedy Strategy: At each step, pick the largest denomination that doesn‚Äôt exceed the remaining amount.\n\n\\[\\operatorname{coins\\_used}(A) = \\sum_{i=1}^{n} \\left\\lfloor \\frac{A}{c_i} \\right\\rfloor \\times c_i \\quad \\operatorname{where} \\quad A = A - \\left\\lfloor \\frac{A}{c_i} \\right\\rfloor \\times c_i\\]\nWhere \\(\\left\\lfloor \\frac{A}{c_i} \\right\\rfloor\\) is the number of coins of denomination \\(c_i\\) is used.\n\\(A\\) is reduced by the value \\(\\left\\lfloor \\frac{A}{c_i} \\right\\rfloor \\times c_i\\)\nAfter using as many \\(c_i\\) denomination coins as possible. The process continues until \\(A=0\\), at which point the minimum number of coins required to make the total amount is found.\n\ndef greedy_coin_change(coins, amount):\n    coins.sort(reverse=True)\n    result = []\n    for coin in coins:\n        while amount &gt;= coin:\n            amount -= coin\n            result.append(coin)\n    \n    # Print the coins used\n    print(f\"Coins used: {result}\")\n    \n    # Return the number of coins used\n    return len(result)\n\n# Get user input (Put in 70 to show answer, but can request information from user)\namount = 70\n# amount = int(input(\"Enter the amount: \"))\n\n# Coin denominations\ncoins = [1, 5, 10, 25]\n\n# Calculate the solution\nnum_coins = greedy_coin_change(coins, amount)\n\nprint(f\"Minimum number of coins needed: {num_coins}\")\n\nCoins used: [25, 25, 10, 10]\nMinimum number of coins needed: 4",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "greedy.html#discrete-vs-continuous",
    "href": "greedy.html#discrete-vs-continuous",
    "title": "4¬† Introduction to Greedy Algorithms",
    "section": "7.1 Discrete vs Continuous",
    "text": "7.1 Discrete vs Continuous\n\n\n\nDiscrete vs Continuous\n\n\n\n7.1.1 Discrete\n\nDomain: The set of all possible input values for a function.\nCodomain: The set of all potential output values that the function can map to.\nAn objective function is a function that is being optimized (maximized or minimized) in a given problem. It takes an input from the domain and produces an output in the codomain.\nGiven two sets \\(A\\) and \\(B\\), and an objective function \\(f\\), we can understand how the function maps elements from the domain \\(A\\) to the codomain \\(B\\).\n\n\n\n7.1.2 Continuous\n\nShows the relationship between the angle Œ∏ and the value of sin‚Å°(Œ∏) at specific points. This relationship arises from the trigonometric sine function, which describes a wave-like pattern that oscillates between -1 and 1.\nŒ∏ represents the angle, typically in radians, and the values given (0, 0.25\\(\\pi\\), 0.50\\(\\pi\\), etc.) are specific points along the unit circle.\nsin(Œ∏) represents the sine of the angle Œ∏, which is the y-coordinate of the corresponding point on the unit circle.\nThe values provided in the table correspond to these properties of the sine function. The function gradually increases from 0 to 1, then decreases back to 0, then continues to -1, and finally returns to 0, completing one full cycle.",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "greedy.html#traveling-salesman-algorithm",
    "href": "greedy.html#traveling-salesman-algorithm",
    "title": "4¬† Introduction to Greedy Algorithms",
    "section": "8.1 Traveling Salesman Algorithm",
    "text": "8.1 Traveling Salesman Algorithm\n\\[\\min_{s \\in \\S_{\\pi}} f(s) = \\left[ \\sum_{i=1}^{n-1} d\\left(c_{\\pi(i)}, c_{\\pi(i+1)}\\right) \\right] + d\\left(c_{\\pi(n)}, c_{\\pi(1)}\\right)\\]\nWhere \\(c_{\\pi} = \\{ c_{\\pi(1)}, c_{\\pi(2)}, \\dots, c_{\\pi(n)} \\}\\), that is, all permutations of the \\(n\\) cities.\n ### Greedy TSP Solution\n\nStart at Richmond. Find the nearest city. From Richmond, the nearest city is Petersburg (25 miles). Move to Petersburg.\nFrom Petersburg, find the nearest unvisited city, which is Newport News (65 miles). Move to Newport News .\nFrom Newport News, the nearest unvisited city is Norfolk (30 miles). Move to Norfolk.\nFrom Norfolk, the nearest unvisited city is Chesapeake (10 miles).Move to Chesapeake.\nFrom Chesapeake, the only unvisited city left is Virginia Beach (15 miles). Move to Virginia Beach.\nFinally, return to Richmond from Virginia Beach (100 miles).\nRichmond -&gt; Petersburg -&gt; Newport News -&gt; Norfolk -&gt; Chesapeake -&gt; Virginia Beach -&gt; Richmond\nTotal distance traveled: = 25+65+30+10+15+100 = 245 miles\n\n\n\n\nTSP Feasibility Map\n\n\n\n8.1.1 TSP Python Implementation\n\n# Define the cities and distances between them\ncities = ['Richmond', 'Petersburg', 'Chesapeake', 'Norfolk', 'Newport News', 'Virginia Beach']\n\n# Distances matrix (symmetric)\ndistances = {\n    'Richmond': {'Petersburg': 25, 'Chesapeake': 95, 'Norfolk': 92, 'Newport News': 70, 'Virginia Beach': 100},\n    'Petersburg': {'Richmond': 25, 'Chesapeake': 85, 'Norfolk': 87, 'Newport News': 65, 'Virginia Beach': 90},\n    'Chesapeake': {'Richmond': 95, 'Petersburg': 85, 'Norfolk': 10, 'Newport News': 40, 'Virginia Beach': 15},\n    'Norfolk': {'Richmond': 92, 'Petersburg': 87, 'Chesapeake': 10, 'Newport News': 30, 'Virginia Beach': 18},\n    'Newport News': {'Richmond': 70, 'Petersburg': 65, 'Chesapeake': 40, 'Norfolk': 30, 'Virginia Beach': 45},\n    'Virginia Beach': {'Richmond': 100, 'Petersburg': 90, 'Chesapeake': 15, 'Norfolk': 18, 'Newport News': 45}\n}\n\nstart_city = 'Richmond'\n\n# Function to find the nearest neighbor\ndef find_nearest_neighbor(current_city, unvisited):\n    nearest_city = None\n    min_distance = float('inf')\n    for city in unvisited:\n        if distances[current_city][city] &lt; min_distance:\n            min_distance = distances[current_city][city]\n            nearest_city = city\n    return nearest_city, min_distance\n\n# Nearest Neighbor algorithm implementation\ndef nearest_neighbor_tsp(start_city):\n    unvisited = cities.copy()\n    unvisited.remove(start_city)\n    current_city = start_city\n    route = [start_city]\n    total_distance = 0\n    \n    while unvisited:\n        next_city, distance = find_nearest_neighbor(current_city, unvisited)\n        route.append(next_city)\n        total_distance += distance\n        current_city = next_city\n        unvisited.remove(current_city)\n    \n    # Return to the starting city\n    total_distance += distances[current_city][start_city]\n    route.append(start_city)\n    \n    return route, total_distance\n\n# Running the algorithm starting from Richmond\nroute, total_distance = nearest_neighbor_tsp(start_city)\n\n# Output the result\nprint(\"Optimal route using Nearest Neighbor:\", \" -&gt; \".join(route))\nprint(\"Total distance traveled:\", total_distance, \"miles\")\n\nOptimal route using Nearest Neighbor: Richmond -&gt; Petersburg -&gt; Newport News -&gt; Norfolk -&gt; Chesapeake -&gt; Virginia Beach -&gt; Richmond\nTotal distance traveled: 245 miles",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "greedy.html#tsp-python-implementation",
    "href": "greedy.html#tsp-python-implementation",
    "title": "4¬† Introduction to Greedy Algorithms",
    "section": "8.2 TSP Python Implementation",
    "text": "8.2 TSP Python Implementation\n\n# Define the cities and distances between them\ncities = ['Richmond', 'Petersburg', 'Chesapeake', 'Norfolk', 'Newport News', 'Virginia Beach']\n\n# Distances matrix (symmetric)\ndistances = {\n    'Richmond': {'Petersburg': 25, 'Chesapeake': 95, 'Norfolk': 92, 'Newport News': 70, 'Virginia Beach': 100},\n    'Petersburg': {'Richmond': 25, 'Chesapeake': 85, 'Norfolk': 87, 'Newport News': 65, 'Virginia Beach': 90},\n    'Chesapeake': {'Richmond': 95, 'Petersburg': 85, 'Norfolk': 10, 'Newport News': 40, 'Virginia Beach': 15},\n    'Norfolk': {'Richmond': 92, 'Petersburg': 87, 'Chesapeake': 10, 'Newport News': 30, 'Virginia Beach': 18},\n    'Newport News': {'Richmond': 70, 'Petersburg': 65, 'Chesapeake': 40, 'Norfolk': 30, 'Virginia Beach': 45},\n    'Virginia Beach': {'Richmond': 100, 'Petersburg': 90, 'Chesapeake': 15, 'Norfolk': 18, 'Newport News': 45}\n}\n\nstart_city = 'Richmond'\n\n# Function to find the nearest neighbor\ndef find_nearest_neighbor(current_city, unvisited):\n    nearest_city = None\n    min_distance = float('inf')\n    for city in unvisited:\n        if distances[current_city][city] &lt; min_distance:\n            min_distance = distances[current_city][city]\n            nearest_city = city\n    return nearest_city, min_distance\n\n# Nearest Neighbor algorithm implementation\ndef nearest_neighbor_tsp(start_city):\n    unvisited = cities.copy()\n    unvisited.remove(start_city)\n    current_city = start_city\n    route = [start_city]\n    total_distance = 0\n    \n    while unvisited:\n        next_city, distance = find_nearest_neighbor(current_city, unvisited)\n        route.append(next_city)\n        total_distance += distance\n        current_city = next_city\n        unvisited.remove(current_city)\n    \n    # Return to the starting city\n    total_distance += distances[current_city][start_city]\n    route.append(start_city)\n    \n    return route, total_distance\n\n# Running the algorithm starting from Richmond\nroute, total_distance = nearest_neighbor_tsp(start_city)\n\n# Output the result\nprint(\"Optimal route using Nearest Neighbor:\", \" -&gt; \".join(route))\nprint(\"Total distance traveled:\", total_distance, \"miles\")\n\nOptimal route using Nearest Neighbor: Richmond -&gt; Petersburg -&gt; Newport News -&gt; Norfolk -&gt; Chesapeake -&gt; Virginia Beach -&gt; Richmond\nTotal distance traveled: 245 miles",
    "crumbs": [
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Introduction to Greedy Algorithms</span>"
    ]
  },
  {
    "objectID": "benchmark.html#onemax-problem",
    "href": "benchmark.html#onemax-problem",
    "title": "5¬† Benchmark Optimization Problems",
    "section": "6.1 OneMax Problem",
    "text": "6.1 OneMax Problem\n\nIn evolutionary algorithms, the OneMax problem serves as a simple test problem where the goal is to evolve a population of binary strings towards the optimal solution (a string of all 1s). The fitness function is used to evaluate the quality of each candidate solution in the population.\nBinary String: A binary string is generated using NumPy‚Äôs randint function, which creates a list of 0s and 1s. It can be any positive integer.\nFitness Function: The one_max function calculates the ‚Äúfitness‚Äù of the binary string, which is simply the sum of all 1s in the string. This is the value that needs to be maximized.\nExample Run: If the generated binary string is [1, 0, 1, 1, 0, 1, 0, 1, 1, 0], the fitness would be 6, since there are six 1s in the string.\nThe objective is to maximize the number of 1s in a binary string.\n\n\\(\\max_{s \\in A} f(s) = \\sum_{i=1}^{n} s_i, \\quad \\text{subject to} \\ s_i \\in \\{0, 1\\}.\\)\n\n6.1.1 Optimal Solution OneMax\n\nThe optimal solution of this problem is that all the subsolutions assume the value 1; i.e., \\(s_i=1\\) for all \\(i\\). For instance, the optimal solution for \\(n=4\\) is \\(s^*=(1111)\\) and the objective value of a possible solution \\(s^*=(0111)\\) can be easily calculated as the count of the number of ones in the solution \\(s\\) as the objective function if \\(f(s) = f(0111) = 0+1+1+1 = 3\\)\n\n\n\n6.1.2 OneMax Pseudocode\nPseudocode:\n\n\nFUNCTION one_max(binary_string):\n# Calculate the fitness as the sum of 1s in the binary string\n    RETURN sum(binary_string)\n\n# Example usage\nSET n = 10  # Length of the binary string\n\n# Generate a random binary string of length n\nSET binary_string = generate a random list of 0s and 1s of size n\n\n# Calculate the fitness\nSET fitness = one_max(binary_string)\n\n# Print the binary string and its fitness\nPRINT \"Binary string:\", binary_string\nPRINT \"Fitness (number of 1s):\", fitness\n\n\n\n6.1.3 OneMax Python Implementation\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef one_max(binary_string):\n    return sum(binary_string)\n\n# Example usage: # Length of the binary string\nn = 10\n\n# Generate a random binary string of length n\nbinary_string = np.random.randint(0, 2, size=n).tolist()\n\n# Calculate the fitness\nfitness = one_max(binary_string)\n\nprint(f\"Binary string: {binary_string}\")\nprint(f\"Fitness (number of 1s): {fitness}\")\n\n\niterations = 20\nfitness_over_time = [np.random.randint(0, n + 1) for _ in range(iterations)]  # Random example of fitness change\n\n# Line plot of fitness over iterations\nplt.figure(figsize=(8, 4))\nplt.plot(range(iterations), fitness_over_time, marker='o', color='green', linestyle='-', linewidth=2)\nplt.fill_between(range(iterations), fitness_over_time, color='lightgreen', alpha=0.4)\nplt.title(f\"Fitness Evolution Over Time\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Fitness (number of 1s)\")\nplt.xticks(np.arange(0, iterations, step=1))  # Show whole numbers on the x-axis\n\nplt.grid(True)\nplt.show()\n\nBinary string: [0, 1, 0, 1, 0, 1, 0, 0, 0, 1]\nFitness (number of 1s): 4\n\n\n\n\n\n\n\n\n\n\nThe plot above tracks how fitness improves or changes across iterations in an optimization algorithm, giving insight into the convergence of the algorithm.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "benchmark.html#comparing-onemax-problem-to-greedy-algorithm",
    "href": "benchmark.html#comparing-onemax-problem-to-greedy-algorithm",
    "title": "5¬† Benchmark Optimization Problems",
    "section": "6.2 Comparing OneMax Problem to Greedy Algorithm",
    "text": "6.2 Comparing OneMax Problem to Greedy Algorithm\n\nIf a greedy search algorithm is used and is allowed to randomly add one to or subtract one from the current solution \\(s\\) to create the next possible solution \\(v\\) for solving the one-max problem, that is, it is allowed to move one and only one step to either the left or the right of the current solution in the landscape of the solution space.\nWithout knowledge of the landscape of the solution space, the search process will easily get stuck in the peaks of this solution space.\nHence, most researchers prefer using the one-max problem as an example because it is easy to implement and also because it can be used to prove if a new concept for a search algorithm is correct.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "benchmark.html#the-knapsack-problem",
    "href": "benchmark.html#the-knapsack-problem",
    "title": "5¬† Benchmark Optimization Problems",
    "section": "6.3 The Knapsack Problem",
    "text": "6.3 The Knapsack Problem\n\nThe Knapsack Problem is a classic NP-complete optimization problem, where you are given a set of items, each with a weight and a value.\nThe goal is to determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible.\nTypes of Knapsack Problems:\n\n0/1 Knapsack Problem:\n\nEach item can be included (1) or excluded (0) in the knapsack.\nYou cannot break items into smaller parts. \\(\\max_{s \\in A} f(s) = \\sum_{i=1}^{n} s_i v_i, \\quad \\text{subject to} \\quad w(s) = \\sum_{i=1}^{n} s_i w_i \\leq W, \\quad s_i \\in \\{0, 1\\}\\) Where \\(v_i\\) is the value associated with \\(s_1\\) and \\(w_i\\) is the weight associated with \\(s_i\\)\n\nFractional Knapsack Problem:\n\nYou can break items into smaller parts and include fractions of them in the knapsack.\nRatio = \\(\\frac{v_i}{w_i}\\), where \\(v_i\\) is the value of item \\(i\\), and \\(w_i\\) is the weight of item \\(i\\).\n\n\n\n\n6.3.1 NP Complete\n\nNP (Nondeterministic Polynomial Time): A problem is in NP if a solution can be verified in polynomial time by a deterministic algorithm. In other words, given a solution, it is possible to check if it is correct relatively quickly (in polynomial time). However, finding the solution itself might take much longer (potentially exponential time) unless the problem can also be solved in polynomial time.\nNP-complete refers to a class of problems in computational complexity theory that are both NP (nondeterministic polynomial time) and every problem in NP can be reduced to it in polynomial time\nNP-hard problems are optimization or decision problems that are at least as difficult to solve as the hardest problems in NP (nondeterministic polynomial time).\n\nUnlike NP-complete problems, NP-hard problems do not have to be verifiable in polynomial time. This means that while it may be incredibly hard to find an optimal solution, even verifying a proposed solution might take more than polynomial time.\nEssentially, NP-hard problems are hard to solve optimally, and their complexity often prevents efficient algorithms from finding or checking solutions within a reasonable time frame.\nNP-hard problems are broader and potentially harder than NP-complete problems because they can include problems that aren‚Äôt even in NP. They may not have a polynomial-time verification process.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "benchmark.html#key-characteristics-of-np-complete-problems",
    "href": "benchmark.html#key-characteristics-of-np-complete-problems",
    "title": "5¬† Benchmark Optimization Problems",
    "section": "6.4 Key Characteristics of NP-complete Problems",
    "text": "6.4 Key Characteristics of NP-complete Problems\n\nDifficult to solve: No known algorithms can solve NP-complete problems efficiently (in polynomial time) for all instances.\nVerification in polynomial time: If someone provides a solution, it can be verified quickly. Equivalence to other NP-complete problems: If one NP-complete problem can be solved in polynomial time, all NP-complete problems can be solved in polynomial time.\nKnapsack Problem: The 0/1 knapsack problem is NP-complete. Finding the optimal solution is hard, but verifying if a solution meets the constraints and maximizes value can be done in polynomial time.\nThe Fractional knapsack problem is not NP-complete and can be solved in polynomial time using a greedy algorithm.\nThe Travelling Salesman is a NP-hard problem.\n\n\n6.4.1 Example Fractional Knapsack Problem\n\nKnapsack Capacity: 15 kg\n\nItems Available:\n\nItem 1: Value = 10, Weight = 5 kg\nItem 2: Value = 40, Weight = 10 kg\nItem 3: Value = 30, Weight = 15 kg\n\n\nObjective: Maximize the total value without exceeding 15 kg.\nThe greedy algorithm works well by prioritizing items with the highest value-to-weight ratio.\n0/1 Knapsack Problem requires more complex algorithms like dynamic programming to find the optimal solution over the Fractional Knapsack Problem\n\n\n6.4.1.1 Greedy Algorithm for Fractional Knapsack\n\nStep 1: Calculate Value-to-Weight Ratio:\n\nItem 1: 10/5=2\nItem 2: 40/10=4\nItem 3: 30/15=2\n\nStep 2: Sort Items by Ratio (Descending): Item 2, Item 1, Item 3\n\nStep 3: Fill the Knapsack:\nTake Item 2 (10 kg, Value = 40).\nTake as much of Item 1 as possible (5 kg, Value = 10).\n\nResults: Total Weight = 15 kg, Total Value = 50.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "benchmark.html#binary-to-decimal-b2d-problem-b2d-1",
    "href": "benchmark.html#binary-to-decimal-b2d-problem-b2d-1",
    "title": "5¬† Benchmark Optimization Problems",
    "section": "6.5 Binary to Decimal (B2D) Problem: B2D-1",
    "text": "6.5 Binary to Decimal (B2D) Problem: B2D-1\n\nThe binary to decimal model is often used in optimization problems, particularly in the context of genetic algorithms and heuristic methods.\nWith a minor modification, the solution space of the one-max problem can be simplified as the solution space of another optimization problem.\nThe model uses binary strings to represent numbers. Each string represents a decimal number when interpreted in binary form.\nThe B2D-1 problem is to maximize the value of the objective function of a binary string.\n\n\n6.5.1 Characteristics and Visualization of B2D-1\n\nThese two examples are possible landscapes to the B2D problem.\nThe first chart to the left implies that there are only two possible next states (candidate solutions) that can be generated from the current solution except for solutions (0000) and (1111), which can only be moved to the right and to the left, respectively.\nIf another search algorithm can generate a new candidate solution by randomly inverting (flipping) one of the subsolutions of the current solution, the number of possible states of the new candidate solution will be \\(n\\), where \\(n\\) is the number of subsolutions.\n\n\n\n\nOneMax",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "benchmark.html#b2d-with-deception-b2d-2",
    "href": "benchmark.html#b2d-with-deception-b2d-2",
    "title": "5¬† Benchmark Optimization Problems",
    "section": "6.6 B2D with Deception: B2D-2",
    "text": "6.6 B2D with Deception: B2D-2\n\nB2D deception problems mislead optimization algorithms away from finding the global optimum by presenting local optima that seem promising but are actually suboptimal.\nUsed to test whether a search algorithm is capable of escaping local optima or not.\nDeception problems highlight the necessity of exploration in heuristic algorithms, such as introducing diversity through mutation or crossover in genetic algorithms. If the algorithm becomes too greedy and focuses only on local fitness improvements (exploitation), it may get stuck at deceptive local optima.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "benchmark.html#single-objective-optimization-problem-sop",
    "href": "benchmark.html#single-objective-optimization-problem-sop",
    "title": "5¬† Benchmark Optimization Problems",
    "section": "7.1 Single-objective Optimization Problem (SOP)",
    "text": "7.1 Single-objective Optimization Problem (SOP)\n\nA single-objective optimization problem involves finding the best solution from a set of feasible solutions based on a single objective function. The goal is to either maximize or minimize this objective function.\n\n\\[\\underset{s \\in \\mathbb{R}^n}{\\text{opt}} f(s), \\quad \\text{subject to } \\, c_i(s) \\odot b_i, \\quad i = 1, 2, \\ldots, m,\\]\n\nwhere\n\n\\({R}^n\\) and \\({R}\\) are the domain and codomain, respectively,\n\\(f(s) {R}^n\\) and \\({R}\\) is the objective function to be optimized,\n\\(c_i(s): {R}^n\\) and \\({R}\\odot b_i, \\quad i = 1, 2, \\ldots, m,\\) are the constraints,\nand \\(opt\\) and \\(\\odot\\) are as given in Definition 1 as &lt;, &gt;, =, ‚©Ω, or ‚©æ.\n\n\n\n7.1.1 Ackley Function: A Single Optimization Problem\n\nThe Ackley Function is a widely used benchmark function for testing optimization algorithms. It is characterized by its multi-modal nature with a nearly flat outer region and a large hole at the center.\nApplications\n\nUsed as a standard test case in evaluating the performance of optimization algorithms like genetic algorithms, simulated annealing, and particle swarm optimization.\nRelevant in fields such as machine learning, control systems, and operations research.\n\nLimitations\n\nThe function‚Äôs large search space and numerous local minima make it difficult for algorithms to converge to the global minimum.\nLarge importance of balancing exploration and exploitation in optimization strategies when dealing with the Ackley Function.\n\n\n\n7.1.1.1 Explanation of Ackley Function(x, y)\n\nComputes the value of the Ackley function given a point (x, y) in the search space.\nThe optimization algorithm optimizes the Ackley function to find the point where it reaches its minimum. It initializes a population of random solutions, evaluates their fitness (using the Ackley function), and iteratively improves them using an optimization method (like gradient descent or a genetic algorithm).\nThe best solution and corresponding function value (score) are returned as the result.\n\n\n\n7.1.1.2 Ackley function and B2D: Converting to Decimal\n\nThe Ackley function uses the binary representation, where the binary strings need to be converted to decimal values (i.e., real numbers). In this case, the converted decimal values correspond to points in the continuous search space.\nFor example, a binary string like 1010 can be converted into a decimal value, which can then be used as input to the Ackley function. Binary String: 1010, where the binary number is 1010_2.\nEach position in the binary number represents a power of 2, starting from the right (least significant bit):\nThe rightmost bit (0) is \\(2^0\\),The next bit (1) is \\(2^1\\) ,The next bit (0) is \\(2^2\\),The leftmost bit (1) is \\(2^3\\).\n\n\\(1010_2= 0 ‚àó 2^0+ 1 ‚àó 2^1 +0 ‚àó 2^2+1 ‚àó 2^3\\) \\(= 0 ‚àó 1+ 1 ‚àó 2 + 0 ‚àó 4 + 1 ‚àó 8\\) \\(=  0 + 2 + 0 + 8 = 10\\) Thus, the decimal equivalent of the binary string ‚Äú1010‚Äù is 10. Use this value as input for the Ackley function.\n\n\n7.1.1.3 Characteristics and Visualization of Ackley Function\n\nThe Ackley function is evaluated in the hypercube.\nThe global optimum (minimum) of the Ackley function is ùëì(ùë†^‚àó)=0 is located at \\(s^*=(0,0,‚Ä¶0)\\).\nThis function has many local optima, which makes it hard for the search algorithm to find the global optimum.\n\n\n\n\nAckley\n\n\n\n\n7.1.1.4 Ackley Function Formula\n\\[\n\\begin{array}{rl}\n\\min_{s \\in \\mathbb{R}^n} f(s) &= -20 \\exp \\left(-0.2 \\sqrt{\\frac{1}{n} \\sum_{i=1}^n s_i^2} \\right) \\\\\n& \\quad - \\exp \\left( \\frac{1}{n} \\sum_{i=1}^n \\cos(2 \\pi s_i) \\right) + 20 + e, \\\\\n\\text{subject to} & \\quad -30 \\leq s_i \\leq 30, \\quad i = 1, 2, \\dots, n.\n\\end{array}\n\\]\n\n\n7.1.1.5 Ackley Function: Pseudocode\n\nThe below example uses a random function to pull a point that we want to hit the local minima. You can imagine, this might not be the best way to do this.\n\nPSEUDOCODE\nFUNCTION Ackley(s):\n    SET a = 20, b = 0.2, c = 2 * pi\n    SET n = length of s\n    COMPUTE sum_sq_term = sum of squares of all elements in s\n    COMPUTE cos_term = sum of cos(2 * pi * each element in s)\n    \n    COMPUTE term1 = -a * exp(-b * sqrt(sum_sq_term / n))\n    COMPUTE term2 = -exp(cos_term / n)\n    \n    RETURN term1 + term2 + a + e\n\n# Main Execution\nSET n = 2  # Dimension of the Ackley function\n\n# Generate random vector s with elements between -30 and 30\nSET s = random values in range [-30, 30] of length n\n\n# Compute the Ackley function result for the vector s\nSET result = Ackley(s)\n\nPRINT vector s\nPRINT Ackley function result for vector s\n\n\n\n7.1.1.6 Ackley Function Python Implementation\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nnp.random.seed(5042)\n\n# Ackley function implementation\ndef ackley(s):\n    a, b, c = 20, 0.2, 2 * np.pi\n    n = len(s)\n    sum_sq_term = np.sum(s**2)\n    cos_term = np.sum(np.cos(c * s))\n    term1 = -a * np.exp(-b * np.sqrt(sum_sq_term / n))\n    term2 = -np.exp(cos_term / n)\n    return term1 + term2 + a + np.e\n\n# Example usage: \nn = 2  # Dimension \ns = np.random.uniform(-30, 30, n)  # Generate random s_i values in the range [-30, 30]\n\nresult = ackley(s)  # Evaluate the Ackley function\n\nprint(f\"Vector s: {s}\")\nprint(f\"Ackley function result: {result}\")\n\n# Visualization of the Ackley function\nx = np.linspace(-30, 30, 400)\ny = np.linspace(-30, 30, 400)\nX, Y = np.meshgrid(x, y)\n\n# Compute Z for the Ackley function\nZ = np.array([ackley(np.array([x_val, y_val])) for x_val, y_val in zip(np.ravel(X), np.ravel(Y))])\nZ = Z.reshape(X.shape)\n\n# Plotting the Ackley function surface\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none')\n\n# Customize the plot\nax.set_title(\"Ackley Function Surface\")\nax.set_xlabel(\"s_1\")\nax.set_ylabel(\"s_2\")\nax.set_zlabel(\"f(s)\")\n\n# Show the plot\nplt.show()\n\nVector s: [12.84779359  3.16468632]\nAckley function result: 17.91745666838746\n\n\n\n\n\n\n\n\n\n\nExample results: Vector s: [12.8 3.16]; Ackley function result: 17.9\nDistance from the Origin: The Ackley function reaches its global minimum of 0 at the origin (i.e., when both \\(x_1\\) and \\(x_2\\) are close to 0). Our vector values are quite far from the origin, which is why the function result is positive and relatively large at 17.9‚Äù\nThe Ackley landscape has an exponentially increasing structure as you move away from the global minimum. It has many local minima, which makes optimization algorithms prone to getting stuck in suboptimal solutions. A result like 17.9 is far from zero, and indicates that the vector is located in such a suboptimal region of the function space.\nThus, the Ackley result of 17.9 suggests that the point [12.8 3.16]; is not close to the global minimum (which is zero at the origin) and is located in a region of higher function values.\n\n\n\n7.1.1.7 Differential Evolution with the Ackley Function\n\nTo demonstrate how an algorithm does well on the Ackley function, we can use a global optimization algorithm such as Differential Evolution (DE), which is effective for non-convex functions with many local minima.\nDifferential Evolution (DE) is a population-based optimization algorithm used for solving complex multidimensional problems. It belongs to the family of evolutionary algorithms, where a population of candidate solutions evolves over time to find the global optimum of a function.\nThe differential_evolution function from the scipy.optimize module is a powerful optimization tool designed to solve global optimization problems. It is a type of evolutionary algorithm, which is used when the function to optimize is non-linear, has many local minima, or is not differentiable.\n\n\nimport numpy as np\nfrom scipy.optimize import differential_evolution\nimport matplotlib.pyplot as plt\n\nnp.random.seed(5042)\n\n# Define the Ackley function\n# Ackley function implementation\ndef ackley(s):\n    a, b, c = 20, 0.2, 2 * np.pi\n    n = len(s)\n    sum_sq_term = np.sum(s**2)\n    cos_term = np.sum(np.cos(c * s))\n    term1 = -a * np.exp(-b * np.sqrt(sum_sq_term / n))\n    term2 = -np.exp(cos_term / n)\n    return term1 + term2 + a + np.e\n\n# Set the bounds for the variables \nbounds = [(-30, 30), (-30, 30)]\n\n# Use differential evolution to minimize the Ackley function\nresult = differential_evolution(ackley, bounds, seed=42)\n\n# Print the result\nprint(f'Optimized parameters (x1, x2): {result.x}')\nprint(f'Function value at minimum: {result.fun}')\n\n\n# Visualization of the Ackley function\nx = np.linspace(-30, 30, 400)\ny = np.linspace(-30, 30, 400)\nX, Y = np.meshgrid(x, y)\n\n# Compute Z for the Ackley function\nZ = np.array([ackley(np.array([x_val, y_val])) for x_val, y_val in zip(np.ravel(X), np.ravel(Y))])\nZ = Z.reshape(X.shape)\n\n# Plotting the Ackley function surface\nfig = plt.figure(figsize=(10, 7))\nax = fig.add_subplot(111, projection='3d')\nax.plot_surface(X, Y, Z, cmap='viridis', edgecolor='none')\n\n# Customize the plot\nax.set_title(\"Ackley Function Surface (2D)\")\nax.set_xlabel(\"s_1\")\nax.set_ylabel(\"s_2\")\nax.set_zlabel(\"f(s)\")\n\n# Show the plot\nplt.show()\n\nOptimized parameters (x1, x2): [0. 0.]\nFunction value at minimum: 4.440892098500626e-16\n\n\n\n\n\n\n\n\n\n\nUsing a great function that is designed to solve problems with multiple local minimums, you can see that we got extremely close to the local minimum 0.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  },
  {
    "objectID": "benchmark.html#multi-objective-optimization-problem-mop",
    "href": "benchmark.html#multi-objective-optimization-problem-mop",
    "title": "5¬† Benchmark Optimization Problems",
    "section": "7.2 Multi-objective Optimization Problem (MOP)",
    "text": "7.2 Multi-objective Optimization Problem (MOP)\n\nGiven a set of functions and a set of constraints, the MOP is to find the optimal value or a set of optimal values (also called Pareto front), subject to the constraints, out of all possible solutions of these functions.\n\n\\[\\text{opt}\\left( f_1(s), f_2(s), \\dots, f_k(s) \\right),\n\\quad \\mathbf{s} \\in \\mathbb{R}^n,\n\\quad \\text{subject to } c_i(s) \\odot b_i, \\quad i = 1, 2, \\dots, m,\\]\n\nWhere\n\n\\({R}^n\\) and \\({R}\\) are the domain and codomain, respectively,\n\\(f(s) {R}^n\\) and \\({R}\\) is the objective function to be optimized,\n\\(c_i(s): {R}^n\\) and \\({R}\\odot b_i, \\quad i = 1, 2, \\ldots, m,\\) are the constraints,\nand \\(opt\\) and \\(\\odot\\) are as given in Definition 1 as &lt;, &gt;, =, ‚©Ω, or ‚©æ.\n\n\n\n7.2.1 The Schaffer min-min Multi-objective Optimization Problem\n\nThe Schaffer min-min problem is a well-known test function in the field of multi-objective optimization.\nIt is often used to evaluate optimization algorithms due to its simplicity and well-defined structure. The problem is particularly famous for having a simple Pareto-optimal front.\nThe Schaffer function can be defined as a two-objective optimization problem, where the objectives are functions of a single variable x.\nThe goal is to minimize both of these objective functions simultaneously.\n\n\n7.2.1.1 Characteristics and Visualization\n\nConvexity: The Pareto front of the Schaffer min-min problem is convex, making it relatively easy to identify the trade-off surface between the two objectives.\nUniqueness: The problem has a unique global Pareto-optimal front.\nComplexity: Despite its simplicity, the problem is useful for benchmarking optimization algorithms, especially in multi-objective optimization.\nGraphically, the Pareto front of the Schaffer min-min problem can be visualized as a curve in the objective space, where \\(f_1(x)\\) is plotted against \\(f_2(x)\\) and the curve represents the set of optimal trade-offs between the two objectives.\n\n\n\n\n\n\n\n\n\n\n\n\n7.2.1.2 scipy.optimize import minimize\n\nminimize is a general-purpose function from scipy.optimize used for finding the minimum of a scalar function.\nUses the BFGS (Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno) algorithm when no specific method is provided. This method is a quasi-Newton optimization algorithm, particularly useful for smooth unconstrained problems.\n\nIt can handle different types of optimization problems, including: Unconstrained minimization\nConstrained minimization (equality and inequality constraints) Bounded minimization (where variables are limited to a certain range)\n\nBasic Workflow:\n\nDefine the objective function (the function to minimize). Choose an initial guess for the variables.\nRun the minimize function with the desired method.\nAnalyze the results: returns optimized variables, the function value, and other diagnostic information.\n\n\n\n\n7.2.1.3 Schaffer Min-Min Formula\n\\[\n\\min_{s \\in \\mathbb{R}^n}\n\\begin{cases}\nf_1(s) = s^2, \\\\\nf_2(s) = (s - 2)^2,\n\\end{cases}\n\\quad \\text{subject to} \\quad s \\in [-10^3, 10^3].\n\\]\n\n\n7.2.1.4 Schaffer Min-Min Pseudocode\nPseudocode\n\n# FUNCTION to calculate f1(s)\nFUNCTION f1(s):\n    RETURN s^2\n\n# FUNCTION to calculate f2(s)\nFUNCTION f2(s):\n    RETURN (s - 2)^2\n\n# FUNCTION for combined objective, weighted sum of f1 and f2\nFUNCTION combined_objective(s, w1=0.5, w2=0.5):\n    RETURN w1 * f1(s) + w2 * f2(s)\n\n# MAIN EXECUTION\n# Step 1: Set up the bounds for the solution (s ‚àà [-1000, 1000])\nSET bounds = [-1000, 1000]\n\n# Step 2: Initialize a starting guess for the solution\nSET initial_guess = 0\n\n# Step 3: Minimize the combined objective function using an optimization algorithm\nCALL minimize function with combined_objective, initial_guess, and bounds\nSTORE the result in result\n\n# Step 4: Print the optimization result\nPRINT \"Optimal value of s:\", result.x\nPRINT \"f1(s):\", f1(result.x)\nPRINT \"f2(s):\", f2(result.x)\nPRINT \"Combined objective:\", combined_objective(result.x)\n\n# Step 5: Visualization - Create a range of values for s from -1000 to 1000\n\n\n\n7.2.1.5 Schaffer Min-Min Python Implementation\n\nPopulation Initialization: A population of random solutions is initialized within the bounds [‚àí1000,1000]. Objective Function Evaluation: For each solution, both objective functions are evaluated.\nScore Combination: The results of the two functions are combined into a single score, which can be minimized. In this case, the combination is a simple sum of f1 and f2.\nOptimization Loop: Iteratively updates the solutions to find the minimum combined score using an optimization technique (e.g., gradient descent, genetic algorithm).\n\n\nimport numpy as np\nfrom scipy.optimize import minimize\nimport matplotlib.pyplot as plt\n\n\n# Define the two objective functions for the Schaffer problem\ndef f1(s):\n    return s**2\n\ndef f2(s):\n    return (s - 2)**2\n\n# Combined objective function: weighted sum of f1 and f2\n# You can adjust the weights to explore different trade-offs between the two objectives\ndef combined_objective(s, w1=0.5, w2=0.5):\n    return w1 * f1(s) + w2 * f2(s)\n\n# Bounds for the solution (s ‚àà [-1000, 1000])\nbounds = [(-1000, 1000)]\n\n# Initial guess for the solution\ninitial_guess = np.array([0])\n\n# Use scipy's minimize function to find the solution\nresult = minimize(combined_objective, initial_guess, bounds=bounds)\n\n# Print the result\nprint(\"Optimal value of s:\", result.x[0])\nprint(\"f1(s):\", f1(result.x[0]))\nprint(\"f2(s):\", f2(result.x[0]))\nprint(\"Combined objective:\", combined_objective(result.x[0]))\n\n# Visualization of the objective functions and combined objective\ns_values = np.linspace(-1000, 1000, 400)\nf1_values = f1(s_values)\nf2_values = f2(s_values)\ncombined_values = combined_objective(s_values)\n\n# Plotting\nplt.figure(figsize=(10, 6))\n\n# Plot f1(s), f2(s), and combined objective\nplt.plot(s_values, f1_values, label=\"f1(s) = s^2\", color='blue')\nplt.plot(s_values, f2_values, label=\"f2(s) = (s - 2)^2\", color='green')\nplt.plot(s_values, combined_values, label=\"Combined Objective\", color='red', linestyle='--')\n\n# Mark the optimal solution found\nplt.axvline(x=result.x[0], color='black', linestyle=':', label=f\"Optimal s = {result.x[0]:.2f}\")\n\n# Customize the plot\nplt.title(\"Schaffer Min-Min Problem Visualization\")\nplt.xlabel(\"s\")\nplt.ylabel(\"Objective Function Value\")\nplt.legend()\nplt.grid(True)\n\n# Show the plot\nplt.show()\n\nOptimal value of s: 1.0000000134831095\nf1(s): 1.0000000269662193\nf2(s): 0.9999999730337812\nCombined objective: 1.0000000000000002\n\n\n\n\n\n\n\n\n\n\nThe results you achieved for the Schaffer Min-Min problem look excellent, as they closely approximate the optimal solution.\nOptimal value of \\(s\\) we found is nearly exactly \\(1\\), the known optimal solution for the Schaffer function.\n\nObjective function values: \\(f_1(s) = s^2 = 1.000000027\\)\n: \\(f_2(s) = (s-2)^2 = 0.999999973\\)\nThese values are very close to 1 for both \\(f_1\\) and \\(f_2\\), indicating that the function values at this \\(s\\) are near-optimal.\n\nCombined objective: The combined objective (likely calculated as a weighted sum or some other combination of \\(f_1\\) and \\(f_2\\) is 1.00000, which is extremely close to the expected combined optimal value of 1. This negligible difference suggests that the optimization algorithm has performed very well.",
    "crumbs": [
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Benchmark Optimization Problems</span>"
    ]
  }
]